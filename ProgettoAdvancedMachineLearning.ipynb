{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ProgettoAdvancedMachineLearning.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "XB7vfL2kF1xv",
        "Wokm9nbpNCmy",
        "blBDHXrL1a7H",
        "pJF4__P8R3pp",
        "CuCfEGYxR5uT",
        "XnPHmOPER8D-",
        "Mn9ciyfGGg_c",
        "-eaW3A8eGNTN",
        "n7Qr0DdsyKjh",
        "edostJ_HiZg2",
        "Rk51gtleic4v",
        "SH90FdQ4vfD9",
        "MJ1MLmarSyiu"
      ],
      "authorship_tag": "ABX9TyN7+NdQSMmX/rhA5eeZnQ2f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sbarbagnem/AdvancedProject/blob/master/ProgettoAdvancedMachineLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJ8c2ioz09L5",
        "colab_type": "text"
      },
      "source": [
        "#Multi-label object classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMoW7MM81Gs9",
        "colab_type": "text"
      },
      "source": [
        "## Componenti del gruppo\n",
        "* Carta Costantino\n",
        "* Hamrani Hamza\n",
        "* Ventura Samuele 793060"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOajSqEU50XR",
        "colab_type": "text"
      },
      "source": [
        "## Module Import\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRj2H2n953x_",
        "colab_type": "code",
        "outputId": "f63a69ef-5b96-46b8-a1ab-f89da2928220",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        }
      },
      "source": [
        "from google.colab import drive\n",
        "import xml.etree.ElementTree as ET\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import time\n",
        "import os\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, hamming_loss, accuracy_score\n",
        "import numpy as np\n",
        "from ast import literal_eval\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Dropout, Flatten, BatchNormalization, AveragePooling2D\n",
        "from keras.optimizers import SGD\n",
        "from math import ceil\n",
        "from keras.callbacks import EarlyStopping, Callback\n",
        "from keras import backend as K\n",
        "from itertools import tee  # finally! I found something useful for it\n",
        "from sklearn import metrics\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.metrics import classification_report\n",
        "from keras.applications.vgg16 import preprocess_input, VGG16 \n",
        "import tensorflow as tf\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from keras import layers, models, optimizers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XB7vfL2kF1xv",
        "colab_type": "text"
      },
      "source": [
        "## Costants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79bHV0fG1iXZ",
        "colab_type": "code",
        "outputId": "5379e0df-d779-4f49-dc5a-cf8537d199ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEliLAW0F5nw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PATH_ABSOLUTE = '/content/drive/My Drive/Laurea Magistrale/Adavanched Machine Learning/Progetto/Dataset/'\n",
        "\n",
        "PATH_TRAIN = PATH_ABSOLUTE + 'Train/'\n",
        "PATH_TEST = PATH_ABSOLUTE + 'Test/'\n",
        "\n",
        "PATH_ANNOTATIONS_TRAIN =  PATH_TRAIN + 'Annotations'\n",
        "PATH_IMAGES_TRAIN = PATH_TRAIN + 'Images'\n",
        "\n",
        "PATH_MAIN_VAL = PATH_TRAIN + 'Main/val.txt'\n",
        "\n",
        "PATH_ANNOTATIONS_TEST =  PATH_TEST + 'Annotations'\n",
        "PATH_IMAGES_TEST = PATH_TEST + 'Images'\n",
        "\n",
        "PATH_CSV_IMAGE_LABEL_TRAIN_VAL = PATH_ABSOLUTE + 'image_label_train_val.csv'\n",
        "PATH_CSV_IMAGE_LABEL_TRAIN = PATH_ABSOLUTE + 'image_label_train.csv'\n",
        "PATH_CSV_IMAGE_LABEL_VAL = PATH_ABSOLUTE + 'image_label_val.csv'\n",
        "PATH_CSV_IMAGE_LABEL_TEST = PATH_ABSOLUTE + 'image_label_test.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wokm9nbpNCmy",
        "colab_type": "text"
      },
      "source": [
        "## Util function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuZy22B2NEfe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def xml_to_labels(xml_data, unique_labels):\n",
        "  root = ET.XML(xml_data)\n",
        "  labels = set() if unique_labels else []\n",
        "  labels_add = labels.add if unique_labels else labels.append \n",
        "  for i, child in enumerate(root):\n",
        "    if child.tag == 'filename':\n",
        "      img_filename = child.text\n",
        "    if child.tag == 'object':\n",
        "      for subchild in child:\n",
        "        if subchild.tag == 'name':\n",
        "          labels_add(subchild.text)\n",
        "  return img_filename, list(labels)\n",
        "\n",
        "def get_labels(annotations_dir, unique_labels=True):\n",
        "  '''\n",
        "  # annotation_dir: directory with annotations file\n",
        "  # unique_labels:  if true in presence of mutliple label of the same object we \n",
        "                    take label only one time\n",
        "  Return all image-labels from annotations file in annotation_dir\n",
        "  '''\n",
        "  dir_path = Path(annotations_dir)\n",
        "  for annotation_file in dir_path.iterdir():\n",
        "    with open(annotation_file) as f:\n",
        "      yield xml_to_labels(f.read(), unique_labels)\n",
        "      \n",
        "def plot_history(network_history, n_epochs):\n",
        "  \n",
        "    x_plot = list(range(1,n_epochs+1))\n",
        "    \n",
        "    plt.figure()\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.plot(x_plot, network_history.history['loss'])\n",
        "    plt.plot(x_plot, network_history.history['val_loss'])\n",
        "    plt.legend(['Training', 'Validation'])\n",
        "\n",
        "    plt.figure()\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.plot(x_plot, network_history.history['f1'])\n",
        "    plt.plot(x_plot, network_history.history['val_f1'])\n",
        "    plt.legend(['Training', 'Validation'], loc='lower right')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blBDHXrL1a7H",
        "colab_type": "text"
      },
      "source": [
        "## Creazione dataset csv (da eseguire solo la prima volta)\n",
        "Crea i vari csv contenenti i dati di train, val e test da usare come dataframe nei generator\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sHLJ76M5uA3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read annotatons file in train/annotations in dataframe \n",
        "# and save to csv image_label_train_val\n",
        "df = pd.DataFrame(get_labels(PATH_ANNOTATIONS_TRAIN), columns=['filename', 'labels'])\n",
        "df.to_csv(PATH_CSV_IMAGE_LABEL_TRAIN_VAL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJF4__P8R3pp",
        "colab_type": "text"
      },
      "source": [
        "### Train data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMIfGiKJXaj2",
        "colab_type": "code",
        "outputId": "bc234a50-9322-4c08-d815-cb8d4fa35060",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# dal csv image_label_train_val tolgo quelle immagini da usare come validation,\n",
        "# indicate nel file main/val.txt\n",
        "df = pd.read_csv(PATH_CSV_IMAGE_LABEL_TRAIN_VAL, index_col=0)\n",
        "\n",
        "print(df.shape)\n",
        "\n",
        "lines = []\n",
        "\n",
        "# image to use as validation\n",
        "with open(PATH_MAIN_VAL) as file:\n",
        "    lines = [line.strip() for line in file]\n",
        "lines = [line + \".jpg\" for line in lines]\n",
        "\n",
        "train = df[~df['filename'].isin(lines)]\n",
        "\n",
        "print(train.shape)\n",
        "\n",
        "train.to_csv(PATH_CSV_IMAGE_LABEL_TRAIN)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(11302, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuCfEGYxR5uT",
        "colab_type": "text"
      },
      "source": [
        "### Validation data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohLLHa8KXofP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dal csv image_label_train_val tengo quelle immagini\n",
        "# indicate nel file main/val.txt\n",
        "val = df[df['filename'].isin(lines)]\n",
        "\n",
        "print(val.shape)\n",
        "\n",
        "val.to_csv(PATH_CSV_IMAGE_LABEL_VAL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnPHmOPER8D-",
        "colab_type": "text"
      },
      "source": [
        "### Test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATovl2L-X3v7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tutte le immagini in dataset/test/annotations\n",
        "df = pd.DataFrame(get_labels(PATH_ANNOTATIONS_TEST), columns=['filename', 'labels'])\n",
        "df.to_csv(PATH_CSV_IMAGE_LABEL_TEST)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mn9ciyfGGg_c",
        "colab_type": "text"
      },
      "source": [
        "## Salvo immagini in csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iieZuYUFGsNO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# leggo da csv precedente e salvo train\n",
        "\n",
        "# leggo da csv precedente e salvo val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eaW3A8eGNTN",
        "colab_type": "text"
      },
      "source": [
        "## Image generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJ4QfsxfGPWb",
        "colab_type": "code",
        "outputId": "02ca8547-3b84-44c7-b032-ec7c41b761fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "import random\n",
        "\n",
        "def random_sample_from_dataset(dataframe, path_images):\n",
        "\n",
        "  image_generator = ImageDataGenerator( rescale=1/255,\n",
        "                                        )\n",
        "\n",
        "  '''images = image_generator.flow_from_dataframe(\n",
        "      dataframe,\n",
        "      directory=Path(path_images),\n",
        "      x_col='filename',\n",
        "      y_col='labels',\n",
        "      target_size=((256, 256)),\n",
        "      batch_size=64\n",
        "  )'''\n",
        "\n",
        "  idx_random = random.sample(range(0, len(dataframe)), 1000)\n",
        "\n",
        "  # tengo solo righe df con idx_random\n",
        "\n",
        "  # flow_from_dataframe da questo nuovo df\n",
        "\n",
        "  # fit per estrarre media e deviazione standard\n",
        "\n",
        "  # ritorno media e deviazione standard\n",
        "\n",
        "  return idx_random\n",
        "\n",
        "#idx_random = random_sample_from_dataset(train, PATH_IMAGES_TRAIN)\n",
        "\n",
        "# The helper function\n",
        "def multilabel_flow_from_dataframe(data_generator, mlb, df):\n",
        "    for x, y in data_generator:\n",
        "        assert isinstance(mlb, MultiLabelBinarizer), \\\n",
        "               \"MultiLabelBinarizer is required.\"\n",
        "        indices = y.astype(np.int).tolist()\n",
        "        y_multi = mlb.transform(\n",
        "             df.iloc[indices]['labels'].values\n",
        "        )\n",
        "        yield x, y_multi\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# usiamo 0.8 come train e 0.2 come val\n",
        "train = pd.read_csv(PATH_CSV_IMAGE_LABEL_TRAIN, index_col=0)\n",
        "train['labels'] = train['labels'].apply(lambda x: literal_eval(x))\n",
        "\n",
        "# sarebbe il validation da usare come test, così manteniamo un equilibrio nelle classi\n",
        "val = pd.read_csv(PATH_CSV_IMAGE_LABEL_VAL, index_col=0)\n",
        "val['labels'] = val['labels'].apply(lambda x: literal_eval(x))\n",
        "\n",
        "# add index to multilabel generator\n",
        "train['index'] = np.arange(0,len(train))\n",
        "val['index'] = np.arange(0,len(val))\n",
        "\n",
        "'''\n",
        "labels_count = Counter(label for lbs in train['labels'] for label in lbs)\n",
        "total_count = sum(labels_count.values())\n",
        "class_weights = {cls: total_count / count for cls, count in labels_count.items()}\n",
        "plt.bar(class_weights.keys(), class_weights.values())\n",
        "\n",
        "new = {}\n",
        "\n",
        "for i,label in zip(range(0,20), class_weights.keys()):\n",
        "  print(i, \" \", label)\n",
        "  new[i] = class_weights[label]\n",
        "\n",
        "class_weights = new\n",
        "'''\n",
        "# per usare data augmentation le immagini devono essere in flow separati e nel nostro\n",
        "# caso train e val sono insieme (se uso validation_split) per non fare data augmentation\n",
        "# sul validation uso tutto train e val separatamente.\n",
        "img_gen_train = ImageDataGenerator( rescale=1/255,\n",
        "                                    #featurewise_center=True,\n",
        "                                    #featurewise_std_normalization=True\n",
        "                                  )\n",
        "#img_gen_val = ImageDataGenerator(rescale=1/255, validation_split=0.5)\n",
        "\n",
        "train_generator = img_gen_train.flow_from_dataframe(\n",
        "    train,\n",
        "    shuffle=True,\n",
        "    directory=Path(PATH_IMAGES_TRAIN),\n",
        "    x_col='filename',\n",
        "    y_col='index',\n",
        "    class_mode='other',\n",
        "    target_size=((256, 256)),\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "val_generator = img_gen_train.flow_from_dataframe(\n",
        "    val,\n",
        "    shuffle=False,\n",
        "    directory=Path(PATH_IMAGES_TRAIN),\n",
        "    x_col='filename',\n",
        "    y_col='index',\n",
        "    class_mode='other',\n",
        "    target_size=((256, 256)),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    subset = 'training'\n",
        ")\n",
        "\n",
        "mlb = MultiLabelBinarizer()\n",
        "mlb.fit(train['labels'].values)\n",
        "mlb.classes_\n",
        "\n",
        "train_gen_multi = multilabel_flow_from_dataframe(train_generator, mlb, train)\n",
        "val_gen_multi = multilabel_flow_from_dataframe(val_generator, mlb, val)\n",
        "\n",
        "'''\n",
        "val_generator = img_gen_val.flow_from_dataframe(\n",
        "    val,\n",
        "    shuffle=False,\n",
        "    directory=Path(PATH_IMAGES_TRAIN),\n",
        "    x_col='filename',\n",
        "    y_col='labels',\n",
        "    class_mode='categorical',\n",
        "    target_size=((512, 512)),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    subset = 'training'\n",
        ")\n",
        "\n",
        "test_generator = img_gen_val.flow_from_dataframe(\n",
        "    val,\n",
        "    shuffle=False,\n",
        "    directory=Path(PATH_IMAGES_TRAIN),\n",
        "    x_col='filename',\n",
        "    y_col='labels',\n",
        "    class_mode='categorical',\n",
        "    target_size=((512, 512)),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    subset = 'validation'\n",
        ")\n",
        "'''\n",
        "'''\n",
        "from skimage import io\n",
        "\n",
        "def imshow(image_RGB):\n",
        "  io.imshow(image_RGB)\n",
        "  io.show()\n",
        "\n",
        "x,y = train_generator.next()\n",
        "\n",
        "for i in range(0,11):\n",
        "    image = x[i]\n",
        "    imshow(image)\n",
        "'''\n",
        "\n",
        "TRAIN_LEN = train_generator.n\n",
        "VAL_LEN = val_generator.n\n",
        "#TEST_LEN = test_generator.n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2010_005064.jpg</td>\n",
              "      <td>[dog]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2007_000713.jpg</td>\n",
              "      <td>[boat]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2010_003415.jpg</td>\n",
              "      <td>[person, car]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2011_006850.jpg</td>\n",
              "      <td>[person]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2008_005636.jpg</td>\n",
              "      <td>[cat]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          filename         labels\n",
              "1  2010_005064.jpg          [dog]\n",
              "2  2007_000713.jpg         [boat]\n",
              "3  2010_003415.jpg  [person, car]\n",
              "4  2011_006850.jpg       [person]\n",
              "7  2008_005636.jpg          [cat]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7Qr0DdsyKjh",
        "colab_type": "text"
      },
      "source": [
        "## Modello base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edostJ_HiZg2",
        "colab_type": "text"
      },
      "source": [
        "### Metriche su validation completo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0rXzAmWmGI1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Metrics(Callback):\n",
        "  def __init__(self, validation_generator, validation_steps, threshold=0.5):\n",
        "    self.validation_generator = validation_generator\n",
        "    self.validation_steps = validation_steps or len(validation_generator)\n",
        "    self.threshold = threshold\n",
        "\n",
        "  def on_train_begin(self, logs={}):\n",
        "    self.val_f1_scores = []\n",
        "    #self.val_hamming_scores = []\n",
        "    #self.subset_accuracy = []\n",
        "    self.val_recalls = []\n",
        "    self.val_precisions = []\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    #gen_1, gen_2 = tee(self.validation_generator)\n",
        "    y_true = np.vstack(next(self.validation_generator)[1] for _ in range(self.validation_steps)).astype('int')\n",
        "    y_pred = (self.model.predict_generator(self.validation_generator, steps=self.validation_steps) > self.threshold).astype('int')\n",
        "    # avarge indica come le misure sulle diverse classi sono mediate per avere un singolo valore di output\n",
        "    f1 = f1_score(y_true, y_pred, average='macro')\n",
        "    #hamming = hamming_loss(y_true, y_pred)\n",
        "    #accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred, average='macro')\n",
        "    recall = recall_score(y_true, y_pred, average='macro')\n",
        "    self.val_f1_scores.append(f1)\n",
        "    #self.val_hamming_scores.append(hamming)\n",
        "    #self.subset_accuracy.append(accuracy)\n",
        "    self.val_recalls.append(recall)\n",
        "    self.val_precisions.append(precision)\n",
        "    print(f\"- val_f1: {f1:.5f} - val_precision: {precision:.5f} - val_recall: {recall:.5f}\")\n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rk51gtleic4v",
        "colab_type": "text"
      },
      "source": [
        "### Metriche on batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzjK69_npjMr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def f1(y_true, y_pred):\n",
        "    def recall(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        return recall\n",
        "\n",
        "    def precision(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "    precision = precision(y_true, y_pred)\n",
        "    recall = recall(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SH90FdQ4vfD9",
        "colab_type": "text"
      },
      "source": [
        "## Focal loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aMgY8MOvgko",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def KerasFocalLoss(target, input):\n",
        "    \n",
        "    gamma = 2.\n",
        "    alpha = 1.\n",
        "\n",
        "    # from prob to logits\n",
        "    input = tf.cast(input, tf.float32)\n",
        "    \n",
        "    max_val = K.clip(-input, 0, 1)\n",
        "\n",
        "    # calculate binary cross entropy between target and input(prediction)\n",
        "    BCE_loss = tf.keras.backend.binary_crossentropy(target, input, from_logits=True)\n",
        "    \n",
        "    pt = tf.exp(-BCE_loss)\n",
        "    F_loss = alpha * (1-pt)**gamma * BCE_loss\n",
        "\n",
        "    return tf.keras.backend.mean(F_loss)\n",
        "\n",
        "    '''\n",
        "    loss = input - input * target + max_val + K.log(K.exp(-max_val) + K.exp(-input - max_val))\n",
        "    invprobs = tf.log_sigmoid(-input * (target * 2.0 - 1.0))\n",
        "    loss = K.exp(invprobs * gamma) * loss\n",
        "    \n",
        "    return K.mean(K.sum(loss, axis=1))\n",
        "    '''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4yWJbEQihuF",
        "colab_type": "text"
      },
      "source": [
        "### Modello"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwMvmTayyNGy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#metrics = Metrics(val_generator, ceil(VAL_LEN/BATCH_SIZE))\n",
        "#metrics = Metrics()\n",
        "input_image = layers.Input(shape = (256,256,3))\n",
        "x = layers.Conv2D(8, (3, 3), padding = 'same',activation='relu')(input_image)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Conv2D(8, (3, 3), padding = 'same', strides = 2, activation='relu')(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.MaxPooling2D(pool_size = (2, 2))(x)\n",
        "\n",
        "x = layers.Conv2D(16, (3, 3), padding = 'same',activation='relu')(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Conv2D(16, (3, 3), padding = 'same', strides = 2, activation='relu')(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.MaxPooling2D(pool_size = (2, 2))(x)\n",
        "\n",
        "x = layers.Conv2D(32, (3, 3), padding = 'same',activation='relu')(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Conv2D(32, (3, 3), padding = 'same', strides = 2, activation='relu')(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.MaxPooling2D(pool_size = (2, 2))(x)\n",
        "\n",
        "x = layers.Conv2D(64, (3, 3), padding = 'same',activation='relu')(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Conv2D(64, (3, 3), padding = 'same', strides = 2, activation='relu')(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.MaxPooling2D(pool_size = (2, 2))(x)\n",
        "\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(128,activation='relu')(x)\n",
        "x = layers.Dropout(0.3)(x)\n",
        "x = layers.Dense(64,activation='relu')(x)\n",
        "x = layers.Dropout(0.3)(x)\n",
        "\n",
        "output = layers.Dense(20, name = 'dense1')(x)\n",
        "output = layers.Activation('sigmoid',name='output_layer')(output)\n",
        "\n",
        "\n",
        "model = models.Model(input_image,output)\n",
        "\n",
        "model.compile(loss=KerasFocalLoss, optimizer=optimizers.Adam(lr = 0.0001), metrics=[f1])\n",
        "model.summary()\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=2, verbose=1)\n",
        "\n",
        "net_history = model.fit_generator(train_gen_multi, epochs=50, verbose=1,\n",
        "                  validation_data = val_gen_multi,\n",
        "                  steps_per_epoch = ceil(TRAIN_LEN/BATCH_SIZE),\n",
        "                  validation_steps = ceil(VAL_LEN/BATCH_SIZE),\n",
        "                  callbacks = [early_stop],\n",
        "                  workers = 8,\n",
        "                  max_queue_size=10,\n",
        "                  use_multiprocessing=True\n",
        "                  #class_weight=class_weights\n",
        "              )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bh663O-94ewS",
        "colab_type": "text"
      },
      "source": [
        "## Plot performance train epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_Fg4-nB4jzD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_history(net_history, 50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJ1MLmarSyiu",
        "colab_type": "text"
      },
      "source": [
        "## Performance on test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YAjcU4rS0Go",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = (model.predict_generator(test_generator, \n",
        "                                  workers = 8, \n",
        "                                  max_queue_size=10,\n",
        "                                  use_multiprocessing=True) > 0.5).astype('int')\n",
        "mlb = MultiLabelBinarizer()\n",
        "y_true = mlb.fit_transform(test_generator.labels)\n",
        "\n",
        "report = classification_report(y_true=y_true, y_pred=y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxKyf5iqFaKa",
        "colab_type": "text"
      },
      "source": [
        "## Modello VGG16 e Vector Machine per ogni label\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ysv3Fowlu-gn",
        "colab_type": "text"
      },
      "source": [
        "## Ottimizzazione\n",
        "* Provare diverse loss:\n",
        "  * Hamming-loss\n",
        "  * Exact match-ratio (subset accuracy) in sklearn è accuracy_score\n",
        "* Provare diverse strutture (almeno 2/3)\n",
        "* Provare qualche ottimizzatore diverso (Adam e SGD)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vn5opYE0nicG",
        "colab_type": "text"
      },
      "source": [
        "## Prova image da txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Yhp0cdLnlk5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PATH = PATH_TRAIN \n",
        "\n",
        "PATH_MAIN = PATH + 'Main/'\n",
        "\n",
        "PATH_IMAGES = PATH + 'Images/'\n",
        "\n",
        "labels = ['filename', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', \n",
        "          'car', 'cat', 'chair', 'cow', 'diningtable',\n",
        "          'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa',\n",
        "          'train', 'tvmonitor']\n",
        "\n",
        "train = pd.DataFrame(columns=labels)\n",
        "val = pd.DataFrame(columns=labels)\n",
        "\n",
        "# read all id of train and val\n",
        "rows_train = []\n",
        "f = open(PATH_MAIN + 'train.txt', \"r\")\n",
        "for line in f.readlines():\n",
        "  rows_train.append(line.split('\\n')[0] + '.jpg')\n",
        "\n",
        "rows_val = []\n",
        "f = open(PATH_MAIN + 'val.txt', \"r\")\n",
        "for line in f.readlines():\n",
        "  rows_val.append(line.split('\\n')[0] + '.jpg')\n",
        "\n",
        "# initialize all row of dataframe to 0\n",
        "train = train.append(pd.DataFrame(rows_train, columns=['filename']), ignore_index=True)\n",
        "val = val.append(pd.DataFrame(rows_val, columns=['filename']), ignore_index=True)\n",
        "\n",
        "for col in train.columns:\n",
        "    if col != 'filename':\n",
        "      train[col].values[:] = 0\n",
        "\n",
        "for col in val.columns:\n",
        "    if col != 'filename':\n",
        "      val[col].values[:] = 0\n",
        "\n",
        "# read txt in main and set 1 if there is that class in image\n",
        "# if is -1 set to 0\n",
        "for filename in os.listdir(PATH_MAIN):\n",
        "\n",
        "    if filename.endswith(\"_train.txt\"): \n",
        "\n",
        "      label = filename.split('_train.txt')[0]\n",
        "      f = open(PATH_MAIN + filename, \"r\")\n",
        "\n",
        "      for line in f.readlines():\n",
        "        image = line.split(' ')[0] + '.jpg'\n",
        "        presence = line.split(' ')[-1].strip()\n",
        "        if presence == '1':\n",
        "          train.loc[train['filename'] == image, label] = int(presence)\n",
        "\n",
        "    elif filename.endswith(\"_val.txt\"):\n",
        "\n",
        "      label = filename.split('_val.txt')[0]\n",
        "      f = open(PATH_MAIN + filename, \"r\")\n",
        "\n",
        "      for line in f.readlines():\n",
        "        image = line.split(' ')[0] + '.jpg'\n",
        "        presence = line.split(' ')[-1].strip()\n",
        "        if presence == '1':\n",
        "          val.loc[val['filename'] == image, label] = int(presence)\n",
        "\n",
        "train.to_csv(PATH_ABSOLUTE + 'train.csv')\n",
        "val.to_csv(PATH_ABSOLUTE + 'val.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2JifL2F4m7H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train = pd.read_csv(PATH_ABSOLUTE + 'train.csv', index_col=0)\n",
        "val = pd.read_csv(PATH_ABSOLUTE + 'val.csv', index_col=0)\n",
        "\n",
        "train_val = pd.concat([train, val])\n",
        "\n",
        "x_train, x_val, y_train, y_val = train_test_split(  train_val['filename'], train_val[labels[1:]], \n",
        "                                                    test_size=0.2, random_state=42)\n",
        "train_only = pd.concat([x_train, y_train], axis=1)\n",
        "val_only = pd.concat([x_val, y_val], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xR_-BJyEVVw6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calcualate class weights\n",
        "train_only.head()\n",
        "\n",
        "# frequency of labels\n",
        "freq = train_only.apply(pd.value_counts).iloc[1,1:]\n",
        "\n",
        "min_count = freq.min()\n",
        "\n",
        "# between 0 and 1\n",
        "freq_balance = freq.apply(lambda x: min_count/x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyRmCFAN4utr",
        "colab_type": "code",
        "outputId": "a2897578-e0ec-4d20-e869-a97fb6332198",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "from keras.applications.mobilenet import preprocess_input as preprocess_input_mobilenet\n",
        "from keras.applications.vgg16 import preprocess_input as preprocess_input_vgg16\n",
        "from keras.applications.resnet50 import preprocess_input as preprocess_input_resnet50\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "IM_SIZE = (224, 224)\n",
        "\n",
        "img_gen_train = ImageDataGenerator( rescale=1/255,\n",
        "                                    preprocessing_function=preprocess_input_vgg16,\n",
        "                                    featurewise_center=True,\n",
        "                                    featurewise_std_normalization=True,\n",
        "                                    rotation_range=20,\n",
        "                                    width_shift_range=0.2,\n",
        "                                    height_shift_range=0.2,\n",
        "                                    horizontal_flip=True\n",
        "                                  )\n",
        "img_gen_val = ImageDataGenerator( rescale=1/255,\n",
        "                                  preprocessing_function=preprocess_input_vgg16,\n",
        "                                  featurewise_center=True,\n",
        "                                  featurewise_std_normalization=True\n",
        "                                )\n",
        "img_gen_train.mean = np.array([0.4589, 0.4355, 0.4032], dtype=np.float32).reshape((1,1,3)) \n",
        "img_gen_train.std = np.array([0.2239, 0.2186, 0.2206], dtype=np.float32).reshape((1,1,3))\n",
        "img_gen_val.mean = np.array([0.4589, 0.4355, 0.4032], dtype=np.float32).reshape((1,1,3)) \n",
        "img_gen_val.std = np.array([0.2239, 0.2186, 0.2206], dtype=np.float32).reshape((1,1,3))\n",
        "\n",
        "train_generator = img_gen_train.flow_from_dataframe(\n",
        "    train_only,\n",
        "    shuffle=True,\n",
        "    directory=Path(PATH_IMAGES),\n",
        "    x_col='filename',\n",
        "    y_col=labels[1:],\n",
        "    class_mode='other',\n",
        "    target_size=IM_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "\n",
        ")\n",
        "'''\n",
        "from skimage import io\n",
        "\n",
        "def imshow(image_RGB):\n",
        "  io.imshow(image_RGB)\n",
        "  io.show()\n",
        "\n",
        "x,y = train_generator.next()\n",
        "\n",
        "for i in range(0,11):\n",
        "    image = x[i]\n",
        "    imshow(image)\n",
        "'''\n",
        "val_generator = img_gen_val.flow_from_dataframe(\n",
        "    val_only,\n",
        "    shuffle=False,\n",
        "    directory=Path(PATH_IMAGES),\n",
        "    x_col='filename',\n",
        "    y_col=labels[1:],\n",
        "    class_mode='other',\n",
        "    target_size=IM_SIZE,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "TRAIN_LEN = train_generator.n\n",
        "VAL_LEN = val_generator.n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 9232 validated image filenames.\n",
            "Found 2308 validated image filenames.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkb1xEa8CGmo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def macro_soft_f1(y, y_hat):\n",
        "    y = tf.cast(y, tf.float32)\n",
        "    y_hat = tf.cast(y_hat, tf.float32)\n",
        "    tp = tf.reduce_sum(y_hat * y, axis=0)\n",
        "    fp = tf.reduce_sum(y_hat * (1 - y), axis=0)\n",
        "    fn = tf.reduce_sum((1 - y_hat) * y, axis=0)\n",
        "    soft_f1 = 2*tp / (2*tp + fn + fp + 1e-16)\n",
        "    cost = 1 - soft_f1 # reduce 1 - soft-f1 in order to increase soft-f1\n",
        "    macro_cost = tf.reduce_mean(cost) # average on all labels\n",
        "    return macro_cost\n",
        "\n",
        "def macro_f1(y, y_hat, thresh=0.5):\n",
        "    y_pred = tf.cast(tf.greater(y_hat, thresh), tf.float32)\n",
        "    tp = tf.cast(tf.math.count_nonzero(y_pred * y, axis=0), tf.float32)\n",
        "    fp = tf.cast(tf.math.count_nonzero(y_pred * (1 - y), axis=0), tf.float32)\n",
        "    fn = tf.cast(tf.math.count_nonzero((1 - y_pred) * y, axis=0), tf.float32)\n",
        "    f1 = 2*tp / (2*tp + fn + fp + 1e-16)\n",
        "    macro_f1 = tf.reduce_mean(f1)\n",
        "    return macro_f1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ei3PMJCD6P04",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install extra_keras_metrics\n",
        "\n",
        "from keras.applications import VGG16, ResNet50, VGG19\n",
        "from keras.layers import GlobalAveragePooling2D\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.metrics import average_precision_score\n",
        "#from extra_keras_metrics import \n",
        "\n",
        "base_model_resnet50 = ResNet50(weights = 'imagenet', input_shape=(224,224,3), include_top = False)\n",
        "\n",
        "for layer in base_model_resnet50.layers:\n",
        "    layer.trainable=False\n",
        "\n",
        "x = base_model_resnet50.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "predictions = Dense(20, activation = 'sigmoid')(x)\n",
        "model = Model(input = base_model_resnet50.input, output = predictions)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[macro_f1])\n",
        "\n",
        "# callbacks\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=2, verbose=1)\n",
        "#reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
        "                              #patience=2, min_lr=0.000001, verbose=1)\n",
        "#metrics = Metrics(val_generator, ceil(VAL_LEN/BATCH_SIZE))\n",
        "net_history = model.fit_generator(train_generator, epochs=50, verbose=1,\n",
        "                  validation_data = val_generator,\n",
        "                  steps_per_epoch = ceil(TRAIN_LEN/BATCH_SIZE),\n",
        "                  validation_steps = ceil(VAL_LEN/BATCH_SIZE),\n",
        "                  callbacks = [early_stop],\n",
        "                  workers = 8,\n",
        "                  use_multiprocessing=True,\n",
        "                  class_weight = freq_balance\n",
        "              )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TIUCoqGzKEz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.style as style\n",
        "\n",
        "def learning_curves(history):\n",
        "    \"\"\"Plot the learning curves of loss and macro f1 score \n",
        "    for the training and validation datasets.\n",
        "    \n",
        "    Args:\n",
        "        history: history callback of fitting a tensorflow keras model \n",
        "    \"\"\"\n",
        "    \n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "\n",
        "    macro_f1 = history.history['macro_f1']\n",
        "    val_macro_f1 = history.history['val_macro_f1']\n",
        "    \n",
        "    epochs = len(loss)\n",
        "\n",
        "    style.use(\"bmh\")\n",
        "    plt.figure(figsize=(8, 8))\n",
        "\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.plot(range(1, epochs+1), loss, label='Training Loss')\n",
        "    plt.plot(range(1, epochs+1), val_loss, label='Validation Loss')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.plot(range(1, epochs+1), macro_f1, label='Training Macro F1-score')\n",
        "    plt.plot(range(1, epochs+1), val_macro_f1, label='Validation Macro F1-score')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.ylabel('Macro F1-score')\n",
        "    plt.title('Training and Validation Macro F1-score')\n",
        "    plt.xlabel('epoch')\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3kzSkh2zSIk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_curves(net_history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvytHhxwvm_G",
        "colab_type": "text"
      },
      "source": [
        "## TODO\n",
        "* calcolare pesi delle classi sull'intero dataset prima di splittare\n",
        "* provare a implementare la average precision come metrica"
      ]
    }
  ]
}