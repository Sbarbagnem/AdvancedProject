{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ProgettoAdvancedMachineLearning.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "pPBcZ9Sq95Gt"
      ],
      "authorship_tag": "ABX9TyPNAgSsVCXQSsct3YmwOTyD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sbarbagnem/AdvancedProject/blob/master/ProgettoAdvancedMachineLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJ8c2ioz09L5",
        "colab_type": "text"
      },
      "source": [
        "#Multi-label object classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMoW7MM81Gs9",
        "colab_type": "text"
      },
      "source": [
        "## Componenti del gruppo\n",
        "* Carta Costantino\n",
        "* Hamrani Hamza\n",
        "* Ventura Samuele 793060"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOajSqEU50XR",
        "colab_type": "text"
      },
      "source": [
        "## Module Import\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRj2H2n953x_",
        "colab_type": "code",
        "outputId": "2d7c6d0e-62f2-4688-faab-63f0c60fca32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        }
      },
      "source": [
        "from google.colab import drive\n",
        "import xml.etree.ElementTree as ET\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import time\n",
        "import os\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, hamming_loss, accuracy_score\n",
        "import numpy as np\n",
        "from ast import literal_eval\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Dropout, Flatten, BatchNormalization, AveragePooling2D\n",
        "from keras.optimizers import SGD\n",
        "from math import ceil\n",
        "from keras.callbacks import EarlyStopping, Callback\n",
        "from keras import backend as K\n",
        "from itertools import tee  \n",
        "from sklearn import metrics\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.metrics import classification_report\n",
        "from keras.applications.vgg16 import preprocess_input, VGG16 \n",
        "import tensorflow as tf\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from keras import layers, models, optimizers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XB7vfL2kF1xv",
        "colab_type": "text"
      },
      "source": [
        "## Costants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79bHV0fG1iXZ",
        "colab_type": "code",
        "outputId": "1bbf9872-44ce-4877-c74a-11b27323e220",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEliLAW0F5nw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PATH_ABSOLUTE = '/content/drive/My Drive/Laurea Magistrale/Adavanched Machine Learning/Progetto/Dataset/'\n",
        "\n",
        "PATH_TRAIN = PATH_ABSOLUTE + 'Train/'\n",
        "PATH_TEST = PATH_ABSOLUTE + 'Test/'\n",
        "\n",
        "PATH_ANNOTATIONS_TRAIN =  PATH_TRAIN + 'Annotations'\n",
        "PATH_IMAGES_TRAIN = PATH_TRAIN + 'Images'\n",
        "\n",
        "PATH_MAIN_VAL = PATH_TRAIN + 'Main/val.txt'\n",
        "\n",
        "PATH_ANNOTATIONS_TEST =  PATH_TEST + 'Annotations'\n",
        "PATH_IMAGES_TEST = PATH_TEST + 'Images'\n",
        "\n",
        "PATH = PATH_TRAIN \n",
        "\n",
        "PATH_MAIN = PATH + 'Main/'\n",
        "\n",
        "PATH_IMAGES = PATH + 'Images/'\n",
        "\n",
        "PATH_CSV_IMAGE_LABEL_TRAIN_VAL = PATH_ABSOLUTE + 'image_label_train_val.csv'\n",
        "PATH_CSV_IMAGE_LABEL_TRAIN = PATH_ABSOLUTE + 'image_label_train.csv'\n",
        "PATH_CSV_IMAGE_LABEL_VAL = PATH_ABSOLUTE + 'image_label_val.csv'\n",
        "PATH_CSV_IMAGE_LABEL_TEST = PATH_ABSOLUTE + 'image_label_test.csv'\n",
        "\n",
        "labels = ['filename', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', \n",
        "          'car', 'cat', 'chair', 'cow', 'diningtable',\n",
        "          'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa',\n",
        "          'train', 'tvmonitor']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vn5opYE0nicG",
        "colab_type": "text"
      },
      "source": [
        "## Creo dataframe da .txt in .../main (da fare solo prima volta)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Yhp0cdLnlk5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.DataFrame(columns=labels)\n",
        "val = pd.DataFrame(columns=labels)\n",
        "\n",
        "# read all id of train and val\n",
        "rows_train = []\n",
        "f = open(PATH_MAIN + 'train.txt', \"r\")\n",
        "for line in f.readlines():\n",
        "  rows_train.append(line.split('\\n')[0] + '.jpg')\n",
        "\n",
        "rows_val = []\n",
        "f = open(PATH_MAIN + 'val.txt', \"r\")\n",
        "for line in f.readlines():\n",
        "  rows_val.append(line.split('\\n')[0] + '.jpg')\n",
        "\n",
        "# initialize all row of dataframe to 0\n",
        "train = train.append(pd.DataFrame(rows_train, columns=['filename']), ignore_index=True)\n",
        "val = val.append(pd.DataFrame(rows_val, columns=['filename']), ignore_index=True)\n",
        "\n",
        "for col in train.columns:\n",
        "    if col != 'filename':\n",
        "      train[col].values[:] = 0\n",
        "\n",
        "for col in val.columns:\n",
        "    if col != 'filename':\n",
        "      val[col].values[:] = 0\n",
        "\n",
        "# read txt in main and set 1 if there is that class in image\n",
        "# if is -1 set to 0\n",
        "for filename in os.listdir(PATH_MAIN):\n",
        "\n",
        "    if filename.endswith(\"_train.txt\"): \n",
        "\n",
        "      label = filename.split('_train.txt')[0]\n",
        "      f = open(PATH_MAIN + filename, \"r\")\n",
        "\n",
        "      for line in f.readlines():\n",
        "        image = line.split(' ')[0] + '.jpg'\n",
        "        presence = line.split(' ')[-1].strip()\n",
        "        if presence == '1':\n",
        "          train.loc[train['filename'] == image, label] = int(presence)\n",
        "\n",
        "    elif filename.endswith(\"_val.txt\"):\n",
        "\n",
        "      label = filename.split('_val.txt')[0]\n",
        "      f = open(PATH_MAIN + filename, \"r\")\n",
        "\n",
        "      for line in f.readlines():\n",
        "        image = line.split(' ')[0] + '.jpg'\n",
        "        presence = line.split(' ')[-1].strip()\n",
        "        if presence == '1':\n",
        "          val.loc[val['filename'] == image, label] = int(presence)\n",
        "\n",
        "train.to_csv(PATH_ABSOLUTE + 'train.csv')\n",
        "val.to_csv(PATH_ABSOLUTE + 'val.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1VtU9XY9ncu",
        "colab_type": "text"
      },
      "source": [
        "## Lettura da csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2JifL2F4m7H",
        "colab_type": "code",
        "outputId": "1b2cbc58-63ac-4128-8c9c-a5466f6d282b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train = pd.read_csv(PATH_ABSOLUTE + 'train.csv', index_col=0)\n",
        "val = pd.read_csv(PATH_ABSOLUTE + 'val.csv', index_col=0)\n",
        "'''\n",
        "train_val = pd.concat([train, val])\n",
        "\n",
        "x_train, x_val, y_train, y_val = train_test_split(  train_val['filename'], train_val[labels[1:]], \n",
        "                                                    test_size=0.2, random_state=42)\n",
        "train_only = pd.concat([x_train, y_train], axis=1)\n",
        "val_only = pd.concat([x_val, y_val], axis=1)\n",
        "'''\n",
        "# uso train come dato nel dataset\n",
        "train_only = train\n",
        "\n",
        "# divido validation in val e test\n",
        "x_val, x_test, y_val, y_test = train_test_split(  val['filename'], val[labels[1:]], \n",
        "                                                  test_size=0.5, random_state=42)\n",
        "val_only = pd.concat([x_val, y_val], axis=1)\n",
        "test_only = pd.concat([x_test, y_test], axis=1)\n",
        "\n",
        "print('Immagini di train: ', train_only.shape[0])\n",
        "print('Immagini di validation: ', val_only.shape[0])\n",
        "print('Immagini di test:', test_only.shape[0])\n",
        "\n",
        "# calcolo frequenze label in train, test, validation\n",
        "temp_train = train_only.drop(['filename'], axis=1)\n",
        "temp_val = val_only.drop(['filename'], axis=1)\n",
        "temp_test = test_only.drop(['filename'], axis=1)\n",
        "freq_train = temp_train.apply(pd.value_counts).iloc[1,:]\n",
        "freq_val = temp_val.apply(pd.value_counts).iloc[1,:]\n",
        "freq_test = temp_test.apply(pd.value_counts).iloc[1,:]\n",
        "\n",
        "print('Frequenze classi in train:')\n",
        "print(freq_train)\n",
        "print('Frequenze classi in val:')\n",
        "print(freq_val)\n",
        "print('Frequenze classi in test:')\n",
        "print(freq_test)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Immagini di train:  5717\n",
            "Immagini di validation:  2911\n",
            "Immagini di test: 2912\n",
            "Frequenze classi in train:\n",
            "aeroplane       327\n",
            "bicycle         268\n",
            "bird            395\n",
            "boat            260\n",
            "bottle          365\n",
            "bus             213\n",
            "car             590\n",
            "cat             539\n",
            "chair           566\n",
            "cow             151\n",
            "diningtable     269\n",
            "dog             632\n",
            "horse           237\n",
            "motorbike       265\n",
            "person         1994\n",
            "pottedplant     269\n",
            "sheep           171\n",
            "sofa            257\n",
            "train           273\n",
            "tvmonitor       290\n",
            "Name: 1, dtype: int64\n",
            "Frequenze classi in val:\n",
            "aeroplane       178\n",
            "bicycle         133\n",
            "bird            190\n",
            "boat            111\n",
            "bottle          179\n",
            "bus             106\n",
            "car             280\n",
            "cat             258\n",
            "chair           286\n",
            "cow              74\n",
            "diningtable     139\n",
            "dog             340\n",
            "horse           128\n",
            "motorbike       132\n",
            "person         1063\n",
            "pottedplant     148\n",
            "sheep            81\n",
            "sofa            123\n",
            "train           147\n",
            "tvmonitor       130\n",
            "Name: 1, dtype: int64\n",
            "Frequenze classi in test:\n",
            "aeroplane       165\n",
            "bicycle         151\n",
            "bird            180\n",
            "boat            137\n",
            "bottle          162\n",
            "bus             102\n",
            "car             291\n",
            "cat             283\n",
            "chair           267\n",
            "cow              78\n",
            "diningtable     130\n",
            "dog             314\n",
            "horse           117\n",
            "motorbike       129\n",
            "person         1030\n",
            "pottedplant     110\n",
            "sheep            73\n",
            "sofa            127\n",
            "train           124\n",
            "tvmonitor       155\n",
            "Name: 1, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nONTkcJu9vys",
        "colab_type": "text"
      },
      "source": [
        "## Calcolo pesi delle diverse classi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xR_-BJyEVVw6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "temp = train_only.drop(['filename'], axis=1)\n",
        "\n",
        "# frequency of labels\n",
        "freq = temp.apply(pd.value_counts).iloc[1,:]\n",
        "\n",
        "min_count = freq.min()\n",
        "\n",
        "# between 0 and 1\n",
        "freq_balance = freq.apply(lambda x: min_count/x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gr3cxO0590K7",
        "colab_type": "text"
      },
      "source": [
        "## Image generator con data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyRmCFAN4utr",
        "colab_type": "code",
        "outputId": "56c52057-0ad3-4619-990c-b97a7a397fa1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from keras.applications.mobilenet import preprocess_input as preprocess_input_mobilenet\n",
        "from keras.applications.vgg16 import preprocess_input as preprocess_input_vgg16\n",
        "from keras.applications.resnet50 import preprocess_input as preprocess_input_resnet50\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "IM_SIZE = (224, 224)\n",
        "\n",
        "img_gen_train = ImageDataGenerator( rescale=1/255,\n",
        "                                    preprocessing_function=preprocess_input_resnet50,\n",
        "                                    #featurewise_center=True,\n",
        "                                    #featurewise_std_normalization=True,\n",
        "                                    #rotation_range=20,\n",
        "                                    #width_shift_range=0.2,\n",
        "                                    #height_shift_range=0.2,\n",
        "                                    #horizontal_flip=True\n",
        "                                  )\n",
        "img_gen_val = ImageDataGenerator( rescale=1/255,\n",
        "                                  preprocessing_function=preprocess_input_resnet50,\n",
        "                                  #featurewise_center=True,\n",
        "                                  #featurewise_std_normalization=True\n",
        "                                )\n",
        "#img_gen_train.mean = np.array([0.4589, 0.4355, 0.4032], dtype=np.float32).reshape((1,1,3)) \n",
        "#img_gen_train.std = np.array([0.2239, 0.2186, 0.2206], dtype=np.float32).reshape((1,1,3))\n",
        "#img_gen_val.mean = np.array([0.4589, 0.4355, 0.4032], dtype=np.float32).reshape((1,1,3)) \n",
        "#img_gen_val.std = np.array([0.2239, 0.2186, 0.2206], dtype=np.float32).reshape((1,1,3))\n",
        "\n",
        "train_generator = img_gen_train.flow_from_dataframe(\n",
        "    train_only,\n",
        "    shuffle=True,\n",
        "    directory=Path(PATH_IMAGES),\n",
        "    x_col='filename',\n",
        "    y_col=labels[1:],\n",
        "    class_mode='other',\n",
        "    target_size=IM_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "\n",
        ")\n",
        "\n",
        "val_generator = img_gen_val.flow_from_dataframe(\n",
        "    val_only,\n",
        "    shuffle=False,\n",
        "    directory=Path(PATH_IMAGES),\n",
        "    x_col='filename',\n",
        "    y_col=labels[1:],\n",
        "    class_mode='other',\n",
        "    target_size=IM_SIZE,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "TRAIN_LEN = train_generator.n\n",
        "VAL_LEN = val_generator.n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 5715 validated image filenames.\n",
            "Found 2911 validated image filenames.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPBcZ9Sq95Gt",
        "colab_type": "text"
      },
      "source": [
        "## Metrica F1 e loss F1 on batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkb1xEa8CGmo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def macro_soft_f1(y, y_hat):\n",
        "    y = tf.cast(y, tf.float32)\n",
        "    y_hat = tf.cast(y_hat, tf.float32)\n",
        "    tp = tf.reduce_sum(y_hat * y, axis=0)\n",
        "    fp = tf.reduce_sum(y_hat * (1 - y), axis=0)\n",
        "    fn = tf.reduce_sum((1 - y_hat) * y, axis=0)\n",
        "    soft_f1 = 2*tp / (2*tp + fn + fp + 1e-16)\n",
        "    cost = 1 - soft_f1 # reduce 1 - soft-f1 in order to increase soft-f1\n",
        "    macro_cost = tf.reduce_mean(cost) # average on all labels\n",
        "    return macro_cost\n",
        "\n",
        "def macro_f1(y, y_hat, thresh=0.5):\n",
        "    y_pred = tf.cast(tf.greater(y_hat, thresh), tf.float32)\n",
        "    tp = tf.cast(tf.math.count_nonzero(y_pred * y, axis=0), tf.float32)\n",
        "    fp = tf.cast(tf.math.count_nonzero(y_pred * (1 - y), axis=0), tf.float32)\n",
        "    fn = tf.cast(tf.math.count_nonzero((1 - y_pred) * y, axis=0), tf.float32)\n",
        "    f1 = 2*tp / (2*tp + fn + fp + 1e-16)\n",
        "    macro_f1 = tf.reduce_mean(f1)\n",
        "    return macro_f1\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    def recall(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        return recall\n",
        "\n",
        "    def precision(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "    precision = precision(y_true, y_pred)\n",
        "    recall = recall(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon())) \n",
        "\n",
        "def mAP(y_true, y_pred):\n",
        "\n",
        "  y_true = tf.cast(y_true, tf.float32)\n",
        "  y_pred = tf.cast(tf.greater(y_pred, 0.5), tf.float32)\n",
        "\n",
        "  y_true = tf.keras.backend.eval(y_true)\n",
        "  y_pred = tf.keras.backend.eval(y_pred)\n",
        "\n",
        "  _, classes = y_true.shape\n",
        "      \n",
        "  average_precisions = []\n",
        "\n",
        "  for index in range(classes):\n",
        "          row_indices_sorted = np.argsort(-y_pred[:, index])\n",
        "\n",
        "          y_true_cls = y_true[row_indices_sorted, index]\n",
        "          y_pred_cls = y_pred[row_indices_sorted, index]\n",
        "\n",
        "          tp = (y_true_cls == 1)\n",
        "          fp = (y_true_cls == 0)\n",
        "\n",
        "          fp = np.cumsum(fp)\n",
        "          tp = np.cumsum(tp)\n",
        "\n",
        "          npos = np.sum(y_true_cls)\n",
        "\n",
        "          rec = tp*1.0 / npos\n",
        "\n",
        "          # avoid divide by zero in case the first detection matches a difficult\n",
        "          # ground truth\n",
        "          prec = tp*1.0 / np.maximum((tp + fp), np.finfo(np.float64).eps)\n",
        "\n",
        "          mrec = np.concatenate(([0.], rec, [1.]))\n",
        "          mpre = np.concatenate(([0.], prec, [0.]))\n",
        "\n",
        "          # compute the precision envelope\n",
        "          for i in range(mpre.size - 1, 0, -1):\n",
        "              mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
        "\n",
        "          # to calculate area under PR curve, look for points\n",
        "          # where X axis (recall) changes value\n",
        "          i = np.where(mrec[1:] != mrec[:-1])[0]\n",
        "\n",
        "          # and sum (\\Delta recall) * prec\n",
        "          average_precisions.append(np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1]))\n",
        "\n",
        "  mAP = np.mean(average_precisions)\n",
        "  \n",
        "  return mAP"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "154dmnYREYiD",
        "colab_type": "text"
      },
      "source": [
        "## Metrica F1 on epoch end"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcoENTePEc_Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Metrics(Callback):\n",
        "  def __init__(self, validation_generator, validation_steps, threshold=0.5):\n",
        "    self.validation_generator = validation_generator\n",
        "    self.validation_steps = validation_steps\n",
        "    self.threshold = threshold\n",
        "\n",
        "  def on_train_begin(self, logs={}):\n",
        "    self.val_f1_scores = []\n",
        "    self.val_recalls = []\n",
        "    self.val_precisions = []\n",
        "    \n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    gen_1, gen_2 = tee(self.validation_generator)\n",
        "    y_true = np.vstack(next(self.validation_generator)[1] for _ in range(self.validation_steps)).astype('int')\n",
        "    y_pred = (self.model.predict_generator(self.validation_generator, steps=self.validation_steps) > self.threshold).astype('int')\n",
        "    f1 = f1_score(y_true, y_pred, average='macro')\n",
        "    precision = precision_score(y_true, y_pred, average='macro')\n",
        "    recall = recall_score(y_true, y_pred, average='macro')\n",
        "    self.val_f1_scores.append(f1)\n",
        "    self.val_recalls.append(recall)\n",
        "    self.val_precisions.append(precision)\n",
        "    print(f\"- val_f1: {f1:.5f} - val_precision: {precision:.5f} - val_recall: {recall:.5f}\")\n",
        "    return\n",
        "\n",
        "metrics = Metrics(val_generator, ceil(VAL_LEN/BATCH_SIZE))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vPO9KOe9_7k",
        "colab_type": "text"
      },
      "source": [
        "## Modello con transfer learning da ResNet50"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ei3PMJCD6P04",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install extra_keras_metrics\n",
        "\n",
        "from keras.applications import VGG16, ResNet50, VGG19\n",
        "from keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.metrics import average_precision_score\n",
        "#from extra_keras_metrics import \n",
        "\n",
        "base_model_resnet50 = ResNet50(weights = 'imagenet', input_shape=(224,224,3), include_top = False)\n",
        "\n",
        "for layer in base_model_resnet50.layers:\n",
        "    layer.trainable=False\n",
        "\n",
        "x = base_model_resnet50.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = Dropout(0.3)(x)\n",
        "predictions = Dense(20, activation = 'sigmoid')(x)\n",
        "model = Model(input = base_model_resnet50.input, output = predictions)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "\n",
        "# callbacks\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=2, verbose=1)\n",
        "\n",
        "net_history = model.fit_generator(train_generator, epochs=50, verbose=1,\n",
        "                  validation_data = val_generator,\n",
        "                  steps_per_epoch = ceil(TRAIN_LEN/BATCH_SIZE),\n",
        "                  validation_steps = ceil(VAL_LEN/BATCH_SIZE),\n",
        "                  callbacks = [early_stop, metrics],\n",
        "                  workers = 8,\n",
        "                  use_multiprocessing=True,\n",
        "                  class_weight = freq_balance\n",
        "              )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vyqhz79s-F20",
        "colab_type": "text"
      },
      "source": [
        "## Plot epoche"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TIUCoqGzKEz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.style as style\n",
        "\n",
        "def learning_curves(history):\n",
        "    \"\"\"Plot the learning curves of loss and macro f1 score \n",
        "    for the training and validation datasets.\n",
        "    \n",
        "    Args:\n",
        "        history: history callback of fitting a tensorflow keras model \n",
        "    \"\"\"\n",
        "    \n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "\n",
        "    macro_f1 = history.history['macro_f1']\n",
        "    val_macro_f1 = history.history['val_macro_f1']\n",
        "    \n",
        "    epochs = len(loss)\n",
        "\n",
        "    style.use(\"bmh\")\n",
        "    plt.figure(figsize=(8, 8))\n",
        "\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.plot(range(1, epochs+1), loss, label='Training Loss')\n",
        "    plt.plot(range(1, epochs+1), val_loss, label='Validation Loss')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.plot(range(1, epochs+1), macro_f1, label='Training Macro F1-score')\n",
        "    plt.plot(range(1, epochs+1), val_macro_f1, label='Validation Macro F1-score')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.ylabel('Macro F1-score')\n",
        "    plt.title('Training and Validation Macro F1-score')\n",
        "    plt.xlabel('epoch')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "learning_curves(net_history)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}