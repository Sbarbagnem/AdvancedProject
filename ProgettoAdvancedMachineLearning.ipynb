{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ProgettoAdvancedMachineLearning.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "sOajSqEU50XR",
        "XB7vfL2kF1xv",
        "blBDHXrL1a7H",
        "pJF4__P8R3pp",
        "CuCfEGYxR5uT",
        "Vn5opYE0nicG"
      ],
      "authorship_tag": "ABX9TyPAwDr5xzogMhQl2jOeCfPN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sbarbagnem/AdvancedProject/blob/master/ProgettoAdvancedMachineLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJ8c2ioz09L5",
        "colab_type": "text"
      },
      "source": [
        "#Multi-label object classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMoW7MM81Gs9",
        "colab_type": "text"
      },
      "source": [
        "## Componenti del gruppo\n",
        "* Carta Costantino\n",
        "* Hamrani Hamza\n",
        "* Ventura Samuele 793060"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOajSqEU50XR",
        "colab_type": "text"
      },
      "source": [
        "## Module Import\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRj2H2n953x_",
        "colab_type": "code",
        "outputId": "f63a69ef-5b96-46b8-a1ab-f89da2928220",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        }
      },
      "source": [
        "from google.colab import drive\n",
        "import xml.etree.ElementTree as ET\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import time\n",
        "import os\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, hamming_loss, accuracy_score\n",
        "import numpy as np\n",
        "from ast import literal_eval\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Dropout, Flatten, BatchNormalization, AveragePooling2D\n",
        "from keras.optimizers import SGD\n",
        "from math import ceil\n",
        "from keras.callbacks import EarlyStopping, Callback\n",
        "from keras import backend as K\n",
        "from itertools import tee  # finally! I found something useful for it\n",
        "from sklearn import metrics\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.metrics import classification_report\n",
        "from keras.applications.vgg16 import preprocess_input, VGG16 \n",
        "import tensorflow as tf\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from keras import layers, models, optimizers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XB7vfL2kF1xv",
        "colab_type": "text"
      },
      "source": [
        "## Costants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79bHV0fG1iXZ",
        "colab_type": "code",
        "outputId": "5379e0df-d779-4f49-dc5a-cf8537d199ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEliLAW0F5nw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PATH_ABSOLUTE = '/content/drive/My Drive/Laurea Magistrale/Adavanched Machine Learning/Progetto/Dataset/'\n",
        "\n",
        "PATH_TRAIN = PATH_ABSOLUTE + 'Train/'\n",
        "PATH_TEST = PATH_ABSOLUTE + 'Test/'\n",
        "\n",
        "PATH_ANNOTATIONS_TRAIN =  PATH_TRAIN + 'Annotations'\n",
        "PATH_IMAGES_TRAIN = PATH_TRAIN + 'Images'\n",
        "\n",
        "PATH_MAIN_VAL = PATH_TRAIN + 'Main/val.txt'\n",
        "\n",
        "PATH_ANNOTATIONS_TEST =  PATH_TEST + 'Annotations'\n",
        "PATH_IMAGES_TEST = PATH_TEST + 'Images'\n",
        "\n",
        "PATH = PATH_TRAIN \n",
        "\n",
        "PATH_MAIN = PATH + 'Main/'\n",
        "\n",
        "PATH_IMAGES = PATH + 'Images/'\n",
        "\n",
        "PATH_CSV_IMAGE_LABEL_TRAIN_VAL = PATH_ABSOLUTE + 'image_label_train_val.csv'\n",
        "PATH_CSV_IMAGE_LABEL_TRAIN = PATH_ABSOLUTE + 'image_label_train.csv'\n",
        "PATH_CSV_IMAGE_LABEL_VAL = PATH_ABSOLUTE + 'image_label_val.csv'\n",
        "PATH_CSV_IMAGE_LABEL_TEST = PATH_ABSOLUTE + 'image_label_test.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vn5opYE0nicG",
        "colab_type": "text"
      },
      "source": [
        "## Creo dataframe da .txt in .../main (da fare solo prima volta)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Yhp0cdLnlk5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = ['filename', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', \n",
        "          'car', 'cat', 'chair', 'cow', 'diningtable',\n",
        "          'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa',\n",
        "          'train', 'tvmonitor']\n",
        "\n",
        "train = pd.DataFrame(columns=labels)\n",
        "val = pd.DataFrame(columns=labels)\n",
        "\n",
        "# read all id of train and val\n",
        "rows_train = []\n",
        "f = open(PATH_MAIN + 'train.txt', \"r\")\n",
        "for line in f.readlines():\n",
        "  rows_train.append(line.split('\\n')[0] + '.jpg')\n",
        "\n",
        "rows_val = []\n",
        "f = open(PATH_MAIN + 'val.txt', \"r\")\n",
        "for line in f.readlines():\n",
        "  rows_val.append(line.split('\\n')[0] + '.jpg')\n",
        "\n",
        "# initialize all row of dataframe to 0\n",
        "train = train.append(pd.DataFrame(rows_train, columns=['filename']), ignore_index=True)\n",
        "val = val.append(pd.DataFrame(rows_val, columns=['filename']), ignore_index=True)\n",
        "\n",
        "for col in train.columns:\n",
        "    if col != 'filename':\n",
        "      train[col].values[:] = 0\n",
        "\n",
        "for col in val.columns:\n",
        "    if col != 'filename':\n",
        "      val[col].values[:] = 0\n",
        "\n",
        "# read txt in main and set 1 if there is that class in image\n",
        "# if is -1 set to 0\n",
        "for filename in os.listdir(PATH_MAIN):\n",
        "\n",
        "    if filename.endswith(\"_train.txt\"): \n",
        "\n",
        "      label = filename.split('_train.txt')[0]\n",
        "      f = open(PATH_MAIN + filename, \"r\")\n",
        "\n",
        "      for line in f.readlines():\n",
        "        image = line.split(' ')[0] + '.jpg'\n",
        "        presence = line.split(' ')[-1].strip()\n",
        "        if presence == '1':\n",
        "          train.loc[train['filename'] == image, label] = int(presence)\n",
        "\n",
        "    elif filename.endswith(\"_val.txt\"):\n",
        "\n",
        "      label = filename.split('_val.txt')[0]\n",
        "      f = open(PATH_MAIN + filename, \"r\")\n",
        "\n",
        "      for line in f.readlines():\n",
        "        image = line.split(' ')[0] + '.jpg'\n",
        "        presence = line.split(' ')[-1].strip()\n",
        "        if presence == '1':\n",
        "          val.loc[val['filename'] == image, label] = int(presence)\n",
        "\n",
        "train.to_csv(PATH_ABSOLUTE + 'train.csv')\n",
        "val.to_csv(PATH_ABSOLUTE + 'val.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1VtU9XY9ncu",
        "colab_type": "text"
      },
      "source": [
        "## Lettura da csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2JifL2F4m7H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train = pd.read_csv(PATH_ABSOLUTE + 'train.csv', index_col=0)\n",
        "val = pd.read_csv(PATH_ABSOLUTE + 'val.csv', index_col=0)\n",
        "\n",
        "train_val = pd.concat([train, val])\n",
        "\n",
        "x_train, x_val, y_train, y_val = train_test_split(  train_val['filename'], train_val[labels[1:]], \n",
        "                                                    test_size=0.2, random_state=42)\n",
        "train_only = pd.concat([x_train, y_train], axis=1)\n",
        "val_only = pd.concat([x_val, y_val], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nONTkcJu9vys",
        "colab_type": "text"
      },
      "source": [
        "## Calcolo pesi delle diverse classi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xR_-BJyEVVw6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calcualate class weights\n",
        "train_only.head()\n",
        "\n",
        "# frequency of labels\n",
        "freq = train_only.apply(pd.value_counts).iloc[1,1:]\n",
        "\n",
        "min_count = freq.min()\n",
        "\n",
        "# between 0 and 1\n",
        "freq_balance = freq.apply(lambda x: min_count/x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gr3cxO0590K7",
        "colab_type": "text"
      },
      "source": [
        "## Image generator con data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyRmCFAN4utr",
        "colab_type": "code",
        "outputId": "a2897578-e0ec-4d20-e869-a97fb6332198",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "from keras.applications.mobilenet import preprocess_input as preprocess_input_mobilenet\n",
        "from keras.applications.vgg16 import preprocess_input as preprocess_input_vgg16\n",
        "from keras.applications.resnet50 import preprocess_input as preprocess_input_resnet50\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "IM_SIZE = (224, 224)\n",
        "\n",
        "img_gen_train = ImageDataGenerator( rescale=1/255,\n",
        "                                    preprocessing_function=preprocess_input_vgg16,\n",
        "                                    featurewise_center=True,\n",
        "                                    featurewise_std_normalization=True,\n",
        "                                    rotation_range=20,\n",
        "                                    width_shift_range=0.2,\n",
        "                                    height_shift_range=0.2,\n",
        "                                    horizontal_flip=True\n",
        "                                  )\n",
        "img_gen_val = ImageDataGenerator( rescale=1/255,\n",
        "                                  preprocessing_function=preprocess_input_vgg16,\n",
        "                                  featurewise_center=True,\n",
        "                                  featurewise_std_normalization=True\n",
        "                                )\n",
        "img_gen_train.mean = np.array([0.4589, 0.4355, 0.4032], dtype=np.float32).reshape((1,1,3)) \n",
        "img_gen_train.std = np.array([0.2239, 0.2186, 0.2206], dtype=np.float32).reshape((1,1,3))\n",
        "img_gen_val.mean = np.array([0.4589, 0.4355, 0.4032], dtype=np.float32).reshape((1,1,3)) \n",
        "img_gen_val.std = np.array([0.2239, 0.2186, 0.2206], dtype=np.float32).reshape((1,1,3))\n",
        "\n",
        "train_generator = img_gen_train.flow_from_dataframe(\n",
        "    train_only,\n",
        "    shuffle=True,\n",
        "    directory=Path(PATH_IMAGES),\n",
        "    x_col='filename',\n",
        "    y_col=labels[1:],\n",
        "    class_mode='other',\n",
        "    target_size=IM_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "\n",
        ")\n",
        "\n",
        "val_generator = img_gen_val.flow_from_dataframe(\n",
        "    val_only,\n",
        "    shuffle=False,\n",
        "    directory=Path(PATH_IMAGES),\n",
        "    x_col='filename',\n",
        "    y_col=labels[1:],\n",
        "    class_mode='other',\n",
        "    target_size=IM_SIZE,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "TRAIN_LEN = train_generator.n\n",
        "VAL_LEN = val_generator.n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 9232 validated image filenames.\n",
            "Found 2308 validated image filenames.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPBcZ9Sq95Gt",
        "colab_type": "text"
      },
      "source": [
        "## Metrica F1 e loss F1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkb1xEa8CGmo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def macro_soft_f1(y, y_hat):\n",
        "    y = tf.cast(y, tf.float32)\n",
        "    y_hat = tf.cast(y_hat, tf.float32)\n",
        "    tp = tf.reduce_sum(y_hat * y, axis=0)\n",
        "    fp = tf.reduce_sum(y_hat * (1 - y), axis=0)\n",
        "    fn = tf.reduce_sum((1 - y_hat) * y, axis=0)\n",
        "    soft_f1 = 2*tp / (2*tp + fn + fp + 1e-16)\n",
        "    cost = 1 - soft_f1 # reduce 1 - soft-f1 in order to increase soft-f1\n",
        "    macro_cost = tf.reduce_mean(cost) # average on all labels\n",
        "    return macro_cost\n",
        "\n",
        "def macro_f1(y, y_hat, thresh=0.5):\n",
        "    y_pred = tf.cast(tf.greater(y_hat, thresh), tf.float32)\n",
        "    tp = tf.cast(tf.math.count_nonzero(y_pred * y, axis=0), tf.float32)\n",
        "    fp = tf.cast(tf.math.count_nonzero(y_pred * (1 - y), axis=0), tf.float32)\n",
        "    fn = tf.cast(tf.math.count_nonzero((1 - y_pred) * y, axis=0), tf.float32)\n",
        "    f1 = 2*tp / (2*tp + fn + fp + 1e-16)\n",
        "    macro_f1 = tf.reduce_mean(f1)\n",
        "    return macro_f1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vPO9KOe9_7k",
        "colab_type": "text"
      },
      "source": [
        "## Modello con transfer learning da ResNet50"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ei3PMJCD6P04",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install extra_keras_metrics\n",
        "\n",
        "from keras.applications import VGG16, ResNet50, VGG19\n",
        "from keras.layers import GlobalAveragePooling2D\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.metrics import average_precision_score\n",
        "#from extra_keras_metrics import \n",
        "\n",
        "base_model_resnet50 = ResNet50(weights = 'imagenet', input_shape=(224,224,3), include_top = False)\n",
        "\n",
        "for layer in base_model_resnet50.layers:\n",
        "    layer.trainable=False\n",
        "\n",
        "x = base_model_resnet50.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "predictions = Dense(20, activation = 'sigmoid')(x)\n",
        "model = Model(input = base_model_resnet50.input, output = predictions)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[macro_f1])\n",
        "\n",
        "# callbacks\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=2, verbose=1)\n",
        "\n",
        "net_history = model.fit_generator(train_generator, epochs=50, verbose=1,\n",
        "                  validation_data = val_generator,\n",
        "                  steps_per_epoch = ceil(TRAIN_LEN/BATCH_SIZE),\n",
        "                  validation_steps = ceil(VAL_LEN/BATCH_SIZE),\n",
        "                  callbacks = [early_stop],\n",
        "                  workers = 8,\n",
        "                  use_multiprocessing=True,\n",
        "                  class_weight = freq_balance\n",
        "              )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vyqhz79s-F20",
        "colab_type": "text"
      },
      "source": [
        "## Plot epoche"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TIUCoqGzKEz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.style as style\n",
        "\n",
        "def learning_curves(history):\n",
        "    \"\"\"Plot the learning curves of loss and macro f1 score \n",
        "    for the training and validation datasets.\n",
        "    \n",
        "    Args:\n",
        "        history: history callback of fitting a tensorflow keras model \n",
        "    \"\"\"\n",
        "    \n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "\n",
        "    macro_f1 = history.history['macro_f1']\n",
        "    val_macro_f1 = history.history['val_macro_f1']\n",
        "    \n",
        "    epochs = len(loss)\n",
        "\n",
        "    style.use(\"bmh\")\n",
        "    plt.figure(figsize=(8, 8))\n",
        "\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.plot(range(1, epochs+1), loss, label='Training Loss')\n",
        "    plt.plot(range(1, epochs+1), val_loss, label='Validation Loss')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.plot(range(1, epochs+1), macro_f1, label='Training Macro F1-score')\n",
        "    plt.plot(range(1, epochs+1), val_macro_f1, label='Validation Macro F1-score')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.ylabel('Macro F1-score')\n",
        "    plt.title('Training and Validation Macro F1-score')\n",
        "    plt.xlabel('epoch')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "learning_curves(net_history)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}