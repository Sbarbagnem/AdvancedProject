{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "eD9R4p_JGS8j",
        "bwTcVNKCAoJt",
        "awq2lzbxBXgj",
        "AHm9t-_NGYDa",
        "WgzdNcHZGgch",
        "ISzDSogyQ12u",
        "dDw9W32GaThb",
        "XXwxrNdCShlc",
        "oXNhkv10FLgG",
        "l7fUcTZApdLB",
        "1gpMHcy2SpB7",
        "22ReLsjTEeCZ",
        "se7bKuAJbulU",
        "pNm8VPigbjay",
        "oAg57-_ib21t",
        "Pz6Y1ic80Rz2",
        "8U2BptkC0JcN",
        "mrlxqfdszCbf",
        "-KaAO8omzV73",
        "yJrfI4v50nnV",
        "wTVlUukw3Fcl",
        "rUM7BP6b6U_K",
        "A-ZHDD5C4Ksp",
        "ngqgVtqrIsD-"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sbarbagnem/AdvancedProject/blob/master/Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGjYNEjPFiCE",
        "colab_type": "text"
      },
      "source": [
        "# Module\n",
        "**caricare prima librerie di smac e poi il resto**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_AUBPWzkgSI",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "317bd448-72bb-4cb3-e6e6-a254f7e16aa4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#!pip uninstall scikit-learn -y\n",
        "#!pip install scikit-learn==0.21.3\n",
        "!apt-get install swig\n",
        "!pip install smac\n",
        "!pip install smac[all]\n",
        "!pip install smt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "swig is already the newest version (3.0.12-1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 43 not upgraded.\n",
            "Requirement already satisfied: smac in /usr/local/lib/python3.6/dist-packages (0.12.2)\n",
            "Requirement already satisfied: numpy>=1.7.1 in /usr/local/lib/python3.6/dist-packages (from smac) (1.18.5)\n",
            "Requirement already satisfied: sobol-seq in /usr/local/lib/python3.6/dist-packages (from smac) (0.2.0)\n",
            "Requirement already satisfied: pyrfr>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from smac) (0.8.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (from smac) (5.4.8)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from smac) (1.4.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from smac) (0.15.1)\n",
            "Requirement already satisfied: lazy-import in /usr/local/lib/python3.6/dist-packages (from smac) (0.2.2)\n",
            "Requirement already satisfied: ConfigSpace<0.5,>=0.4.9 in /usr/local/lib/python3.6/dist-packages (from smac) (0.4.13)\n",
            "Requirement already satisfied: scikit-learn>=0.22.0 in /usr/local/lib/python3.6/dist-packages (from smac) (0.22.2.post1)\n",
            "Requirement already satisfied: pynisher>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from smac) (0.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from lazy-import->smac) (1.12.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from ConfigSpace<0.5,>=0.4.9->smac) (0.29.19)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from ConfigSpace<0.5,>=0.4.9->smac) (2.4.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pynisher>=0.4.1->smac) (47.1.1)\n",
            "Requirement already satisfied: docutils>=0.3 in /usr/local/lib/python3.6/dist-packages (from pynisher>=0.4.1->smac) (0.15.2)\n",
            "Requirement already satisfied: smac[all] in /usr/local/lib/python3.6/dist-packages (0.12.2)\n",
            "Requirement already satisfied: pynisher>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from smac[all]) (0.5.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (from smac[all]) (5.4.8)\n",
            "Requirement already satisfied: ConfigSpace<0.5,>=0.4.9 in /usr/local/lib/python3.6/dist-packages (from smac[all]) (0.4.13)\n",
            "Requirement already satisfied: scikit-learn>=0.22.0 in /usr/local/lib/python3.6/dist-packages (from smac[all]) (0.22.2.post1)\n",
            "Requirement already satisfied: pyrfr>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from smac[all]) (0.8.0)\n",
            "Requirement already satisfied: sobol-seq in /usr/local/lib/python3.6/dist-packages (from smac[all]) (0.2.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from smac[all]) (0.15.1)\n",
            "Requirement already satisfied: numpy>=1.7.1 in /usr/local/lib/python3.6/dist-packages (from smac[all]) (1.18.5)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from smac[all]) (1.4.1)\n",
            "Requirement already satisfied: lazy-import in /usr/local/lib/python3.6/dist-packages (from smac[all]) (0.2.2)\n",
            "Requirement already satisfied: sphinx; extra == \"all\" in /usr/local/lib/python3.6/dist-packages (from smac[all]) (1.8.5)\n",
            "Requirement already satisfied: emcee>=2.1.0; extra == \"all\" in /usr/local/lib/python3.6/dist-packages (from smac[all]) (3.0.2)\n",
            "Requirement already satisfied: sphinx-gallery==0.5.0; extra == \"all\" in /usr/local/lib/python3.6/dist-packages (from smac[all]) (0.5.0)\n",
            "Requirement already satisfied: scikit-optimize; extra == \"all\" in /usr/local/lib/python3.6/dist-packages (from smac[all]) (0.7.4)\n",
            "Requirement already satisfied: sphinx-rtd-theme; extra == \"all\" in /usr/local/lib/python3.6/dist-packages (from smac[all]) (0.4.3)\n",
            "Requirement already satisfied: pyDOE; extra == \"all\" in /usr/local/lib/python3.6/dist-packages (from smac[all]) (0.3.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pynisher>=0.4.1->smac[all]) (47.1.1)\n",
            "Requirement already satisfied: docutils>=0.3 in /usr/local/lib/python3.6/dist-packages (from pynisher>=0.4.1->smac[all]) (0.15.2)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from ConfigSpace<0.5,>=0.4.9->smac[all]) (0.29.19)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from ConfigSpace<0.5,>=0.4.9->smac[all]) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from lazy-import->smac[all]) (1.12.0)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx; extra == \"all\"->smac[all]) (1.2.0)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx; extra == \"all\"->smac[all]) (2.1.3)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from sphinx; extra == \"all\"->smac[all]) (2.23.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx; extra == \"all\"->smac[all]) (0.7.12)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.6/dist-packages (from sphinx; extra == \"all\"->smac[all]) (2.11.2)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx; extra == \"all\"->smac[all]) (2.8.0)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx; extra == \"all\"->smac[all]) (1.2.2)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx; extra == \"all\"->smac[all]) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from sphinx; extra == \"all\"->smac[all]) (20.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from sphinx-gallery==0.5.0; extra == \"all\"->smac[all]) (3.2.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from sphinx-gallery==0.5.0; extra == \"all\"->smac[all]) (7.0.0)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize; extra == \"all\"->smac[all]) (20.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx; extra == \"all\"->smac[all]) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx; extra == \"all\"->smac[all]) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx; extra == \"all\"->smac[all]) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx; extra == \"all\"->smac[all]) (1.24.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.3->sphinx; extra == \"all\"->smac[all]) (1.1.1)\n",
            "Requirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.6/dist-packages (from babel!=2.0,>=1.3->sphinx; extra == \"all\"->smac[all]) (2018.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->sphinx-gallery==0.5.0; extra == \"all\"->smac[all]) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->sphinx-gallery==0.5.0; extra == \"all\"->smac[all]) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->sphinx-gallery==0.5.0; extra == \"all\"->smac[all]) (0.10.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from pyaml>=16.9->scikit-optimize; extra == \"all\"->smac[all]) (3.13)\n",
            "Requirement already satisfied: smt in /usr/local/lib/python3.6/dist-packages (0.5.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from smt) (0.22.2.post1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from smt) (3.2.1)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from smt) (1.12.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from smt) (1.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from smt) (20.4)\n",
            "Requirement already satisfied: pyDOE2 in /usr/local/lib/python3.6/dist-packages (from smt) (1.3.0)\n",
            "Requirement already satisfied: numpydoc in /usr/local/lib/python3.6/dist-packages (from smt) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->smt) (1.18.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->smt) (0.15.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->smt) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->smt) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->smt) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->smt) (1.2.0)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.6/dist-packages (from numpydoc->smt) (2.11.2)\n",
            "Requirement already satisfied: sphinx>=1.6.5 in /usr/local/lib/python3.6/dist-packages (from numpydoc->smt) (1.8.5)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.3->numpydoc->smt) (1.1.1)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc->smt) (1.2.2)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc->smt) (2.23.0)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc->smt) (2.8.0)\n",
            "Requirement already satisfied: docutils>=0.11 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc->smt) (0.15.2)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc->smt) (0.7.12)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc->smt) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc->smt) (47.1.1)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc->smt) (2.1.3)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc->smt) (2.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx>=1.6.5->numpydoc->smt) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx>=1.6.5->numpydoc->smt) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx>=1.6.5->numpydoc->smt) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx>=1.6.5->numpydoc->smt) (3.0.4)\n",
            "Requirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.6/dist-packages (from babel!=2.0,>=1.3->sphinx>=1.6.5->numpydoc->smt) (2018.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJr3vEODFZE1",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "f32c7b03-bb4f-40c7-fc2b-b6a1b5fe5c28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from math import ceil\n",
        "import xml.etree.ElementTree as ET\n",
        "from google.colab import drive\n",
        "from google.colab.patches import cv2_imshow\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import gc\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications import VGG16, ResNet50, MobileNet\n",
        "from keras.applications.vgg16 import preprocess_input as preprocess_input_vgg16\n",
        "from keras.applications.resnet50 import preprocess_input as preprocess_input_resnet50\n",
        "from keras.applications.mobilenet import preprocess_input as preprocess_input_mobilenet\n",
        "from keras.layers import Dense, Dropout, GlobalAveragePooling2D, Flatten, BatchNormalization, Activation\n",
        "from keras import Model\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras import backend as K\n",
        "\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8YYvM7UbCMd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from smac.configspace import ConfigurationSpace \n",
        "from smac.configspace import UniformFloatHyperparameter, UniformIntegerHyperparameter, CategoricalHyperparameter, InCondition\n",
        "from smac.configspace import Configuration # serve per costruire le configurazioni iniziali da passare alla funzione di minimizzazione\n",
        "from smac.scenario.scenario import Scenario\n",
        "from smac.facade.smac_bo_facade import SMAC4BO #Â bayesian optimization with GaussianProcess\n",
        "from smac.facade.smac_hpo_facade import SMAC4HPO # bayesian optimization with RandomForest\n",
        "from smac.optimizer.acquisition import EI, LCB, PI\n",
        "from smac.runhistory.runhistory import RunHistory # util class to collect all data of optimization\n",
        "from smac.initial_design.latin_hypercube_design import LHDesign\n",
        "from smac.initial_design.random_configuration_design import RandomConfigurations\n",
        "from smac.stats.stats import Stats # util class to save history on directory\n",
        "from smac.utils.io.traj_logging import TrajLogger\n",
        "#from smac.optimizer.objective import average_cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eD9R4p_JGS8j",
        "colab_type": "text"
      },
      "source": [
        "# Costant"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jui_q9SGGXsi",
        "colab_type": "code",
        "outputId": "6c0c89a5-1a67-4ce8-e04d-506fab0c1734",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "PATH_ANNOTATIONS = '/content/drive/My Drive/Laurea Magistrale/Adavanched Machine Learning/Progetto/Dataset/Train/Annotations/'\n",
        "PATH_MAIN = '/content/drive/My Drive/Laurea Magistrale/Adavanched Machine Learning/Progetto/Dataset/Train/Main/'\n",
        "PATH_IMAGES = '/content/drive/My Drive/Laurea Magistrale/Adavanched Machine Learning/Progetto/Dataset/Train/Images/'\n",
        "PATH_IMAGES_CROPPED_TRAIN = '/content/drive/My Drive/Laurea Magistrale/Adavanched Machine Learning/Progetto/Dataset/Train/Cropped_train/'\n",
        "PATH_IMAGES_CROPPED_VAL_TEST = '/content/drive/My Drive/Laurea Magistrale/Adavanched Machine Learning/Progetto/Dataset/Train/Cropped_val_test/'\n",
        "PATH_IMAGES_CROPPED_TRAIN_BALANCED = '/content/drive/My Drive/Laurea Magistrale/Adavanched Machine Learning/Progetto/Dataset/Train/Cropped_train_balanced/'\n",
        "PATH_IMAGES_CROPPED_VAL_TEST_BALANCED = '/content/drive/My Drive/Laurea Magistrale/Adavanched Machine Learning/Progetto/Dataset/Train/Cropped_val_test_balanced/'\n",
        "PATH_OPTIMIZATION = '/content/drive/My Drive/Laurea Magistrale/Adavanched Machine Learning/Progetto/Ottimizzazione/'\n",
        "PATH_PICKLE = '/content/drive/My Drive/Laurea Magistrale/Adavanched Machine Learning/Progetto/Pickles/'\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "IM_SIZE = (128, 128)\n",
        "\n",
        "LABELS = ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', \n",
        "          'car', 'cat', 'chair', 'cow', 'diningtable',\n",
        "          'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa',\n",
        "          'train', 'tvmonitor']"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwTcVNKCAoJt",
        "colab_type": "text"
      },
      "source": [
        "# Directory for cropped images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shqDlbvsAsmi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creo 20 directory, una per ogni classe, per fare classificazione (flow_from_datframe) sulle immagini croppate\n",
        "def create_directories(path, labels):\n",
        "  for label in labels:\n",
        "    os.mkdir(os.path.join(path, label))\n",
        "\n",
        "create_folder_classes = True\n",
        "'''\n",
        "if create_folder_classes == True:\n",
        "  create_directories(PATH_IMAGES_CROPPED_TRAIN, LABELS)\n",
        "  create_directories(PATH_IMAGES_CROPPED_VAL_TEST, LABELS)\n",
        "'''\n",
        "if create_folder_classes == True:\n",
        "  #create_directories(PATH_IMAGES_CROPPED_TRAIN_BALANCED, LABELS)\n",
        "  create_directories(PATH_IMAGES_CROPPED_VAL_TEST_BALANCED, LABELS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awq2lzbxBXgj",
        "colab_type": "text"
      },
      "source": [
        "# Util function for mapping from label to #class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NN3ewyc_BenA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_dict_mapping(labels):\n",
        "  mapping = {}\n",
        "  for label,i in zip(labels, range(len(labels))):\n",
        "    mapping[label] = i\n",
        "  return mapping\n",
        "\n",
        "def from_label_to_number(mapping, label):\n",
        "  return mapping[label]\n",
        "\n",
        "def from_number_to_label(mapping, number):\n",
        "  for key, val in mapping.items(): \n",
        "    if val == number: \n",
        "      return key \n",
        "  return \"key doesn't exist\"\n",
        "\n",
        "def from_onehot_to_label(mapping, one_hot):\n",
        "  return from_number_to_label(mapping,np.where(one_hot == 1)[0][0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EGlorv3K-NG",
        "colab_type": "code",
        "outputId": "441d02f2-a8c2-4ae3-f046-828529710ded",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "mapping = create_dict_mapping(LABELS)\n",
        "print(mapping)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'aeroplane': 0, 'bicycle': 1, 'bird': 2, 'boat': 3, 'bottle': 4, 'bus': 5, 'car': 6, 'cat': 7, 'chair': 8, 'cow': 9, 'diningtable': 10, 'dog': 11, 'horse': 12, 'motorbike': 13, 'person': 14, 'pottedplant': 15, 'sheep': 16, 'sofa': 17, 'train': 18, 'tvmonitor': 19}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHm9t-_NGYDa",
        "colab_type": "text"
      },
      "source": [
        "# Crop and save image for class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5P1zAHxAGdTJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def crop_image(img, x_min, y_min, x_max, y_max):\n",
        "  crop_img = img[y_min:y_max, x_min:x_max]\n",
        "  if (y_max-y_min) >= 100 and (x_max-x_min) >= 100:\n",
        "    crop_img = cv2.resize(crop_img, IM_SIZE)\n",
        "    return crop_img\n",
        "  else:\n",
        "    return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QkTnM8cPtA_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def list_images(path_images, path_txt, file_txt):\n",
        "  # creo liste immagini presenti in file_txt\n",
        "  temp = []\n",
        "  f = open(os.path.join(path_txt, file_txt), \"r\")\n",
        "  for line in f.readlines():\n",
        "    temp.append(line.split('\\n')[0] + '.jpg')\n",
        "  list_images = [os.path.join(path_images, name) for name in temp]\n",
        "  print('Ho trovato ', len(list_images), 'per il file ', file_txt)\n",
        "  return list_images\n",
        "\n",
        "# creo liste immagini da train.txt e val.txt\n",
        "list_images_train = list_images(PATH_IMAGES, PATH_MAIN, 'train.txt')\n",
        "list_images_val = list_images(PATH_IMAGES, PATH_MAIN, 'val.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gt_3vjWp7sUu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_count = {}\n",
        "\n",
        "# dict to count number of image for every class\n",
        "for label in LABELS:\n",
        "  train_count[label] = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCnf_0tOQ687",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_and_crop(list_images, annotation_dir, path_to_save, train=False):\n",
        "  '''\n",
        "  leggo annotations dei file presenti nelle due liste di immagini \n",
        "  '''\n",
        "  for path in list_images:\n",
        "    image_name = path.split('/')[-1].split('.')[0] \n",
        "    print(image_name)\n",
        "    with open(os.path.join(annotation_dir, image_name + '.xml')) as f:\n",
        "      read_xml(f.read(), path, path_to_save, train)\n",
        "  return\n",
        "\n",
        "def read_xml(file_xml, path_image, path_to_save, train=False):\n",
        "  '''\n",
        "  leggo xml e per ogni box che trovo croppo e salvo\n",
        "  '''\n",
        "  #print(path_image)\n",
        "  img = cv2.imread(path_image)  \n",
        "  root = ET.XML(file_xml)\n",
        "  for _, child in enumerate(root):\n",
        "    if child.tag == 'object':\n",
        "      x_min = None\n",
        "      y_min = None\n",
        "      x_max = None\n",
        "      y_max = None\n",
        "      for subchild in child:\n",
        "        if subchild.tag == 'name':\n",
        "          name_object = subchild.text\n",
        "          #print(name_object)\n",
        "        if subchild.tag == 'bndbox':\n",
        "          for bndbox in subchild:\n",
        "            if bndbox.tag == 'xmin':\n",
        "              x_min = int(bndbox.text)\n",
        "              #print('x_min ', x_min)\n",
        "            if bndbox.tag == 'ymin':\n",
        "              y_min = int(bndbox.text)\n",
        "              #print('y_min ', y_min)\n",
        "            if bndbox.tag == 'xmax':\n",
        "              x_max = int(bndbox.text)\n",
        "              #print('x_max ', x_max)\n",
        "            if bndbox.tag == 'ymax':\n",
        "              y_max = int(bndbox.text)\n",
        "              #print('y_max ', y_max)\n",
        "        if(x_min!=None and y_min!=None and x_max!=None and y_max!=None):\n",
        "          if(train_count[name_object] < 500 and train) or (not train):\n",
        "            image_cropped = crop_image(img, x_min, y_min, x_max, y_max)\n",
        "            x_min = None\n",
        "            y_min = None\n",
        "            x_max = None\n",
        "            y_max = None\n",
        "            #cv2_imshow(image_cropped)\n",
        "            if image_cropped is not None:\n",
        "              save_image_cropped(name_object, image_cropped, path_to_save, train)\n",
        "  return \n",
        "\n",
        "def save_image_cropped(obj, img, path_to_save, train=False):\n",
        "  '''\n",
        "  salvo immagine croppata con numero progressivo in base \n",
        "  all'ultima presente nella cartella\n",
        "  '''\n",
        "  list_dir_classes = os.listdir(path_to_save)\n",
        "  for dir_class in list_dir_classes:\n",
        "    if dir_class == obj:\n",
        "      dir_temp = os.path.join(path_to_save,dir_class)\n",
        "      list_temp = os.listdir(dir_temp)\n",
        "      if list_temp == []:\n",
        "        cv2.imwrite(os.path.join(dir_temp, obj + '_1.jpg'), img)\n",
        "        if train:\n",
        "          train_count[obj] += 1\n",
        "      else:\n",
        "        number_file = int(list_temp[-1].split('.')[0].split('_')[1]) + 1\n",
        "        cv2.imwrite(os.path.join(dir_temp, obj + '_' + str(number_file) + '.jpg'), img)\n",
        "        if train:\n",
        "          train_count[obj] += 1\n",
        "  return\n",
        "\n",
        "#read_and_crop(list_images=list_images_train, annotation_dir=PATH_ANNOTATIONS, path_to_save=PATH_IMAGES_CROPPED_TRAIN_BALANCED, train=True)\n",
        "read_and_crop(list_images=list_images_val, annotation_dir=PATH_ANNOTATIONS, path_to_save=PATH_IMAGES_CROPPED_VAL_TEST_BALANCED, train=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJ8TWz7E5pKU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_count\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgzdNcHZGgch",
        "colab_type": "text"
      },
      "source": [
        "# Generator from directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMtwpZ_kGfsx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_generator( batch_size, im_size, train_directory, val_directory, labels,\n",
        "                      validation_split, aug, augment_params):\n",
        "    \n",
        "  preprocess_function = preprocess_input_vgg16\n",
        "\n",
        "  if not(aug):\n",
        "    img_gen_train = ImageDataGenerator(#rescale=1./255, \n",
        "                                       preprocessing_function=preprocess_function)\n",
        "  elif aug:\n",
        "    img_gen_train = ImageDataGenerator( preprocessing_function=preprocess_function,\n",
        "                                        #rescale=1./255,\n",
        "                                        **augment_params)  \n",
        "\n",
        "\n",
        "  img_gen_val = ImageDataGenerator( preprocessing_function=preprocess_function,\n",
        "                                    validation_split = validation_split\n",
        "                                  )\n",
        "    \n",
        "  train_gen = img_gen_train.flow_from_directory(\n",
        "      directory = train_directory,\n",
        "      shuffle=True,\n",
        "      class_mode='categorical',\n",
        "      target_size=im_size,\n",
        "      batch_size=batch_size,\n",
        "      classes=labels,\n",
        "      seed=42\n",
        "  )\n",
        "\n",
        "  val_gen = img_gen_val.flow_from_directory(\n",
        "      directory = val_directory,\n",
        "      shuffle=False,\n",
        "      class_mode='categorical',\n",
        "      target_size=im_size,\n",
        "      batch_size=batch_size,\n",
        "      subset='training',\n",
        "      classes=labels,\n",
        "      seed=42\n",
        "  )\n",
        "\n",
        "  test_gen = img_gen_val.flow_from_directory(\n",
        "      directory = val_directory,\n",
        "      shuffle=False,\n",
        "      class_mode='categorical',\n",
        "      target_size=im_size,\n",
        "      batch_size=batch_size,\n",
        "      subset='validation',\n",
        "      classes=labels,\n",
        "      seed=42\n",
        "  )\n",
        "\n",
        "  return train_gen, val_gen, test_gen"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISzDSogyQ12u",
        "colab_type": "text"
      },
      "source": [
        "# Generator from image in memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z27ENn5qQ9AR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_generator_for_pickle( images_train, labels_train, images_val, labels_val, batch_size, base_net,\n",
        "                      validation_split, aug, augment_params):\n",
        "  \n",
        "  if base_net == 'vgg16':\n",
        "    preprocess_function = preprocess_input_vgg16\n",
        "  elif base_net == 'mobilenet':\n",
        "    preprocess_function = preprocess_input_mobilenet\n",
        "  elif base_net == 'resnet':\n",
        "    preprocess_function = preprocess_input_resnet50\n",
        "\n",
        "  if not(aug):\n",
        "    img_gen_train = ImageDataGenerator(preprocessing_function=preprocess_function)\n",
        "  elif aug:\n",
        "    img_gen_train = ImageDataGenerator( preprocessing_function=preprocess_function,\n",
        "                                        **augment_params)  \n",
        "\n",
        "\n",
        "  img_gen_val = ImageDataGenerator( preprocessing_function=preprocess_function,\n",
        "                                    validation_split = validation_split\n",
        "                                  )\n",
        "    \n",
        "  train_gen = img_gen_train.flow(\n",
        "      x = images_train,\n",
        "      y = labels_train,\n",
        "      shuffle=True,\n",
        "      batch_size=batch_size,\n",
        "      seed=42\n",
        "  )\n",
        "\n",
        "  val_gen = img_gen_val.flow(\n",
        "      x = images_val,\n",
        "      y = labels_val,\n",
        "      shuffle=False,\n",
        "      batch_size=batch_size,\n",
        "      subset='training',\n",
        "      seed=42\n",
        "  )\n",
        "\n",
        "  test_gen = img_gen_val.flow(\n",
        "      x = images_val,\n",
        "      y = labels_val,\n",
        "      shuffle=False,\n",
        "      batch_size=batch_size,\n",
        "      subset='validation',\n",
        "      seed=42\n",
        "  )\n",
        "\n",
        "  return train_gen, val_gen, test_gen"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDw9W32GaThb",
        "colab_type": "text"
      },
      "source": [
        "# Frequency classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teCq1HPhaXTg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_frequency_from_generator(generator):\n",
        "  mapping = generator.class_indices\n",
        "  classes, count = np.unique(generator.labels, return_counts=True)\n",
        "  labels = [from_number_to_label(mapping, label) for label in classes]\n",
        "  freq = count\n",
        "  return labels, freq\n",
        "\n",
        "def normalize_frequency(freq):\n",
        "  min_freq = np.min(freq)\n",
        "  weigh = [min_freq/x for x in freq]\n",
        "  return weigh * freq, weigh\n",
        "\n",
        "def plot_label_frequency(labels, freq):\n",
        "  freq = freq / np.sum(freq)\n",
        "  l = list(range(1, len(labels)+1))\n",
        "  plt.barh(l, width=freq, height=0.5)\n",
        "  plt.yticks(l, labels, rotation='horizontal')\n",
        "  plt.show()\n",
        "\n",
        "def plot_stacked_bar_freq(labels, freq, freq_normalize):\n",
        "  N = len(labels)\n",
        "  ind = np.arange(N)    # the x locations for the groups\n",
        "  width = 0.35       # the width of the bars: can also be len(x) sequence\n",
        "\n",
        "  p1 = plt.bar(ind, freq, width)\n",
        "  p2 = plt.bar(ind, freq_normalize, width)\n",
        "\n",
        "  plt.ylabel('Frequency')\n",
        "  plt.legend((p1[0], p2[0]), ('Freq', 'Freq_normalize'))\n",
        "\n",
        "  plt.show() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BR99sl1tKkgt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels, freq = get_frequency_from_generator(train_gen)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hg48KrlvhswU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(labels)\n",
        "print(freq)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWJR49amiauf",
        "colab_type": "code",
        "outputId": "4da348a9-4b5b-4033-bd2c-dc49b45741ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "plot_label_frequency(labels, freq)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ4AAAD4CAYAAADcpoD8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3debQdVYH98e82yIyAwM+FCAYhSINggBcUBCTgDxQHQKFR0zIqgjRD09hNK8uGphVs7AYRAaNLg4pKQ6OitqBCGAwQeAmZkEEmmx/YShDCKELYvz/qPLjc3Dcld3xvf9Z669WtOlV1Ti6+46k6tUu2iYiIaJdXdboCERExvqTjiYiItkrHExERbZWOJyIi2iodT0REtNVKna5At1t//fU9ceLETlcjIqKnzJkzZ7HtDRptS8czjIkTJ9Lf39/pakRE9BRJvxtsWy61RUREW6XjiYiItkrHExERbZWOJyIi2iodT0REtFU6noiIaKt0PBER0VbpeCIioq2a8gCppHWAj9o+vxnHG+W5+4CDbR8naXfgL7ZvbNbxFz60hIkn/6xZhxvUA2e+t+XniIjoBs0a8awDfKpJxxoV2/22jysfdwd2Hs3+kpLeEBHRRs36o3smsJmkecBvgRm2fwYgaQbwU2BNYD9gDWAS8CVgZeBjwHPAPrb/JGkycCGwOnAvcLjtxyRdC8wGplJ1dEfYvqGMck4C/hY4Clgq6W+AY4EHgW8C6wOPAIfZ/p9Spz8D2wGzgBOb9O8QERHDaNaI52TgXtuTge8Bfw0gaWVgT2DgWtVbgA8CU4DPA8/Y3g64CTi4lPk28I+2twUWAv9cc56VbO8InFC3HtsPUHVYZ9uebPsG4CvAReVYFwPn1uzyBmBn28t0OpKOlNQvqX/pM0uW598jIiIG0YrJBT8HpkpaBXgPcL3tZ8u2mbaftP0IsAT4SVm/EJgoaW1gHdvXlfUXAbvVHPvy8nsOMHEEddmJqiME+A6wS822S20vbbST7em2+2z3TVh97RGcJiIiRqrp9zds/7lcFtsbOAj4Qc3m52qWX6z5/OII6zJQfukIyw/l6ZEU2majtenPjf+IiKZp1ojnSWCtms+XAIcBuwJXjvQgtpcAj0nataz6GHDdELsMV48bgQ+X5WnADaM4VkREtEBTOh7bjwKzJC2SdBbwC+CdwK9s/2WUhzsEOEvSAmAy8C+j2PcnwP6S5pXO61jgsHKsjwHHj7IuERHRZLLd6Tp0tb6+PudFcBERoyNpju2+RtuSXBAREW3VMw9PLm86gqT/Lvs9vjznbUVyQVIKImI866URT8N0hOGSB2zvs7ydTkRENF/PjHh4ZTrC81TJA48BWwJbSPoRsDGwKvBl29MBJD0A9FElJ/wc+DVVrM5DwL41zxhFREQb9NKIpzYd4dPA9sDxtrco2w+3vQNVJ3OcpPUaHGMS8FXbWwOPAx9qdKIkF0REtE4vdTz1brF9f83n4yTNB26mGvlMarDP/bbnleVB0w+SXBAR0Tq9dKmt3kvJAyUo9F3ATrafKckJqzbYpzY5YSmw2nAnSXJBRERz9dKIpz6VoNbawGOl09kSeHv7qhUREaPRMyMe249KmiVpEfAs8IeazVcCR0m6A7iL6nJbRER0oZ7peABsf3SQ9c9RJWE32jaxLC6mei3DwPovNbt+ERExvF661BYREWPAmO94JO0q6fYSHDrsZIKIiGitnrrUtpymAWfY/u7y7NyKyJxGEqMTEeNFT454JK0h6WeS5pdXMRwkaU9Jt0laKOmbklaR9HGq13CfLuliSWtKulrS3FJu3063JSJivOnVEc+7gYdtvxegvDJ7EbCn7bslfRs42vY5knYBfmr7spLrtr/tJyStD9ws6QrXvRtC0pHAkQATXrNBO9sVETHm9eSIB1gI/F9JXywvfJtIlUpwd9l+EbBbg/0EfKG8GO5XwEbA6+oLJbkgIqJ1enLEU0Y12wP7AP8KXDPCXacBGwA72H6+BIg2SjiIiIgW6cmOR9LrgT/Z/q6kx4G/BSZK2tz2PVSvub6uwa5rA38snc5U4I3DnSuRORERzdWTHQ+wDXCWpBepXpFwNFWncmm5j3MrcGGD/S4GfiJpIdAP3Nmm+kZERNGTHY/tq4CrGmzarkHZQ2uWFwM7ta5mERExnF6dXBARET2qJzoeSQ+U6c8REdHjevJSWzu1K7lgKEk1iIixpOtGPI1SCcqmY2sSB7asKftNSbeU1IJ9y/oJks6SdKukBZI+WdbvLun6cvy7JF0oqev+DSIixrJu/KM7kErwVttvoXrXDsBi29sDFwAnlXWfBa6xvSMwlWqm2xrAEcAS21OAKcAnJG1a9tkROBbYCtgM+GB9BSQdKalfUv/SZ5a0ppUREeNUN3Y8r0glsD3wl//y8nsOVVIBwF7AyZLmAddSPQy6SVl/cFk/G1gPmFT2ucX2fbaXAt8HdqmvQJILIiJap+vu8dSnEki6umx6rvxeysv1FvAh23fVHkOSgGPLtOva9bsDr8hla/A5IiJaqOs6ngapBB8fovhVVPd+jrVtSdvZvq2sP1rSNSWlYAvgobLPjuWy2++Ag4DpQ9UnyQUREc3VdR0PjVMJLhuk7OnAOcCCMkngfuB9wDeoLsfNLaOfR4D9yj63AucBmwMzgR+2phkREdGI6t4IMKaVS20n2X7fSPfp6+tzf39/6yoVETEGSZpju6/Rtm6cXBAREWNY0zseSSdIWr3m82eW4xiHSjpvmDK7S/rpaI5r+9qB0c7y1CsiIlZcK+7xnAB8F3imfP4M8IUWnGdFjahe3ZBcUCspBhHR64Yd8UiaKOlOSRdLukPSZZJWl7RnSQtYWNIDVpF0HPB6YKakmZLOBFaTNE/SxeV4f1OSBuZJ+pqkCWX9YZLulnQL8I6a888oCQP9Zfsy92ck7SjpplKfGyW9uaw/VNLlkq6U9FtJ/1bWL1OviIhoj5FeanszcL7tvwKeAE4EZgAH2d6GauR0tO1zgYeBqban2j4ZeNb2ZNvTJP0V1RTmd9ieTPVMzjRJGwKnUXU4u1ClCtSaSJU48F7gQkn1bw29E9jV9nbA53jlSGZyOec2wEGSNq6vV31jk1wQEdE6I+14HrQ9qyx/F9gTuN/23WXdRcBuIzjOnsAOwK0lVWBP4E3A24BrbT9i+y/AJXX7/aftF23/FrgP2LJu+8BL4BYBZwNb12y72vYS238GfsMI3jqa5IKIiNYZacdTP+f68eU8n4CLykhjsu032z51Oc5f//l0YGbJdns/VXTOgOdqlmtTDyIiogNG+kd4E0k72b4J+CjVa6M/KWlz2/cAHwOuK2WfBNYCFpfPz0t6te3ngauBH0s62/YfJb22lJ0NfFnSelSX8g4E5tec/0BJFwGbUo2Q7gLeXrN9bV5OJjh0hG2qrdegklwQEdFcIx3x3AUcI+kOYF2qy1mHUV3eWgi8CFxYyk4HrpQ0s+bzAkkX2/4NcArwC0kLgF8CG9r+PXAqcBMwC7ij7vz/A9wC/Bw4qlw2q/VvwBmSbmPknelL9Rph+YiIaIJhkwskTQR+Wi5jtZ2kGeX8g8XmtFSSCyIiRi/JBRER0TWGvSxl+wGgI6Odcv5DO3XuiIhovp6e4SVpJdsvtPIc3ZZcEBHRDq1MSen4pbYhkhF2kHSdpDmSrioPmSLpWknnSOoHjpd0oKRFkuZLur6UWVXSt0qqwm2Sppb1DZMMIiKifbplxPNm4AjbsyR9EzgG2B/Y1/Yjkg4CPg8cXsqvPHDTqsyq29v2Q5LWKduPAWx7G0lbUs2i26JsmwxsR/V8z12SvmL7wdrKSDoSOBJgwms2aFWbIyLGpY6PeIr6ZIS9qe4r/bIkHJwCvKGmfG2ywSxghqRPABPKul3KcbB9J9XbRgc6nmGTDJJcEBHROt0y4qmf0/0kcLvtnQYp//RLO9pHSXobVY7bHEk7DHOuJBlERHRQt/zRrU9GuBn4xMA6Sa8GtrB9e/2OkjazPRuYLek9wMbADcA04JpyiW0Tqodgtx9txZJcEBHRXN3S8QwkI3yT6vLXV4CrgHMlrU1Vz3OAZToe4CxJk6hy4K6mitq5E7ig3P95ATjU9nOSWt+SiIgY0rDJBS2vQIeTEYaT5IKIiNFLckFERHSNjnY8knYHXj/a0Y6kUyWd1GD96yVdVpYPlXRec2oaERHN0ul7PLsDTwE3jnQHSYPW2fbDwAErXq2XtTq5oJVPB0dEdKMVHvHUJA/MkHR3SSB4l6RZJR1gR0mvlfQjSQsk3Sxp23Jv5yjg7yTNk7RrOdY1pdzVkjYp55gh6UJJs6legQDwVkk3lXN8oqYuixrU8b2l7PqS9irLcyVdKmnNFf03iIiIkWvWpbbNgX+neiX1llRToncBTgI+A5wG3GZ72/L52yV89ELg7PI20huoZrNdVMpdDJxbc443ADvbPrF83hbYA9gJ+Jyk1zeqmKT9gZOBfcqqU4B32d6e6oV2JzbaLyIiWqNZl9rut70QQNLtVOkALtOZJ1KlA3wIwPY1ktaT9JoGx9kJ+GBZ/g4vj24ALrW9tObzj20/CzxbXjq3IzCv7nh7AH3AXrafkPQ+YCtgVplavTLVy+deIZE5ERGt06yOpzYN4MWazy+Wcwz5eukRerruc/088Ebzwu+lelX2FlSjGwG/tP2RoU5kezrVG0pZZcNJnZ1vHhExxrRrcsFAksDpZSbb4jICeRKoHfncCHyYarQzrew3mH0lnQGsQTVJ4WSqEUyt3wGfBi6XdCBVIsJXJW1u+x5JawAb2b57sJMkuSAiornaNZ36VGAHSQuAM4FDyvqfAPsPTC4AjgUOK+U+Bhw/xDEXADOpOpPTy4y2ZZSQ0GnApVSd3KHA98s5bqK6JxUREW3S8eSCbpfkgoiI0UtyQUREdI10PBER0VadTi4YVLeEhya5ICKiucbkiGeoWJ2IiOisbu94Jkj6uqTbJf1C0mqSJpfYnQWSfihpXQBJ10o6R1I/cLykAyUtkjRf0vWlzARJZ0m6tez/yY62LiJiHOr2jmcS8FXbWwOPU6UffBv4xxKrsxD455ryK9vus/3vwOeAvW2/FfhA2X4EsMT2FGAK1VtON60/qaQjJfVL6l/6zJKWNS4iYjzq9o7nftsDMThzgM2AdWxfV9ZdBOxWU/6SmuVZwIwSIDqhrNsLOFjSPGA2sB5V5/YKtqeXDqxvwuprN681ERHRvZMLitoonqXAOsOUfylWx/ZRkt4GvBeYI2kHqsicY21fNdIKJLkgIqK5un3EU28J8FhJOYAq3eC6RgUlbWZ7tu3PAY8AGwNXAUdLenUps0WJzYmIiDbp9hFPI4cAF0paHbgPOGyQcmdJmkQ1yrkamE8VszMRmKsqnvoRYL+W1zgiIl6SyJxhJDInImL0EpkTERFdoxcvtQ1K0qnAU7a/1Kxjtjq5oB2SjhAR3SQjnoiIaKue73gkfVbS3ZJ+Dby5rBss3WBKWTevJBgs6mjlIyLGoZ7ueMqzOR8GJgP7UKURwODpBt8CPml7MtVzQYMdN8kFEREt0tMdD7Ar8EPbz9h+AriC6lXYy6QbSFoHWMv2TWX99wY7aJILIiJaZ0xNLmiFJBdERDRXr494rgf2K6nVawHvp4rNWSbdwPbjwJMlRgeqS3QREdFmPT3isT1X0iVUqQR/BG4tmwZLNzgC+LqkF6midnIDJyKizXq64wGw/Xng8w02vb3ButvLhAMknQwkkiAios16vuMZpfdK+ieqdv8OOLSz1YmIGH+WK6ttICEAeA1wve1fDVH2A8BWts9crgpKJwDTbT8zTLkHgD7bixvVdXnTDFbZcJI3POSc5dl1RJIqEBFj0VBZbSs04imvHBiuzBVU05yX1wnAd4EhO56IiOgNI57VNkhCwAxJB5TlBySdJmmupIWStizrD5V0Xk35cyXdKOm+mn1fJel8SXdK+qWk/5Z0gKTjgNcDMyXNLGUvKA933i7ptLpq/kM59y2SNm/Qhs0kXSlpjqQbBuoYERHtM6KOZ4iEgHqLbW8PXACcNEiZDYFdgPcBA5ffPkj1npytqKY/7wRg+1zgYWCq7aml7GfL8G1b4J2Stq059hLb2wDnAY2uj02negPpDqV+5w/S3iQXRES0yEgvtb2UEAAgabBLZ5eX33OoOpNGfmT7ReA3kl5X1u0CXFrW/+/A6GYQfy3pyFL3Dak6qwVl2/drfp9du5OkNYGdgUurd8ABsEqjE9ieTtVJscqGk/LCooiIJmr2rLbnyu+lQxz7uZplDVKmIUmbUo1Upth+TNIMYNWaIh5kGarR3eMlpy0iIjpkpB3P9cAMSWeUfd4PfK2J9ZgFHCLpImADYHdezlJ7ElgLWEw1i+5pYEkZLb0HuLbmOAdRXb47CLipZj22n5B0v6QDbV9aXn29re35Q1UskTkREc01oo5niISAZvkvYE/gN8CDwFxeThWYDlwp6WHbUyXdBtxZys2qO866khZQjao+0uA804ALJJ0CvBr4AVWbIiKiTZbrOZ5WkLSm7ackrQfcArzD9v92ul59fX3u70/AQUTEaLTsOZ4m+2l5dcHKwOnd0OlERETzdU3HY3v3TtchIiJar2s6nm618KElTDz5Zy05duJyImI86un38Ug6WNICSfMlfUfSREnXlHVXS9pE0oQym02S1pG0VNJuZf/rJU3qdDsiIsaTnu14JG0NnALsYfutwPHAV4CLyqsPLgbOtb0UuIvqQdNdqGbM7SppFWBj279tcOwkF0REtEjPdjzAHlRpB4sBbP+JKmpn4Pmf71B1NAA3ALuVnzPK+ikMMi3c9nTbfbb7Jqy+dutaEBExDvVyxzMa11PF/uwI/DewDtVDqjd0sE4REeNSL08uuAb4oaT/sP2opNcCN1KFmX6H6mHRgY7llrLuPtt/ljQP+CRVUOmQklwQEdFcPdvx2L5d0ueB6yQtBW4DjgW+JenTwCPAYaXsc5IeBG4uu99AlWywsP01j4gY37omuaBbJbkgImL0hkouGC/3eCIiokv0bMdT+/bTUexzY6vqExERI9Oz93iWh+2d69dJWsn2C4Pt08rkgmgsiQ4RY1vPjHjqUwrK6t0k3SjpvoHRj6Q1S2rBXEkLJe1bc4ynyu/dJd1Q3qT6m/a3JiJi/OqJEU9NSsHOtheXqdP/QfXq612ALYErgMuAPwP7lxe/rQ/cLOkKLzuLYnvgLbbvb3C+I4EjASa8ZoNWNSsiYlzqlRFPo5QCgB/ZftH2b4DXlXUCvlBeCPcrYKOabbVuadTplOMnuSAiokV6YsQzhOdqllV+T6N6ffYOtp+X9ACwaoN9n25x3SIiooFe6XgapRQMZm3gj6XTmQq8cUVOnOSCiIjm6omOZ5CUgsFcDPxE0kKgH7izHXWMiIiRSXLBMJJcEBExekkuiIiIrjEuO57yHM8yD5NGRETr9cQ9nhbYHXiK6jUKQ+rm5II84R8RvWhMjXjq0w0kvV/SbEm3SfqVpNdJmggcBfydpHmSdu1srSMixpcxM+IZJN3AwNttW9LHgX+w/feSLgSesv2lQY6V5IKIiBYZMx0PDdINJG0DXCJpQ2BloGFSQT3b04HpAKtsOCnT/iIimmhMXWpr4CvAeba3oXrVdaMEg4iIaKOxNOJplG6wNvBQ2X5ITdkngdeM5KBJLoiIaK4xM+KxfTswkG4wnyq9+lTgUklzgMU1xX8C7J/JBRER7TeWRjzYvgi4qG71jxuUuxvYti2VioiIVxgzI56IiOgN6XgiIqKtxtSltuUhaSXbLwy2vZuTC7pR0hQiYjhjquORdDBwEtWDowuA/6R6qHRl4FFgmu0/SDoV2Ax4E/A/wEc6UuGIiHFozHQ8I00uAP6+7LIVsIvtZztT44iI8WnMdDyMPrngisE6nUTmRES0zlifXDBUcsHTg+1ke7rtPtt9E1Zfu9V1jIgYV8bSiGc0yQUjluSCiIjmGjMdj+3bJQ0kFywFbuPl5ILHqDqmTTtYxYiIYAx1PDCq5IJT21KhiIhYxli/xxMREV0mHU9ERLRVz19qK6+y/qntt7Ti+M1OLsiT/REx3mXEExERbTVWOp6VJF0s6Q5Jl0laXdIDktYHkNQn6dqy/M7yHp55km6TtFZHax4RMc6MlY7nzcD5tv8KeAL41BBlTwKOsT0Z2BVYJr1A0pGS+iX1L31mSUsqHBExXo2VjudB27PK8neBXYYoOwv4D0nHAes0SqZOckFEROv0/OSCwg0+v8DLHetLUTm2z5T0M2AfYJakvW3fOdiBk1wQEdFcY2XEs4mkncryR4FfAw8AO5R1HxooKGkz2wttfxG4FdiynRWNiBjvxkrHcxdwjKQ7gHWBC4DTgC9L6geW1pQ9QdIiSQuA54Gft722ERHjWM9farP9AI1HLTcAWzQof2yr6xQREYMbKyOeiIjoEV054hltGoGkE4Dptp8pnz9j+ws125+yveby1KXZyQXtlqSEiOg2Y2XEcwKwes3nz3SqIhERMbRu7ngapRHsWdIGFkr6pqRVyvM4rwdmSpop6UxgtZJMcHH9QSV9WtKtkhZIOq3trYqIGOe6ueOpTyM4EZgBHFReZb0ScLTtc4GHgam2p9o+GXjW9mTb02oPKGkvYBKwIzAZ2EHSbvUnTnJBRETrdHPHU59GsCdwv+27y7qLgGU6jWHsVX5uA+ZSzYabVF8oyQUREa3TlZMLivo0gseB9VbwmALOsP21ke6Q5IKIiObq5hFPfRpBPzBR0uZl3ceA68ryk0BtyvTzkl7d4JhXAYdLWhNA0kaS/k/zqx4REYPp5o6nPo3gbOAw4FJJC4EXgQtL2enAlZJm1nxeUD+5wPYvgO8BN5VjXMYrO6yIiGgx2fVXtKJWX1+f+/v7O12NiIieImmO7b5G27p5xBMREWNQN08uaGi0qQZDHOdQ4Be2Hx6qXKeTC5I8EBFjzXge8RxK9eBpRES0Ua92PCNKNQCQ9LmSVLBI0nRVDgD6gItLwsFqnW1ORMT40asdz4hSDUrZ82xPKZfmVgPeZ/syqunZ00rCwbO1B09yQURE6/RqxzOaVIOpkmaX6dN7AFsPd/AkF0REtE6vdjyNUg2WIWlV4HzggDIS+jqwaovrFhERQ+i5WW3FJpJ2sn0TL6cafFLS5rbv4eVUg4FOZnFJKziA6qFRWDbtoKFE5kRENFevjnhGlGpg+3GqUc4iqricW2uOMQO4MJMLIiLaK8kFw0hyQUTE6CW5ICIiukZPdzySJkpa1GD9NyRtNYL9D5V0XmtqFxERjfTq5IIh2f54o/WSJtheOppjdToyZzCJ0omIXtXTI56iUYrBtZL6ACQ9JenfJc0HdpJ0mKS7Jd0CvKOzVY+IGH/GQsdTn2LwqbrtawCzbb8VuBc4jarD2QVoeDkuyQUREa0zFjqe+hSDXeq2LwX+qyy/DbjW9iO2/wJc0uiASS6IiGidsdDx1M8Hr//859He14mIiNYZC5ML6lMMfg28f5Cys4EvS1qP6rLcgcD8oQ6e5IKIiOYaCyOe+hSDCwYraPv3wKnATcAs4I52VDAiIl7W0yMe2w8AWzbYtHtNmTXr9vkW8K2WViwiIgY1FkY8ERHRQzra8axo8sAozvNUs44VERErpisvtQ2WPNAJ7UouSBJBRIwX3XCpbbjkgXdLmitpvqSrJb1K0m8lbVC2v0rSPZI2kPQ6ST8sZedL2rn+ZJI+LelWSQskndbuxkZEjHfd0PEMmjxQOpevAx8qyQMH2n6R6kHRaaXYu4D5th8BzgWuK2W3B26vPZGkvYBJwI7AZGAHSbtRJ8kFERGt0w0dz1DJA28Hrrd9P4DtP5X13wQOLsuH8/IstT0o06ltL7Vd32vsVX5uA+ZSzYibVF+hJBdERLRON9zjGS55YNkd7Acl/UHSHlSjl2nD7VMIOMP210ZZx4iIaJJu6HiGSh64GThf0qa275f02ppRzzeoRkjfqYnEuRo4GjhH0gRgzbpRz1XA6ZIutv2UpI2A523/cbDKJbkgIqK5uuFS26DJA+W+zZHA5eW1BrWhnlcAa/LKh0GPB6ZKWgjMoS592vYvgO8BN5UylwFrNb1FERExKNnDXtnqSmXW29m2d23lefr6+tzf39/KU0REjDmS5tjua7StGy61jZqkk6kuqY303k5ERHSJnut4JM0Afmr7jZ2uS0REjF5HOx5JE7r9XTntSi6olySDiBirmjK5QNKPJM2RdLukI8u6vSTdVFIHLpW0Zln/gKQvSpoLHCjpI5IWSlok6Ys1x3xK0tnlmFcPJBXUnfdzJYVgkaTpklTWX1vOcYukuyXtWtZPkHRWTXLBJ5vR/oiIGLlmzWo73PYOQB9wnKTXAacA77K9PdAPnFhT/tGy/nrgi1QPfk4Gpkjar5RZA+i3vTVwHfDPDc57nu0ptt8CrAa8r2bbSrZ3BE6o2fcIYIntKcAU4BOSNq0/aJILIiJap1kdz3FluvPNwMbAJ6imMs+SNA84BKi9JzMwLXoKcK3tR2y/AFwMDETYvFhTrj7RYMBUSbPL1Og9gK1rtl1efs8BJpblvYCDS51mA+uR5IKIiLZa4Xs8knanykvbyfYzkq6lep30L21/ZJDdnl6OU71i3rekVYHzgb6SZHAqsGpNkefK76W83E4Bx9q+ajnOHxERTdCMyQVrA4+VTmdLqny1VYF3SNrc9j2S1gA2sn133b63AOdKWh94DPgI8JWy7VXAAcAPeDnRoNZAJ7O43D86gOqB0KFcBRwt6Rrbz0vaAnjI9qAdYZILIiKaqxkdz5XAUSV54C6qy22PAIcC35e0Sil3CvCKjsf278szOTOpRiM/s/3jsvlpYEdJpwB/BA6q2/dxSV8HFgH/C9w6grp+g+qy29wyEeERYL8h94iIiKbq2uQCSU/ZXrPT9UhyQUTE6A2VXNANWW0RETGOdG3H0w2jnYiIaL6u7XgiImJsSscTERFtlY4nIiLaKh1PRES0VTqeiIhoq659jqdbSHqS6sHYXrc+sLjTlWiCsdIOGDttSTu6S7e04422l3mrAPTgi+A64K7BHoLqJZL6047uMlbaknZ0l15oRy61RUREW6XjiYiItkrHM7zpnTwgGasAAAO2SURBVK5Ak6Qd3WestCXt6C5d345MLoiIiLbKiCciItoqHU9ERLTVuO54JL1b0l2S7ikvpKvfvoqkS8r22ZIm1mz7p7L+Lkl7t7Pe9Za3HZImSnpW0rzyc2G7615Xz+HasZukuZJekHRA3bZDJP22/BzSvlovawXbsbTm+7iifbVe1gjacaKk30haIOlqSW+s2dZL38dQ7eil7+MoSQtLXX8taauabV3z9woA2+PyB5gA3Au8CVgZmA9sVVfmU8CFZfnDwCVleatSfhVg03KcCT3YjonAok5/F6Nox0RgW+DbwAE1618L3Fd+r1uW1+21dpRtT3X6uxhFO6YCq5flo2v+u+q176NhO3rw+3hNzfIHgCvLctf8vRr4Gc8jnh2Be2zfZ/svwA+AfevK7AtcVJYvA/Ysr8zeF/iB7eds3w/cU47XCSvSjm4ybDtsP2B7AfBi3b57A7+0/SfbjwG/BN7djko3sCLt6CYjacdM28+UjzcDbyjLvfZ9DNaObjKSdjxR83ENYGDmWDf9vQLG96W2jYAHaz7/v7KuYRnbLwBLgPVGuG+7rEg7ADaVdJuk6yTt2urKDmFF/k177fsYyqqS+iXdLGm/5lZtVEbbjiOAny/nvq20Iu2AHvs+JB0j6V7g34DjRrNvOyUyZ3z7PbCJ7Ucl7QD8SNLWdf/PKdrrjbYfkvQm4BpJC23f2+lKDUXS3wB9wDs7XZcVMUg7eur7sP1V4KuSPgqcAnT0/tpgxvOI5yFg45rPbyjrGpaRtBKwNvDoCPdtl+VuRxl6Pwpgew7Vtd8tWl7jxlbk37TXvo9B2X6o/L4PuBbYrpmVG4URtUPSu4DPAh+w/dxo9m2TFWlHz30fNX4ADIzQuun7qHT6plmnfqhGe/dR3WwbuFm3dV2ZY3jlTfn/LMtb88qbdffRuckFK9KODQbqTXXT8iHgtd3ajpqyM1h2csH9VDey1y3LvdiOdYFVyvL6wG+pu4HcTe2g+iN8LzCpbn1PfR9DtKPXvo9JNcvvB/rLctf8vXqpfp08ead/gH2Au8t/dJ8t6/6F6v/1AKwKXEp1M+4W4E01+3627HcX8J5ebAfwIeB2YB4wF3h/l7djCtX16aepRp631+x7eGnfPcBhvdgOYGdgYfkjsRA4osvb8SvgD+W/n3nAFT36fTRsRw9+H1+u+d/zTGo6pm76e2U7kTkREdFe4/keT0REdEA6noiIaKt0PBER0VbpeCIioq3S8URERFul44mIiLZKxxMREW31/wF+O9xpdmDRogAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jo-t0MmYjmXY",
        "colab_type": "code",
        "outputId": "c46ac3fb-bd96-472a-ad36-4a8c6ab5344d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "freq_normalize = normalize_frequency(freq)\n",
        "print(freq_normalize)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(array([317., 317., 317., 317., 317., 317., 317., 317., 317., 317., 317.,\n",
            "       317., 317., 317., 317., 317., 317., 317., 317., 317.]), [0.674468085106383, 0.7731707317073171, 0.535472972972973, 0.6240157480314961, 0.4232309746328438, 1.0, 0.2661628883291352, 0.5205254515599343, 0.2175703500343171, 0.8929577464788733, 0.8498659517426274, 0.4127604166666667, 0.8408488063660478, 0.8453333333333334, 0.06315999203028491, 0.5691202872531418, 0.6227897838899804, 0.7944862155388471, 0.9694189602446484, 0.7694174757281553])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvIqlKPijrTx",
        "colab_type": "code",
        "outputId": "f531e9b0-7078-4bd4-9f6d-b69b79294d6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "plot_stacked_bar_freq(labels, freq, freq_normalize[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAD4CAYAAADLhBA1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAbOElEQVR4nO3de5QV5Znv8e+PS0SFgJeOwwFjo0GjCXKxUTxRx8sIaBK8JCZ6nBEZjySCWWadkzXKmCjxcpY5cdRJ4uRERw6QEK+jiIk5AY2XmEShwRZQlEYDsYkKA15gFLXjc/7Yb/dsoLvZNFV796Z/n7X22lVvvbXr6erqevp9691VigjMzMx2VY9KB2BmZrsHJxQzM8uEE4qZmWXCCcXMzDLhhGJmZpnoVekA8rD//vtHbW1tpcMwM6sqixcv/veIqOns+rtlQqmtraW+vr7SYZiZVRVJa3ZlfXd5mZlZJpxQzMwsE04oZmaWid3yGkpbPvzwQ5qamtiyZUulQ7Gd1KdPHwYPHkzv3r0rHYqZdaDbJJSmpib69etHbW0tkiodjpUoItiwYQNNTU0MGTKk0uGYWQe6TZfXli1b2G+//ZxMqowk9ttvP7cszapArglF0mpJyyQ1SKpPZftKWiCpMb3vk8ol6QeSVklaKmlU0edMTPUbJU3chXh2/YeysvPvzaw6lKOFclJEjIiIujR/BfBoRAwFHk3zAKcBQ9NrMvBjKCQg4GrgGOBo4OqWJGRmZl1HJa6hnAGcmKZnAY8Dl6fy2VF4QMvTkgZIGpjqLoiIjQCSFgDjgTt3JYjaK365K6tvZ/UNn99hnZ49ezJs2LDW+blz5+Jv9JvZ7iLvhBLAfEkB/CQibgMOiIjX0vLXgQPS9CDg1aJ1m1JZe+VbkTSZQsuGT37yk1n+DJnZc889aWhoaHNZRBAR9OjRbS5rmQFt/3NXyj9o1vXkffY6LiJGUejOmirphOKFqTWSySMjI+K2iKiLiLqamk7fiqasVq9ezWGHHcYFF1zAZz/7WV599VW+//3vM3r0aI488kiuvvrq1rrXX389hx56KMcddxznnXceN954YwUjNzPbXq4JJSLWpvd1wAMUroG8kbqySO/rUvW1wIFFqw9OZe2VV5333nuPESNGMGLECM466ywAGhsbmTJlCs8//zwvvfQSjY2NLFy4kIaGBhYvXsyTTz7J4sWLueuuu2hoaODhhx9m0aJFFf5JzMy2l1uXl6S9gR4RsSlNjwWuAeYBE4Eb0vuDaZV5wKWS7qJwAf7tiHhN0q+B/1V0IX4sMC2vuPO0bZfX6tWrOeiggxgzZgwA8+fPZ/78+YwcORKAzZs309jYyKZNmzjrrLPYa6+9AJgwYUL5gzcz24E8r6EcADyQhnz2An4eEf9P0iLgHkkXAWuAr6T6DwOnA6uAd4FJABGxUdK1QMu/5de0XKDfHey9996t0xHBtGnT+NrXvrZVnVtuuaXcYZmZ7bTcurwi4pWIGJ5en4mI61P5hog4JSKGRsTftCSHKJgaEYdExLCIqC/6rBkR8an0+r95xVxp48aNY8aMGWzevBmAtWvXsm7dOk444QTmzp3Le++9x6ZNm3jooYcqHKmZ2fa6za1XttUVR5GMHTuWFStWcOyxxwLQt29ffvaznzFq1Ci++tWvMnz4cD7xiU8wevToCkdqZrY9j1Eto5aWR4va2lqWL1++Vdlll13GsmXLWLZsGX/4wx845JBDALjyyitZuXIlTz31FIceemjZYjYzK5UTipmZZaLbdnlVs+nTp1c6BDOz7biFYmZmmXBCMTOzTDihmJlZJpxQzMwsE933ovz0/hl/3tvZfp6ZWZVxC6WMevbs2XpzyBEjRrB69epKh5SL6dOnt94N+aqrruKRRx6pcERmVg7dt4VSAdXwPJTm5mZ69crusLjmmmsy+ywz69rcQqmgvJ6HcuKJJ3L55Zdz9NFHc+ihh/Lb3/4WgC1btjBp0iSGDRvGyJEjeeyxxwCYOXMmEyZM4OSTT+aUU05h5syZnHnmmZx66qnU1tbyox/9iJtuuomRI0cyZswYNm4s3Jvz9ttvZ/To0QwfPpwvfelLvPvuu9vFcuGFF3LfffdRX1/f2jIbNmxY63PiX375ZcaPH89RRx3F8ccfz4svvpjZ/jWz8nJCKaNyPg+lubmZhQsXcsstt/Dd734XgFtvvRVJLFu2jDvvvJOJEyeyZcsWAJYsWcJ9993HE088AcDy5cu5//77WbRoEVdeeSV77bUXzz77LMceeyyzZ88G4Oyzz2bRokU899xzHH744dxxxx3txlNXV0dDQwMNDQ2MHz+eb33rWwBMnjyZH/7whyxevJgbb7yRKVOmdH4Hm1lFucurjMr5PJSzzz4bgKOOOqr1Ws1TTz3FN77xDQA+/elPc9BBB7Fy5UoATj31VPbdd9/W9U866ST69etHv3796N+/P1/84hcBGDZsGEuXLgUKSefb3/42b731Fps3b2bcuHE7jOvuu+9myZIlzJ8/n82bN/P73/+ec845p3X5+++/v8PPMLOuyQmlwvJ6Hsoee+wBFAYCNDc371QcxesD9OjRo3W+R48erZ934YUXMnfuXIYPH87MmTN5/PHHO9zG8uXLmT59Ok8++SQ9e/bko48+YsCAAe1eVzKz6tJ9E0oXHOY7btw4vvOd73D++efTt29f1q5dS+/evTnhhBO48MILmTZtGs3NzTz00EPbJZ1SHH/88cyZM4eTTz6ZlStX8qc//YnDDjuMJUuWdCreTZs2MXDgQD788EPmzJnDoEGD2q371ltvcd555zF79mxqamoA+PjHP86QIUO49957Oeecc4gIli5dyvDhwzsVj5lVVvdNKF1Q3s9DmTJlCpdccgnDhg2jV69ezJw5c6uWyM669tprOeaYY6ipqeGYY45h06ZN7dZ98MEHWbNmDRdffHFrWUNDA3PmzOGSSy7huuuu48MPP+Tcc891QjGrUoqISseQubq6uqivr9+qbMWKFRx++OEViihb06dPp2/fvq0XtruD3en3Z1urveKX25V1xQfgdQeSFkdEXWfX9ygvMzPLhLu8qlDL81CmTp3K7373u62WXXbZZUyaNKkCUZlZd9etEkpEtH6hbndw6623VjqEstgdu2XNdkfdpsurT58+bNiwwSenKhMRbNiwgT59+lQ6FDPbgW7TQhk8eDBNTU2sX7++0qHYTurTpw+DBw+udBhmtgPdJqH07t2bIUOGVDoMM7PdVrfp8jIzs3w5oZiZWSacUMzMLBNOKGZmlgknFDMzy4QTipmZZcIJxczMMuGEYmZmmXBCMTOzTOSeUCT1lPSspF+k+SGSnpG0StLdkj6WyvdI86vS8tqiz5iWyl+StOMHl5uZWdmVo4VyGbCiaP57wM0R8SngTeCiVH4R8GYqvznVQ9IRwLnAZ4DxwL9I6lmGuM3MbCfkmlAkDQY+D/xrmhdwMnBfqjILODNNn5HmSctPSfXPAO6KiPcj4o/AKuDoPOM2M7Odl3cL5RbgH4CP0vx+wFsR0Zzmm4BBaXoQ8CpAWv52qt9a3sY6rSRNllQvqd53FDYzK7/cEoqkLwDrImJxXtsoFhG3RURdRNTV1NSUY5NmZlYkz9vXfw6YIOl0oA/wceCfgQGSeqVWyGBgbaq/FjgQaJLUC+gPbCgqb1G8jpmZdRG5tVAiYlpEDI6IWgoX1X8TEecDjwFfTtUmAg+m6XlpnrT8N1F4vOI84Nw0CmwIMBRYmFfcZmbWOZV4wNblwF2SrgOeBe5I5XcAP5W0CthIIQkREc9Lugd4AWgGpkbEX8oftpmZdaQsCSUiHgceT9Ov0MYorYjYApzTzvrXA9fnF6GZme0qf1PezMwy4YRiZmaZcEIxM7NMOKGYmVkmnFDMzCwTTihmZpYJJxQzM8uEE4qZmWXCCcXMzDLhhGJmZplwQjEzs0w4oZiZWSacUMzMLBNOKGZmlgknFDMzy4QTipmZZcIJxczMMuGEYmZmmXBCMTOzTDihmJlZJpxQzMwsE04oZmaWCScUMzPLhBOKmZllwgnFzMwy4YRiZmaZcEIxM7NMlJRQJA3LOxAzM6tupbZQ/kXSQklTJPXPNSIzM6tKJSWUiDgeOB84EFgs6eeSTs01MjMzqyolX0OJiEbg28DlwF8DP5D0oqSz8wrOzMyqR6nXUI6UdDOwAjgZ+GJEHJ6mb84xPjMzqxKltlB+CCwBhkfE1IhYAhARf6bQatmOpD7pustzkp6X9N1UPkTSM5JWSbpb0sdS+R5pflVaXlv0WdNS+UuSxnX+xzUzs7yUmlA+D/w8It4DkNRD0l4AEfHTdtZ5Hzg5IoYDI4DxksYA3wNujohPAW8CF6X6FwFvpvKbUz0kHQGcC3wGGE9hgEDPnfsxzcwsb6UmlEeAPYvm90pl7YqCzWm2d3oFhW6y+1L5LODMNH1GmictP0WSUvldEfF+RPwRWAUcXWLcZmZWJqUmlD5FyYE0vdeOVpLUU1IDsA5YALwMvBURzalKEzAoTQ8CXk2f3wy8DexXXN7GOsXbmiypXlL9+vXrS/yxzMwsK6UmlP+QNKplRtJRwHs7Wiki/hIRI4DBFFoVn+5UlCWIiNsioi4i6mpqavLajJmZtaNXifW+Cdwr6c+AgL8CvlrqRiLiLUmPAccCAyT1Sq2QwcDaVG0the+5NEnqBfQHNhSVtyhex8zMuohSv9i4iELr4hLg68DhEbG4o3Uk1UgakKb3BE6lMOz4MeDLqdpE4ME0PS/Nk5b/JiIilZ+bRoENAYYCC0v78czMrFxKbaEAjAZq0zqjJBERszuoPxCYlUZk9QDuiYhfSHoBuEvSdcCzwB2p/h3ATyWtAjZSGNlFRDwv6R7gBaAZmBoRf9mJuM3MrAxKSiiSfgocAjQALSfzANpNKBGxFBjZRvkrtDFKKyK2AOe081nXA9eXEquZmVVGqS2UOuCI1AVlZma2nVJHeS2ncCHezMysTaW2UPYHXpC0kMI34AGIiAm5RGVmZlWn1IQyPc8gzMys+pWUUCLiCUkHAUMj4pF0Hy/fT8vMzFqVevv6iyncX+snqWgQMDevoMzMrPqUelF+KvA54B1ofdjWJ/IKyszMqk+pCeX9iPigZSbdGsVDiM3MrFWpCeUJSf8I7JmeJX8v8FB+YZmZWbUpNaFcAawHlgFfAx6mnSc1mplZ91TqKK+PgNvTy8zMbDul3svrj7RxzSQiDs48IjMzq0o7cy+vFn0o3MRx3+zDMTOzalXq81A2FL3WRsQtwOdzjs3MzKpIqV1eo4pme1BosezMs1TMzGw3V2pS+Kei6WZgNfCVzKMxM7OqVeoor5PyDsTMzKpbqV1e/6Oj5RFxUzbhmJlZtdqZUV6jgXlp/ovAQqAxj6DMzKz6lJpQBgOjImITgKTpwC8j4m/zCszMzKpLqbdeOQD4oGj+g1RmZmYGlN5CmQ0slPRAmj8TmJVPSGZmVo1KHeV1vaRfAcenokkR8Wx+YZmZWbUptcsLYC/gnYj4Z6BJ0pCcYjIzsypU6iOArwYuB6alot7Az/IKyszMqk+pLZSzgAnAfwBExJ+BfnkFZWZm1afUhPJBRATpFvaS9s4vJDMzq0alJpR7JP0EGCDpYuAR/LAtMzMrssNRXpIE3A18GngHOAy4KiIW5BybmZlVkR0mlIgISQ9HxDDAScTMzNpUapfXEkmjc43EzMyqWqnflD8G+FtJqymM9BKFxsuReQVmZmbVpcMWiqRPpslxwMHAyRTuNPyF9N7RugdKekzSC5Kel3RZKt9X0gJJjel9n1QuST+QtErS0uKnREqamOo3SprY+R/XzMzysqMur7kAEbEGuCki1hS/drBuM/A/I+IIYAwwVdIRwBXAoxExFHg0zQOcBgxNr8nAj6GQgICrKbSSjgaubklCZmbWdewooaho+uCd+eCIeC0ilqTpTcAKYBBwBv95Y8lZFG40SSqfHQVPUxiiPJBC62hBRGyMiDcpDAwYvzOxmJlZ/naUUKKd6Z0iqRYYCTwDHBARr6VFr/Oft8EfBLxatFpTKmuvfNttTJZUL6l+/fr1nQ3VzMw6aUcJZbikdyRtAo5M0+9I2iTpnVI2IKkv8G/ANyNiq3WKv32/qyLitoioi4i6mpqaLD7SzMx2QoejvCKi5658uKTeFJLJnIi4PxW/IWlgRLyWurTWpfK1wIFFqw9OZWuBE7cpf3xX4jIzs+ztzO3rd0r6hv0dwIqIuKlo0TygZaTWRODBovIL0mivMcDbqWvs18BYSfuki/FjU5mZmXUhpX4PpTM+B/wdsExSQyr7R+AGCvcGuwhYA3wlLXsYOB1YBbwLTAKIiI2SrgUWpXrXRMTGHOM2M7NOyC2hRMRTbD1KrNgpbdQPYGo7nzUDmJFddGZmlrXcurzMzKx7cUIxM7NMOKGYmVkmnFDMzCwTTihmZpYJJxQzM8uEE4qZmWXCCcXMzDLhhGJmZplwQjEzs0w4oZiZWSacUMzMLBNOKGZmlgknFDMzy4QTipmZZcIJxczMMuGEYmZmmcjzEcBmVaP2il+2Wb76hs+XORKz6uUWipmZZcIJxczMMuGEYmZmmXBCMTOzTDihmJlZJpxQzMwsE04oZmaWCScUMzPLhBOKmZllwgnFzMwy4VuvWJfR1u1PfOsTs+rhFoqZmWXCCcXMzDLhhGJmZpnILaFImiFpnaTlRWX7SlogqTG975PKJekHklZJWippVNE6E1P9RkkT84rXzMx2TZ4tlJnA+G3KrgAejYihwKNpHuA0YGh6TQZ+DIUEBFwNHAMcDVzdkoTMzKxryS2hRMSTwMZtis8AZqXpWcCZReWzo+BpYICkgcA4YEFEbIyIN4EFbJ+kzMysCyj3NZQDIuK1NP06cECaHgS8WlSvKZW1V25mZl1MxS7KR0QAkdXnSZosqV5S/fr167P6WDMzK1G5E8obqSuL9L4ula8FDiyqNziVtVe+nYi4LSLqIqKupqYm88DNzKxj5U4o84CWkVoTgQeLyi9Io73GAG+nrrFfA2Ml7ZMuxo9NZWZm1sXkdusVSXcCJwL7S2qiMFrrBuAeSRcBa4CvpOoPA6cDq4B3gUkAEbFR0rXAolTvmojY9kK/mZl1AbkllIg4r51Fp7RRN4Cp7XzODGBGhqGZmVkO/E15MzPLhBOKmZllwgnFzMwy4YRiZmaZ8AO2zHYDfjiZdQVOKNaqrZMS+MRkZqVxl5eZmWXCCcXMzDLhhGJmZplwQjEzs0w4oZiZWSY8yquL8UirzvPQWbPKcgvFzMwy4RaKmVUtt+i7FrdQzMwsE26h7GZ8HcGsdG7hZMsJpQ0+KZuZ7Tx3eZmZWSacUMzMLBPu8sqY+2TNuo9Kdo93xXONE4pZF1DNJ6aueGKzynBCMcuAT6pmTihmZhWzu40odUJpw+o+/62N0rd3Yd1Kr1+ebTO9fzvl+ce+q+tX8+9tV9ev5p+9mmPf1fV3+e81Bx7lZWZmmXBCMTOzTDihmJlZJpxQzMwsE04oZmaWCScUMzPLhBOKmZllwgnFzMwy4YRiZmaZqJqEImm8pJckrZJ0RaXjMTOzrVVFQpHUE7gVOA04AjhP0hGVjcrMzIpVRUIBjgZWRcQrEfEBcBdwRoVjMjOzIoqISsewQ5K+DIyPiP+e5v8OOCYiLi2qMxmYnGYPA17KYNP7A/+ewefkpSvH59g6ryvH59g6ryvH1xLbQRFR09kP2W3uNhwRtwG3ZfmZkuojoi7Lz8xSV47PsXVeV47PsXVeV44vq9iqpctrLXBg0fzgVGZmZl1EtSSURcBQSUMkfQw4F5hX4ZjMzKxIVXR5RUSzpEuBXwM9gRkR8XwZNp1pF1oOunJ8jq3zunJ8jq3zunJ8mcRWFRflzcys66uWLi8zM+vinFDMzCwTTijs+LYukvaQdHda/oyk2jLFdaCkxyS9IOl5SZe1UedESW9Lakivq8oRW9H2V0talrZd38ZySfpB2ndLJY0qU1yHFe2TBknvSPrmNnXKuu8kzZC0TtLyorJ9JS2Q1Jje92ln3YmpTqOkiWWK7fuSXky/twckDWhn3Q6PgZximy5pbdHv7vR21s31lk3txHZ3UVyrJTW0s26u+y1to81zSG7HXUR06xeFi/wvAwcDHwOeA47Yps4U4P+k6XOBu8sU20BgVJruB6xsI7YTgV9UcP+tBvbvYPnpwK8AAWOAZyr0O36dwpe2KrbvgBOAUcDyorL/DVyRpq8AvtfGevsCr6T3fdL0PmWIbSzQK01/r63YSjkGcoptOvCtEn7vHf5t5xHbNsv/CbiqEvstbaPNc0hex51bKKXd1uUMYFaavg84RZLyDiwiXouIJWl6E7ACGJT3djN2BjA7Cp4GBkgaWOYYTgFejog1Zd7uViLiSWDjNsXFx9Ys4Mw2Vh0HLIiIjRHxJrAAGJ93bBExPyKa0+zTFL7/VXbt7LdS5H7Lpo5iS+eIrwB3ZrnNndHBOSSX484JpbBzXy2ab2L7k3ZrnfQH9jawX1miS1I320jgmTYWHyvpOUm/kvSZcsYFBDBf0uJ0+5ttlbJ/83Yu7f9RV3LfARwQEa+l6deBA9qo0xX24d9TaGm2ZUfHQF4uTd1xM9rpsqn0fjseeCMiGttZXtb9ts05JJfjzgmlCkjqC/wb8M2IeGebxUsodOUMB34IzC1zeMdFxCgKd4KeKumEMm+/Qyp8EXYCcG8biyu977YShX6GLjeOX9KVQDMwp50qlTgGfgwcAowAXqPQtdTVnEfHrZOy7beOziFZHndOKKXd1qW1jqReQH9gQzmCk9SbwoEwJyLu33Z5RLwTEZvT9MNAb0n7lyO2tM216X0d8ACFboZilb5tzmnAkoh4Y9sFld53yRstXYDpfV0bdSq2DyVdCHwBOD+deLZTwjGQuYh4IyL+EhEfAbe3s81K7rdewNnA3e3VKdd+a+cckstx54RS2m1d5gEtIxy+DPymvT+uLKU+2DuAFRFxUzt1/qrleo6koyn8TsuV7PaW1K9lmsJF3OXbVJsHXKCCMcDbRU3tcmj3v8RK7rsixcfWRODBNur8GhgraZ/UtTM2leVK0njgH4AJEfFuO3VKOQbyiK34OtxZ7Wyzkrds+hvgxYhoamthufZbB+eQfI67PEcYVMuLwkiklRRGhFyZyq6h8IcE0IdCl8kqYCFwcJniOo5CU3Qp0JBepwNfB76e6lwKPE9hBMvTwH8t4347OG33uRRDy74rjk8UHo72MrAMqCtjfHtTSBD9i8oqtu8oJLbXgA8p9EdfROFa3KNAI/AIsG+qWwf8a9G6f5+Ov1XApDLFtopCH3rLsdcy0vG/AA93dAyUIbafpuNpKYWT48BtY0vz2/1t5x1bKp/ZcpwV1S3rfkvbae8ckstx51uvmJlZJtzlZWZmmXBCMTOzTDihmJlZJpxQzMwsE04oZmaWCScUMzPLhBOKmZll4v8DYYplxUSESrEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXwxrNdCShlc",
        "colab_type": "text"
      },
      "source": [
        "# Model feature extractor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etwOZM2GSjL_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def feature_extraction(cut_layer, dense, dropouts, im_size, loss, optimizer, metrics, verbose=False):\n",
        "  '''\n",
        "    viene definito il layer di cut della vgg e vengono aggiunti una serie di layer densi prima dell'output\n",
        "  '''\n",
        "  print('porco ziooooooooooo')\n",
        "  base_model = VGG16(weights = 'imagenet', input_shape=im_size + (3,), include_top = False)\n",
        "\n",
        "  x = base_model.layers[cut_layer].output\n",
        "  x = GlobalAveragePooling2D()(x)\n",
        "  for layer,dropout in zip(dense,dropouts):\n",
        "    x = Dense(layer, activation='relu')(x)\n",
        "    x = Dropout(dropout)(x)\n",
        "  predictions = Dense(20, activation = 'softmax')(x)\n",
        "  model = Model(input = base_model.input, output = predictions)\n",
        "\n",
        "  for layer in base_model.layers:\n",
        "    layer.trainable=False\n",
        "    \n",
        "  if verbose:\n",
        "    model.summary()\n",
        "\n",
        "  model.compile(loss=loss, \n",
        "                optimizer=optimizer, \n",
        "                metrics=metrics\n",
        "  )\n",
        "      \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXNhkv10FLgG",
        "colab_type": "text"
      },
      "source": [
        "# Model fine tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8EhFgVcFTa7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fine_tuning(freeze_to, dense, dropouts, im_size, loss, optimizer, metrics, verbose=False):\n",
        "  '''\n",
        "    vengono definiti i layer della vgg trainabili e i dense layer prima dell'output\n",
        "  '''\n",
        "  \n",
        "  base_model = VGG16(weights = 'imagenet', input_shape=im_size + (3,), include_top = False)\n",
        "\n",
        "  x = base_model.layers[-1].output\n",
        "  x = GlobalAveragePooling2D()(x) \n",
        "\n",
        "  for layer,dropout in zip(dense,dropouts): \n",
        "    x = Dense(layer, activation='relu')(x)\n",
        "    x = Dropout(dropout)(x)\n",
        "  predictions = Dense(20, activation = 'softmax')(x)\n",
        "  model = Model(input = base_model.input, output = predictions)\n",
        "\n",
        "  for layer in base_model.layers[:freeze_to]:\n",
        "    layer.trainable=False\n",
        "  for layer in model.layers[freeze_to:]: # layer su cui applicare fine tuning\n",
        "    layer.trainable = True\n",
        "\n",
        "  if verbose:\n",
        "    model.summary()\n",
        "\n",
        "  model.compile(loss=loss, \n",
        "                optimizer=optimizer, \n",
        "                metrics=metrics\n",
        "  )\n",
        "      \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7fUcTZApdLB",
        "colab_type": "text"
      },
      "source": [
        "# Plot performance epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXAGRUyipg-R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_performance(history):\n",
        "  plt.plot(history.history['categorical_accuracy'])\n",
        "  plt.plot(history.history['val_categorical_accuracy'])\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'val'], loc='upper left')\n",
        "  plt.show()\n",
        "  # summarize history for loss\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'val'], loc='upper left')\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gpMHcy2SpB7",
        "colab_type": "text"
      },
      "source": [
        "# Util to load train/val pickle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSiSo2KHSzmo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_pickle(path_file, one_hot=True):\n",
        "\n",
        "  file = open(path_file, 'rb')\n",
        "  data = pickle.load(file)\n",
        "  file.close()\n",
        "  images = data[0]\n",
        "  labels = data[1]\n",
        "\n",
        "  if one_hot:\n",
        "    enc = OneHotEncoder(handle_unknown='ignore')\n",
        "    enc.fit(labels.reshape(-1,1))\n",
        "    labels_one_hot = enc.transform(labels.reshape(-1,1)).toarray()\n",
        "\n",
        "  return images, labels_one_hot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22ReLsjTEeCZ",
        "colab_type": "text"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "se7bKuAJbulU",
        "colab_type": "text"
      },
      "source": [
        "## Set up generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDNTdRNpGBpI",
        "colab_type": "code",
        "outputId": "408ec0a0-e309-4300-f132-64d5ce4045e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#base_net = 'vgg16'\n",
        "\n",
        "augment_params = dict(  rotation_range=30,\n",
        "                        horizontal_flip=True,\n",
        "                        zoom_range=0.1\n",
        "                      )\n",
        "\n",
        "train_gen, val_gen, test_gen = create_generator(batch_size=BATCH_SIZE, \n",
        "                                                im_size=IM_SIZE, \n",
        "                                                train_directory=PATH_IMAGES_CROPPED_TRAIN_BALANCED, \n",
        "                                                val_directory=PATH_IMAGES_CROPPED_VAL_TEST_BALANCED, \n",
        "                                                labels=LABELS,\n",
        "                                                validation_split=0.7,\n",
        "                                                aug=True,\n",
        "                                                augment_params=augment_params\n",
        "                                                )\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 5680 images belonging to 20 classes.\n",
            "Found 2341 images belonging to 20 classes.\n",
            "Found 5432 images belonging to 20 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oR4bO3SF_-EN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x,y = train_gen.next()\n",
        "for i in range(0,10):\n",
        "    image = x[i]\n",
        "    plt.figure()\n",
        "    plt.imshow(image)\n",
        "    plt.title(from_onehot_to_label(mapping, y[i]))\n",
        "    plt.show\n",
        "\n",
        "train_gen.reset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNm8VPigbjay",
        "colab_type": "text"
      },
      "source": [
        "## Analyze frequency label in generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7QYMSTWbaof",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels, freq = get_frequency_from_generator(train_gen)\n",
        "plot_label_frequency(labels, freq)\n",
        "freq_normalize, weigh = normalize_frequency(freq)\n",
        "#plot_label_frequency(labels, freq_normalize)\n",
        "plot_stacked_bar_freq(labels, freq, freq_normalize)\n",
        "'''\n",
        "# dict for weigh model\n",
        "weigh_class = {}\n",
        "for label,i in zip(labels, weigh):\n",
        "  weigh_class[from_label_to_number(mapping, label)] = i\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqmfekWbn-jf",
        "colab_type": "code",
        "outputId": "2b9c4842-ba68-44a6-f120-148988d54c32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "weigh_class"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 0.674468085106383,\n",
              " 1: 0.7731707317073171,\n",
              " 2: 0.535472972972973,\n",
              " 3: 0.6240157480314961,\n",
              " 4: 0.4232309746328438,\n",
              " 5: 1.0,\n",
              " 6: 0.2661628883291352,\n",
              " 7: 0.5205254515599343,\n",
              " 8: 0.2175703500343171,\n",
              " 9: 0.8929577464788733,\n",
              " 10: 0.8498659517426274,\n",
              " 11: 0.4127604166666667,\n",
              " 12: 0.8408488063660478,\n",
              " 13: 0.8453333333333334,\n",
              " 14: 0.06315999203028491,\n",
              " 15: 0.5691202872531418,\n",
              " 16: 0.6227897838899804,\n",
              " 17: 0.7944862155388471,\n",
              " 18: 0.9694189602446484,\n",
              " 19: 0.7694174757281553}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAg57-_ib21t",
        "colab_type": "text"
      },
      "source": [
        "## Set up iper-parameter for fit and model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QSFiR2Ab6v1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss = 'categorical_crossentropy'\n",
        "optimizer = 'adam'\n",
        "reduce_on_plateau = ReduceLROnPlateau(monitor='val_loss', factor=0.01, patience=3, \n",
        "                                      verbose=1, mode='auto', min_delta=0.0001, \n",
        "                                      cooldown=0, min_lr=0.000001)\n",
        "early_stop = EarlyStopping(monitor='val_loss', \n",
        "                           patience=5, \n",
        "                           verbose=1,\n",
        "                           restore_best_weights=True\n",
        "                           )\n",
        "model_checkpoint = ModelCheckpoint(filepath='best_weights.h5', \n",
        "                                   monitor='val_loss', \n",
        "                                   save_best_only=True)\n",
        "metrics = ['categorical_accuracy']\n",
        "epochs = 50\n",
        "steps_per_epoch = ceil(train_gen.n / train_gen.batch_size)\n",
        "validation_steps = ceil(val_gen.n / val_gen.batch_size)\n",
        "test_steps = ceil(test_gen.n / test_gen.batch_size)\n",
        "dense = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pz6Y1ic80Rz2",
        "colab_type": "text"
      },
      "source": [
        "## Create model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQ2obP6f0UDz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model feauture extraction\n",
        "model = feature_extraction(cut_layer=-1,\n",
        "                           dense=[512, 256, 128, 64],\n",
        "                           dropouts=[0.2,0.2,0.2,0.2],\n",
        "                           im_size=IM_SIZE,\n",
        "                           loss='categorical_crossentropy',\n",
        "                           optimizer=optimizer,\n",
        "                           metrics=metrics,\n",
        "                           verbose=False\n",
        "                           )\n",
        "# TODO model fine tuning\n",
        "'''\n",
        "model = fine_tuning(freeze_to=15,\n",
        "                    dense=[128, 64],\n",
        "                    dropouts=[0.2,0.2],\n",
        "                    im_size=IM_SIZE,\n",
        "                    loss='categorical_crossentropy,\n",
        "                    optimizer=optimizer,\n",
        "                    metrics=metrics,\n",
        "                    verbose=False)\n",
        "'''\n",
        "for index,layer in enumerate(model.layers):\n",
        "  print(index, ' ', layer.name, ' ', layer.trainable)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8U2BptkC0JcN",
        "colab_type": "text"
      },
      "source": [
        "## Fit prova"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9HmoEWOUkkU",
        "colab_type": "code",
        "outputId": "d605aac0-b9f7-412f-cb71-0fc4b33c1d9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "net_history = model.fit_generator(train_gen, epochs=2, verbose=1,\n",
        "                                  validation_data = val_gen,\n",
        "                                  steps_per_epoch = steps_per_epoch,\n",
        "                                  validation_steps = validation_steps,\n",
        "                                  callbacks = [early_stop, reduce_on_plateau]                 \n",
        "                                  )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "89/89 [==============================] - 3891s 44s/step - loss: 2.7898 - categorical_accuracy: 0.2632 - val_loss: 0.6715 - val_categorical_accuracy: 0.6886\n",
            "Epoch 2/2\n",
            "89/89 [==============================] - 27s 300ms/step - loss: 1.5000 - categorical_accuracy: 0.5521 - val_loss: 0.2958 - val_categorical_accuracy: 0.7715\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqNkpcddZLB0",
        "colab_type": "code",
        "outputId": "ef00aed0-cd17-437d-be1b-dfb09fb1c277",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        }
      },
      "source": [
        "plot_performance(net_history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GnbuGQ72sMR",
        "colab_type": "code",
        "outputId": "a7ae14e1-2466-42e7-cd9b-85c2e3669582",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "loss, accuracy = model.evaluate(test_gen, steps=test_steps)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "37/37 [==============================] - 4s 115ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3njsQpf7MTvn",
        "colab_type": "code",
        "outputId": "043bff4b-5d67-48b1-9efa-1d3849dfd561",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(f'Loss: {loss}')\n",
        "print(f'Accuracy: {accuracy}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 0.295815646648407\n",
            "Accuracy: 0.7714651823043823\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwte93dQmMzJ",
        "colab_type": "text"
      },
      "source": [
        "# AutoML"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrlxqfdszCbf",
        "colab_type": "text"
      },
      "source": [
        "## Create dataframe for k-cross-fold optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9T3GwCMTMVtZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dataframe_path_class(path_images):\n",
        "  df = pd.DataFrame(columns=['absolute_path', 'class'])\n",
        "  list_dir = os.listdir(path_images)\n",
        "  for sub_dir in list_dir:\n",
        "    list_images = os.listdir(os.path.join(path_images, sub_dir))  \n",
        "    for image in list_images:\n",
        "      df = df.append({'absolute_path': os.path.join(path_images, sub_dir, image),\n",
        "                      'class' : str(sub_dir)} , \n",
        "                     ignore_index=True)\n",
        "  return df\n",
        "\n",
        "df_train = dataframe_path_class(PATH_IMAGES_CROPPED_TRAIN_BALANCED)\n",
        "df_val_test = dataframe_path_class(PATH_IMAGES_CROPPED_VAL_TEST_BALANCED)\n",
        "\n",
        "# creo df_test che uso solo dopo ottimizzazione iperparametri\n",
        "df_val, df_test = train_test_split(df_val_test, test_size=0.7)\n",
        "\n",
        "# unisco df_val a df_train per avere un train generale su cui eseguire k-cross-fold validation\n",
        "df_train = df_train.append(df_val, ignore_index=True)\n",
        "\n",
        "# salvo i due dataframe\n",
        "df_train.to_csv(PATH_OPTIMIZATION + 'train_k_cross.csv')\n",
        "df_test.to_csv(PATH_OPTIMIZATION + 'test.csv')\n",
        "\n",
        "print(df_train.shape, df_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KaAO8omzV73",
        "colab_type": "text"
      },
      "source": [
        "## k-cross-fold to optimize "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YRPDR-AKNLB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cross_validation_optimization(cfg):\n",
        "\n",
        "  kf = KFold(n_splits=2, random_state=None, shuffle=True) # 3 \n",
        "\n",
        "  #accuracy_on_fold = []\n",
        "  losses = []\n",
        "\n",
        "  i = 1\n",
        "  for train_index, val_index in kf.split(df_train):\n",
        "\n",
        "    #gc.collect()\n",
        "    K.clear_session()\n",
        "\n",
        "    # creo modello sulla base delle configurazioni in cfg\n",
        "    if cfg['number_layer'] == 1:\n",
        "      dense = [cfg['number_first_layer']]\n",
        "      dropouts = [cfg['dropout_first_layer']]\n",
        "    elif cfg['number_layer'] == 2:\n",
        "      dense = [cfg['number_first_layer'], \n",
        "               cfg['number_second_layer']]\n",
        "      dropouts = [cfg['dropout_first_layer'],\n",
        "                 cfg['dropout_second_layer']]\n",
        "    elif cfg['number_layer'] == 3:\n",
        "      dense = [cfg['number_first_layer'], \n",
        "               cfg['number_second_layer'], \n",
        "               cfg['number_third_layer']]\n",
        "      dropouts = [cfg['dropout_first_layer'],\n",
        "                 cfg['dropout_second_layer'],\n",
        "                 cfg['dropout_third_layer']]\n",
        "    elif cfg['number_layer'] == 4:\n",
        "      dense = [cfg['number_first_layer'], \n",
        "               cfg['number_second_layer'], \n",
        "               cfg['number_third_layer'], \n",
        "               cfg['number_fourth_layer']]\n",
        "      dropouts = [cfg['dropout_first_layer'],\n",
        "                 cfg['dropout_second_layer'],\n",
        "                 cfg['dropout_third_layer'],\n",
        "                 cfg['dropout_third_layer']]\n",
        "\n",
        "    if cfg['optimizier'] == 'SGD':\n",
        "      optimizer = SGD(learning_rate=cfg['learning_rate'], momentum=cfg['momentum'])\n",
        "      if cfg['nestorov'] == True:\n",
        "        optimizer.nestorov = True\n",
        "    elif cfg['optimizier'] == 'adam':\n",
        "      optimizer = Adam(learning_rate=cfg['learning_rate'])\n",
        "\n",
        "    print('create generator')\n",
        "    trainGenerator = ImageDataGenerator(preprocessing_function=preprocess_input_vgg16,\n",
        "                                        rotation_range=30,\n",
        "                                        horizontal_flip=True,\n",
        "                                        zoom_range=0.1\n",
        "                                        )\n",
        "    valGenerator = ImageDataGenerator(preprocessing_function=preprocess_input_vgg16)\n",
        "    \n",
        "    print('create train val and test')\n",
        "    trainData = df_train.iloc[train_index,:]\n",
        "    trainData, valData = train_test_split(trainData, test_size=0.2)\n",
        "    testData = df_train.iloc[val_index,:]\n",
        "\n",
        "    print(\"=========================================\")\n",
        "    print(\"====== K Fold Validation step => %d =======\" % (i))\n",
        "    print(\"=========================================\")\n",
        "\n",
        "    print('flow from train dataframe')\n",
        "    train_gen = valGenerator.flow_from_dataframe(\n",
        "      dataframe = trainData,\n",
        "      x_col='absolute_path',\n",
        "      y_col='class',\n",
        "      shuffle=True,\n",
        "      class_mode='categorical',\n",
        "      target_size=IM_SIZE,\n",
        "      batch_size=BATCH_SIZE,\n",
        "      classes=LABELS,\n",
        "    )\n",
        "\n",
        "    print('flow from val dataframe')\n",
        "    val_gen = valGenerator.flow_from_dataframe(\n",
        "      dataframe = valData,\n",
        "      x_col='absolute_path',\n",
        "      y_col='class',\n",
        "      shuffle=False,\n",
        "      class_mode='categorical',\n",
        "      target_size=IM_SIZE,\n",
        "      batch_size=BATCH_SIZE,\n",
        "      classes=LABELS,\n",
        "    )\n",
        "\n",
        "    print('flow from test dataframe')\n",
        "    test_gen = valGenerator.flow_from_dataframe(\n",
        "      dataframe = testData,\n",
        "      x_col='absolute_path',\n",
        "      y_col='class',\n",
        "      shuffle=False,\n",
        "      class_mode='categorical',\n",
        "      target_size=IM_SIZE,\n",
        "      batch_size=BATCH_SIZE,\n",
        "      classes=LABELS,\n",
        "    ) \n",
        "    \n",
        "    step_train = ceil(train_gen.n/train_gen.batch_size)\n",
        "    step_val = ceil(val_gen.n/val_gen.batch_size)\n",
        "    step_test = ceil(test_gen.n/test_gen.batch_size)\n",
        "\n",
        "    reduce_on_plateau = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, \n",
        "                                          verbose=1, mode='auto', min_delta=0.0001, \n",
        "                                          cooldown=0, min_lr=0.00001)\n",
        "    early_stop = EarlyStopping(monitor='val_loss', \n",
        "                              patience=5, \n",
        "                              verbose=1,\n",
        "                              restore_best_weights=True\n",
        "                              )\n",
        "\n",
        "    if to_optimize == 'feature_extraction':\n",
        "      print('feature extraction')\n",
        "      model = feature_extraction(cut_layer=cfg['cut_layer'],\n",
        "                                dense=dense,\n",
        "                                dropouts=dropouts,\n",
        "                                im_size=IM_SIZE,\n",
        "                                loss='categorical_crossentropy',\n",
        "                                optimizer=optimizer,\n",
        "                                metrics=['categorical_accuracy'],\n",
        "                                verbose=False\n",
        "                                )\n",
        "    if to_optimize == 'fine_tuning':\n",
        "      model = fine_tunining(freeze_to=cfg['freeze_to'],\n",
        "                                        dense=dense,\n",
        "                                        dropouts=dropouts,\n",
        "                                        im_size=IM_SIZE,\n",
        "                                        loss='categorical_crossentropy',\n",
        "                                        optimizer=optimizer,\n",
        "                                        metrics=['categorical_accuracy'],\n",
        "                                        verbose=False) \n",
        "    \n",
        "    print(\"Inizio fit\")\n",
        "    \n",
        "    history = model.fit_generator(generator=train_gen, \n",
        "                                  epochs=50, \n",
        "                                  verbose=1,\n",
        "                                  validation_data = val_gen,\n",
        "                                  steps_per_epoch = step_train,\n",
        "                                  validation_steps = step_val,\n",
        "                                  callbacks = [early_stop, reduce_on_plateau],\n",
        "                                  )\n",
        "\n",
        "    # test on testData\n",
        "    print(\"Predict on test\")\n",
        "    '''\n",
        "    predictions = model.predict_generator(test_gen, \n",
        "                                          steps=step_test,\n",
        "                                          )\n",
        "    \n",
        "    accuracy_on_fold.append(accuracy_score(y_true=test_gen.labels, \n",
        "                                          y_pred=np.argmax(predictions, axis=1)\n",
        "                                          )\n",
        "                          )\n",
        "    '''\n",
        "    # evaluate test and return loss\n",
        "    loss, _ = model.evaluate_generator(test_gen, steps=step_test)\n",
        "    losses.append(loss)\n",
        "    i+=1\n",
        "\n",
        "  return np.mean(losses) # minimizzo loss media sulla 3-cross validation\n",
        "  #return 1 - np.mean(accuracy_on_fold)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJrfI4v50nnV",
        "colab_type": "text"
      },
      "source": [
        "## Configuration space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgQ0AAgxmP7i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_configuration_space(to_optimize):\n",
        "\n",
        "  cs = ConfigurationSpace()\n",
        "\n",
        "  # iper-parametri da ottimizzare\n",
        "  '''\n",
        "    Optimizier: categorical ('adam', 'SGD')\n",
        "    Learning_rate: uniform_float [0.01, 0.00001]\n",
        "    Momentum: categorical and condition (True,False) if 'SGD'\n",
        "    Nestorov: uniform_float and condition [0.9, 0.8] if 'SGD'\n",
        "    Dropout: uniform_float [0.2,0.4]\n",
        "    Number_fully_connected: categorical (1,2)\n",
        "    Number_neurons_first_layer: uniform_integer [64, 1024]\n",
        "    Numebr_neurons_second_layer: uniform_integer and condition [64,1024] if number_fully_connected==2\n",
        "  '''\n",
        "  optimizier_param = CategoricalHyperparameter(name='optimizier',\n",
        "                                              choices=['adam', 'SGD'],\n",
        "                                              default_value='SGD')\n",
        "\n",
        "  lr_param = UniformFloatHyperparameter(name=\"learning_rate\", \n",
        "                                        lower=0.000001, \n",
        "                                        upper=0.001, \n",
        "                                        default_value=0.001)\n",
        "  momentum_param = UniformFloatHyperparameter(name='momentum',\n",
        "                                              lower=0.1,\n",
        "                                              upper=0.9,\n",
        "                                              default_value=0.5\n",
        "                                              )\n",
        "  cond_momentum = InCondition(child=momentum_param, \n",
        "                              parent=optimizier_param, \n",
        "                              values=['SGD'])\n",
        "\n",
        "  nestorov_param = CategoricalHyperparameter(name=\"nestorov\", \n",
        "                                            choices=[True, False], \n",
        "                                            default_value=False)\n",
        "  cond_nestorov = InCondition(child=nestorov_param, \n",
        "                              parent=optimizier_param, \n",
        "                              values=['SGD'])\n",
        "  \n",
        "  number_layer_param = UniformIntegerHyperparameter(name='number_layer',\n",
        "                                          lower=1,\n",
        "                                          upper=4,\n",
        "                                          default_value=2)\n",
        "  \n",
        "  dropout_first_layer = UniformFloatHyperparameter(name='dropout_first_layer',\n",
        "                                            lower=0.2,\n",
        "                                            upper=0.4,\n",
        "                                            default_value=0.2)\n",
        "  dropout_second_layer = UniformFloatHyperparameter(name='dropout_second_layer',\n",
        "                                            lower=0.2,\n",
        "                                            upper=0.4,\n",
        "                                            default_value=0.2)\n",
        "  dropout_third_layer = UniformFloatHyperparameter(name='dropout_third_layer',\n",
        "                                            lower=0.2,\n",
        "                                            upper=0.4,\n",
        "                                            default_value=0.2)\n",
        "  dropout_fourth_layer = UniformFloatHyperparameter(name='dropout_fourth_layer',\n",
        "                                            lower=0.2,\n",
        "                                            upper=0.4,\n",
        "                                            default_value=0.2)\n",
        "\n",
        "  neurons_first_layer_param = UniformIntegerHyperparameter(name='number_first_layer',\n",
        "                                                    lower=64,\n",
        "                                                    upper=1024,\n",
        "                                                    default_value=512)\n",
        "\n",
        "  neurons_second_layer_param = UniformIntegerHyperparameter(name='number_second_layer',\n",
        "                                                    lower=64,\n",
        "                                                    upper=1024,\n",
        "                                                    default_value=512)\n",
        "  neurons_third_layer_param = UniformIntegerHyperparameter(name='number_third_layer',\n",
        "                                                  lower=64,\n",
        "                                                  upper=1024,\n",
        "                                                  default_value=512)\n",
        "\n",
        "  neurons_fourth_layer_param = UniformIntegerHyperparameter(name='number_fourth_layer',\n",
        "                                                    lower=64,\n",
        "                                                    upper=1024,\n",
        "                                                    default_value=512)\n",
        "  cond_first_layer = InCondition(child=neurons_first_layer_param,\n",
        "                                parent=number_layer_param,\n",
        "                                values=[1,2,3,4])\n",
        "  cond_second_layer = InCondition(child=neurons_second_layer_param,\n",
        "                                  parent=number_layer_param,\n",
        "                                  values=[2,3,4])\n",
        "  cond_third_layer = InCondition(child=neurons_third_layer_param,\n",
        "                                  parent=number_layer_param,\n",
        "                                  values=[3,4])\n",
        "  cond_fourth_layer = InCondition(child=neurons_fourth_layer_param,\n",
        "                                  parent=number_layer_param,\n",
        "                                  values=[4])\n",
        "  \n",
        "  cond__dropout_first_layer = InCondition(child=dropout_first_layer,\n",
        "                                parent=number_layer_param,\n",
        "                                values=[1,2,3,4])\n",
        "  cond_dropout_second_layer = InCondition(child=dropout_second_layer,\n",
        "                                  parent=number_layer_param,\n",
        "                                  values=[2,3,4])\n",
        "  cond_dropout_third_layer = InCondition(child=dropout_third_layer,\n",
        "                                  parent=number_layer_param,\n",
        "                                  values=[3,4])\n",
        "  cond_dropout_fourth_layer = InCondition(child=dropout_fourth_layer,\n",
        "                                  parent=number_layer_param,\n",
        "                                  values=[4])\n",
        "  \n",
        "  if to_optimize == 'feature_extraction':\n",
        "    cut_layer = CategoricalHyperparameter(name='cut_layer',\n",
        "                                          choices=[6,10,14,18],\n",
        "                                          default_value=14)\n",
        "    cs.add_hyperparameters([cut_layer])\n",
        "  if to_optimize == 'fine_tuning':\n",
        "    freeze_to = UniformIntegerHyperparameter(name='freeze_to',\n",
        "                                             lower=7,\n",
        "                                             upper=19,\n",
        "                                             default_value=15)\n",
        "    cs.add_hyperparameters([freeze_to])\n",
        "  cs.add_hyperparameters([optimizier_param, \n",
        "                          lr_param,\n",
        "                          momentum_param,\n",
        "                          nestorov_param,\n",
        "                          number_layer_param,\n",
        "                          neurons_first_layer_param,\n",
        "                          neurons_second_layer_param,\n",
        "                          neurons_third_layer_param,\n",
        "                          neurons_fourth_layer_param,\n",
        "                          dropout_first_layer,\n",
        "                          dropout_second_layer,\n",
        "                          dropout_third_layer,\n",
        "                          dropout_fourth_layer\n",
        "                          ])\n",
        "  cs.add_conditions([cond_momentum,\n",
        "                    cond_nestorov,\n",
        "                    cond_first_layer,\n",
        "                    cond_second_layer,\n",
        "                    cond_third_layer,\n",
        "                    cond_fourth_layer,\n",
        "                    cond__dropout_first_layer,\n",
        "                    cond_dropout_second_layer,\n",
        "                    cond_dropout_third_layer,\n",
        "                    cond_dropout_fourth_layer\n",
        "                    ])\n",
        "\n",
        "  return cs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTVlUukw3Fcl",
        "colab_type": "text"
      },
      "source": [
        "## Scenario"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-YMAPtn3HM0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b5eb7e4e-5f5f-4a51-965c-215f68d12a75"
      },
      "source": [
        "to_optimize = 'feature_extraction'\n",
        "#to_optimize = 'fine_tuning'\n",
        "\n",
        "cs = create_configuration_space(to_optimize)\n",
        "scenario = Scenario({\"run_obj\": \"quality\",  \n",
        "                     \"runcount-limit\": 20, # 15 + 5 configurazioni iniziali   \n",
        "                     \"cs\": cs,               \n",
        "                     \"deterministic\": \"true\"\n",
        "                     })\n",
        "# All statistics collected during configuration run. Written to output-directory to be restored\n",
        "stat = Stats(scenario=scenario)\n",
        "\n",
        "\n",
        "traj_logger  = TrajLogger(output_dir=PATH_OPTIMIZATION + 'traj_logger/',\n",
        "                          stats=stat)\n",
        "\n",
        "run_history = RunHistory()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:smac.utils.io.cmd_reader.CMDReader:Output to smac3-output_2020-06-15_00:15:54_664342\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_91uOB4kAOqz",
        "colab_type": "text"
      },
      "source": [
        "## Optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfqB26ESAQP5",
        "colab_type": "code",
        "outputId": "a7230bb7-6384-4792-fee6-028e84dc0940",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "rng = np.random.RandomState(seed=42)\n",
        "\n",
        "df_train = pd.read_csv(PATH_OPTIMIZATION + 'train_k_cross.csv')\n",
        "\n",
        "smac = SMAC4HPO(scenario=scenario,\n",
        "                rng=rng,\n",
        "                runhistory=run_history,\n",
        "                tae_runner=cross_validation_optimization,\n",
        "                initial_design=RandomConfigurations, # LHDesign\n",
        "                initial_design_kwargs={'n_configs_x_params': 5},\n",
        "                acquisition_function=EI\n",
        "                )\n",
        "\n",
        "incumbent = smac.optimize()\n",
        "\n",
        "# save optim value and history confinguration tried\n",
        "if to_optimize == 'feature_extraction':\n",
        "  with open(PATH_OPTIMIZATION + 'incubent_feature_extraction.pkl', 'wb') as f:  \n",
        "      pickle.dump(incumbent, f)\n",
        "  with open(PATH_OPTIMIZATION + 'history_feature_extraction.pkl', 'wb') as f:  \n",
        "      pickle.dump(run_history, f)\n",
        "if to_optimize == 'fine_tuning':\n",
        "  with open(PATH_OPTIMIZATION + 'incumbent_fine_tuning.pkl', 'wb') as f:  \n",
        "      pickle.dump(incumbent, f)\n",
        "  with open(PATH_OPTIMIZATION + 'history_fine_tuning.pkl', 'wb') as f:  \n",
        "      pickle.dump(run_history, f)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:smac.facade.smac_hpo_facade.SMAC4HPO:Optimizing a deterministic scenario for quality without a tuner timeout - will make SMAC deterministic and only evaluate one configuration per iteration!\n",
            "ERROR:smac.utils.io.output_writer.OutputWriter:Could not write pcs file to disk. ConfigSpace not compatible with (new) pcs format.\n",
            "INFO:smac.initial_design.random_configuration_design.RandomConfigurations:Running initial design for 2 configurations\n",
            "INFO:smac.facade.smac_hpo_facade.SMAC4HPO:<class 'smac.facade.smac_hpo_facade.SMAC4HPO'>\n",
            "INFO:smac.optimizer.smbo.SMBO:Running initial design\n",
            "INFO:smac.intensification.intensification.Intensifier:First run, no incumbent provided; challenger is assumed to be the incumbent\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 1 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 3204 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 801 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 4006 validated image filenames belonging to 20 classes.\n",
            "feature extraction\n",
            "porco ziooooooooooo\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "51/51 [==============================] - 8s 160ms/step - loss: 13.4007 - categorical_accuracy: 0.1239 - val_loss: 2.9951 - val_categorical_accuracy: 0.1261\n",
            "Epoch 2/50\n",
            "51/51 [==============================] - 6s 113ms/step - loss: 3.0892 - categorical_accuracy: 0.1439 - val_loss: 2.9944 - val_categorical_accuracy: 0.1261\n",
            "Epoch 3/50\n",
            "51/51 [==============================] - 6s 119ms/step - loss: 3.0409 - categorical_accuracy: 0.1442 - val_loss: 2.9937 - val_categorical_accuracy: 0.1261\n",
            "Epoch 4/50\n",
            "51/51 [==============================] - 6s 118ms/step - loss: 3.0255 - categorical_accuracy: 0.1436 - val_loss: 2.9930 - val_categorical_accuracy: 0.1261\n",
            "Epoch 5/50\n",
            "51/51 [==============================] - 6s 119ms/step - loss: 3.0186 - categorical_accuracy: 0.1458 - val_loss: 2.9924 - val_categorical_accuracy: 0.1261\n",
            "Epoch 6/50\n",
            "51/51 [==============================] - 6s 119ms/step - loss: 3.0177 - categorical_accuracy: 0.1454 - val_loss: 2.9917 - val_categorical_accuracy: 0.1261\n",
            "Epoch 7/50\n",
            "51/51 [==============================] - 6s 119ms/step - loss: 3.0089 - categorical_accuracy: 0.1454 - val_loss: 2.9910 - val_categorical_accuracy: 0.1261\n",
            "Epoch 8/50\n",
            "51/51 [==============================] - 6s 118ms/step - loss: 3.0149 - categorical_accuracy: 0.1451 - val_loss: 2.9903 - val_categorical_accuracy: 0.1261\n",
            "Epoch 9/50\n",
            "51/51 [==============================] - 6s 119ms/step - loss: 3.0044 - categorical_accuracy: 0.1445 - val_loss: 2.9896 - val_categorical_accuracy: 0.1261\n",
            "Epoch 10/50\n",
            "51/51 [==============================] - 6s 118ms/step - loss: 3.0015 - categorical_accuracy: 0.1448 - val_loss: 2.9890 - val_categorical_accuracy: 0.1261\n",
            "Epoch 11/50\n",
            "51/51 [==============================] - 6s 119ms/step - loss: 2.9943 - categorical_accuracy: 0.1436 - val_loss: 2.9883 - val_categorical_accuracy: 0.1261\n",
            "Epoch 12/50\n",
            "51/51 [==============================] - 6s 119ms/step - loss: 3.0004 - categorical_accuracy: 0.1442 - val_loss: 2.9876 - val_categorical_accuracy: 0.1261\n",
            "Epoch 13/50\n",
            "51/51 [==============================] - 6s 118ms/step - loss: 2.9850 - categorical_accuracy: 0.1448 - val_loss: 2.9869 - val_categorical_accuracy: 0.1261\n",
            "Epoch 14/50\n",
            "51/51 [==============================] - 6s 119ms/step - loss: 2.9935 - categorical_accuracy: 0.1445 - val_loss: 2.9863 - val_categorical_accuracy: 0.1261\n",
            "Epoch 15/50\n",
            "51/51 [==============================] - 6s 122ms/step - loss: 2.9973 - categorical_accuracy: 0.1442 - val_loss: 2.9856 - val_categorical_accuracy: 0.1261\n",
            "Epoch 16/50\n",
            "51/51 [==============================] - 6s 121ms/step - loss: 2.9878 - categorical_accuracy: 0.1448 - val_loss: 2.9850 - val_categorical_accuracy: 0.1261\n",
            "Epoch 17/50\n",
            "51/51 [==============================] - 6s 118ms/step - loss: 2.9961 - categorical_accuracy: 0.1445 - val_loss: 2.9843 - val_categorical_accuracy: 0.1261\n",
            "Epoch 18/50\n",
            "51/51 [==============================] - 6s 118ms/step - loss: 2.9854 - categorical_accuracy: 0.1448 - val_loss: 2.9837 - val_categorical_accuracy: 0.1261\n",
            "Epoch 19/50\n",
            "51/51 [==============================] - 6s 119ms/step - loss: 2.9837 - categorical_accuracy: 0.1445 - val_loss: 2.9831 - val_categorical_accuracy: 0.1261\n",
            "Epoch 20/50\n",
            "51/51 [==============================] - 6s 119ms/step - loss: 2.9817 - categorical_accuracy: 0.1445 - val_loss: 2.9824 - val_categorical_accuracy: 0.1261\n",
            "Epoch 21/50\n",
            "51/51 [==============================] - 6s 117ms/step - loss: 2.9824 - categorical_accuracy: 0.1448 - val_loss: 2.9818 - val_categorical_accuracy: 0.1261\n",
            "Epoch 22/50\n",
            "51/51 [==============================] - 6s 118ms/step - loss: 2.9863 - categorical_accuracy: 0.1445 - val_loss: 2.9812 - val_categorical_accuracy: 0.1261\n",
            "Epoch 23/50\n",
            "51/51 [==============================] - 6s 118ms/step - loss: 2.9822 - categorical_accuracy: 0.1445 - val_loss: 2.9806 - val_categorical_accuracy: 0.1261\n",
            "Epoch 24/50\n",
            "51/51 [==============================] - 6s 124ms/step - loss: 2.9800 - categorical_accuracy: 0.1442 - val_loss: 2.9799 - val_categorical_accuracy: 0.1261\n",
            "Epoch 25/50\n",
            "51/51 [==============================] - 6s 123ms/step - loss: 2.9800 - categorical_accuracy: 0.1445 - val_loss: 2.9793 - val_categorical_accuracy: 0.1261\n",
            "Epoch 26/50\n",
            "51/51 [==============================] - 6s 120ms/step - loss: 2.9779 - categorical_accuracy: 0.1448 - val_loss: 2.9786 - val_categorical_accuracy: 0.1261\n",
            "Epoch 27/50\n",
            "51/51 [==============================] - 6s 117ms/step - loss: 2.9785 - categorical_accuracy: 0.1439 - val_loss: 2.9780 - val_categorical_accuracy: 0.1261\n",
            "Epoch 28/50\n",
            "51/51 [==============================] - 6s 120ms/step - loss: 2.9824 - categorical_accuracy: 0.1445 - val_loss: 2.9774 - val_categorical_accuracy: 0.1261\n",
            "Epoch 29/50\n",
            "51/51 [==============================] - 6s 119ms/step - loss: 2.9776 - categorical_accuracy: 0.1448 - val_loss: 2.9767 - val_categorical_accuracy: 0.1261\n",
            "Epoch 30/50\n",
            "51/51 [==============================] - 6s 120ms/step - loss: 2.9865 - categorical_accuracy: 0.1442 - val_loss: 2.9761 - val_categorical_accuracy: 0.1261\n",
            "Epoch 31/50\n",
            "51/51 [==============================] - 6s 118ms/step - loss: 2.9741 - categorical_accuracy: 0.1442 - val_loss: 2.9755 - val_categorical_accuracy: 0.1261\n",
            "Epoch 32/50\n",
            "51/51 [==============================] - 6s 119ms/step - loss: 2.9729 - categorical_accuracy: 0.1442 - val_loss: 2.9749 - val_categorical_accuracy: 0.1261\n",
            "Epoch 33/50\n",
            "51/51 [==============================] - 6s 117ms/step - loss: 2.9765 - categorical_accuracy: 0.1442 - val_loss: 2.9743 - val_categorical_accuracy: 0.1261\n",
            "Epoch 34/50\n",
            "51/51 [==============================] - 6s 117ms/step - loss: 2.9745 - categorical_accuracy: 0.1442 - val_loss: 2.9737 - val_categorical_accuracy: 0.1261\n",
            "Epoch 35/50\n",
            "51/51 [==============================] - 6s 119ms/step - loss: 2.9807 - categorical_accuracy: 0.1445 - val_loss: 2.9731 - val_categorical_accuracy: 0.1261\n",
            "Epoch 36/50\n",
            "51/51 [==============================] - 6s 118ms/step - loss: 2.9715 - categorical_accuracy: 0.1442 - val_loss: 2.9725 - val_categorical_accuracy: 0.1261\n",
            "Epoch 37/50\n",
            "51/51 [==============================] - 6s 117ms/step - loss: 2.9799 - categorical_accuracy: 0.1448 - val_loss: 2.9719 - val_categorical_accuracy: 0.1261\n",
            "Epoch 38/50\n",
            "51/51 [==============================] - 6s 117ms/step - loss: 2.9728 - categorical_accuracy: 0.1445 - val_loss: 2.9713 - val_categorical_accuracy: 0.1261\n",
            "Epoch 39/50\n",
            "51/51 [==============================] - 6s 118ms/step - loss: 2.9687 - categorical_accuracy: 0.1448 - val_loss: 2.9707 - val_categorical_accuracy: 0.1261\n",
            "Epoch 40/50\n",
            "51/51 [==============================] - 6s 127ms/step - loss: 2.9732 - categorical_accuracy: 0.1442 - val_loss: 2.9701 - val_categorical_accuracy: 0.1261\n",
            "Epoch 41/50\n",
            "51/51 [==============================] - 7s 138ms/step - loss: 2.9734 - categorical_accuracy: 0.1445 - val_loss: 2.9696 - val_categorical_accuracy: 0.1261\n",
            "Epoch 42/50\n",
            "51/51 [==============================] - 6s 120ms/step - loss: 2.9693 - categorical_accuracy: 0.1445 - val_loss: 2.9690 - val_categorical_accuracy: 0.1261\n",
            "Epoch 43/50\n",
            "51/51 [==============================] - 6s 117ms/step - loss: 2.9658 - categorical_accuracy: 0.1445 - val_loss: 2.9685 - val_categorical_accuracy: 0.1261\n",
            "Epoch 44/50\n",
            "51/51 [==============================] - 6s 117ms/step - loss: 2.9710 - categorical_accuracy: 0.1448 - val_loss: 2.9679 - val_categorical_accuracy: 0.1261\n",
            "Epoch 45/50\n",
            "51/51 [==============================] - 6s 117ms/step - loss: 2.9649 - categorical_accuracy: 0.1448 - val_loss: 2.9673 - val_categorical_accuracy: 0.1261\n",
            "Epoch 46/50\n",
            "51/51 [==============================] - 6s 117ms/step - loss: 2.9695 - categorical_accuracy: 0.1445 - val_loss: 2.9668 - val_categorical_accuracy: 0.1261\n",
            "Epoch 47/50\n",
            "51/51 [==============================] - 6s 116ms/step - loss: 2.9644 - categorical_accuracy: 0.1445 - val_loss: 2.9662 - val_categorical_accuracy: 0.1261\n",
            "Epoch 48/50\n",
            "51/51 [==============================] - 6s 116ms/step - loss: 2.9639 - categorical_accuracy: 0.1445 - val_loss: 2.9657 - val_categorical_accuracy: 0.1261\n",
            "Epoch 49/50\n",
            "51/51 [==============================] - 6s 116ms/step - loss: 2.9637 - categorical_accuracy: 0.1445 - val_loss: 2.9651 - val_categorical_accuracy: 0.1261\n",
            "Epoch 50/50\n",
            "51/51 [==============================] - 6s 117ms/step - loss: 2.9644 - categorical_accuracy: 0.1445 - val_loss: 2.9646 - val_categorical_accuracy: 0.1261\n",
            "Predict on test\n",
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 2 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 3204 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 802 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 4005 validated image filenames belonging to 20 classes.\n",
            "feature extraction\n",
            "porco ziooooooooooo\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "51/51 [==============================] - 7s 136ms/step - loss: 16.7740 - categorical_accuracy: 0.1380 - val_loss: 2.9951 - val_categorical_accuracy: 0.1621\n",
            "Epoch 2/50\n",
            "51/51 [==============================] - 6s 109ms/step - loss: 3.0510 - categorical_accuracy: 0.1532 - val_loss: 2.9945 - val_categorical_accuracy: 0.1621\n",
            "Epoch 3/50\n",
            "51/51 [==============================] - 6s 120ms/step - loss: 3.0365 - categorical_accuracy: 0.1520 - val_loss: 2.9938 - val_categorical_accuracy: 0.1621\n",
            "Epoch 4/50\n",
            "51/51 [==============================] - 6s 118ms/step - loss: 3.0213 - categorical_accuracy: 0.1520 - val_loss: 2.9932 - val_categorical_accuracy: 0.1621\n",
            "Epoch 5/50\n",
            "51/51 [==============================] - 6s 118ms/step - loss: 2.9951 - categorical_accuracy: 0.1532 - val_loss: 2.9926 - val_categorical_accuracy: 0.1621\n",
            "Epoch 6/50\n",
            "51/51 [==============================] - 6s 118ms/step - loss: 3.0144 - categorical_accuracy: 0.1529 - val_loss: 2.9921 - val_categorical_accuracy: 0.1621\n",
            "Epoch 7/50\n",
            "51/51 [==============================] - 6s 117ms/step - loss: 3.0126 - categorical_accuracy: 0.1529 - val_loss: 2.9915 - val_categorical_accuracy: 0.1621\n",
            "Epoch 8/50\n",
            "51/51 [==============================] - 6s 116ms/step - loss: 2.9934 - categorical_accuracy: 0.1526 - val_loss: 2.9909 - val_categorical_accuracy: 0.1621\n",
            "Epoch 9/50\n",
            "51/51 [==============================] - 6s 119ms/step - loss: 2.9944 - categorical_accuracy: 0.1532 - val_loss: 2.9903 - val_categorical_accuracy: 0.1621\n",
            "Epoch 10/50\n",
            "51/51 [==============================] - 6s 117ms/step - loss: 2.9973 - categorical_accuracy: 0.1536 - val_loss: 2.9897 - val_categorical_accuracy: 0.1621\n",
            "Epoch 11/50\n",
            "51/51 [==============================] - 6s 117ms/step - loss: 3.0038 - categorical_accuracy: 0.1532 - val_loss: 2.9891 - val_categorical_accuracy: 0.1621\n",
            "Epoch 12/50\n",
            "51/51 [==============================] - 6s 116ms/step - loss: 2.9883 - categorical_accuracy: 0.1529 - val_loss: 2.9886 - val_categorical_accuracy: 0.1621\n",
            "Epoch 13/50\n",
            "51/51 [==============================] - 6s 117ms/step - loss: 2.9904 - categorical_accuracy: 0.1526 - val_loss: 2.9880 - val_categorical_accuracy: 0.1621\n",
            "Epoch 14/50\n",
            "51/51 [==============================] - 6s 117ms/step - loss: 3.0010 - categorical_accuracy: 0.1529 - val_loss: 2.9874 - val_categorical_accuracy: 0.1621\n",
            "Epoch 15/50\n",
            "51/51 [==============================] - 6s 117ms/step - loss: 2.9900 - categorical_accuracy: 0.1529 - val_loss: 2.9869 - val_categorical_accuracy: 0.1621\n",
            "Epoch 16/50\n",
            "51/51 [==============================] - 6s 120ms/step - loss: 2.9904 - categorical_accuracy: 0.1526 - val_loss: 2.9863 - val_categorical_accuracy: 0.1621\n",
            "Epoch 17/50\n",
            "51/51 [==============================] - 6s 119ms/step - loss: 2.9922 - categorical_accuracy: 0.1536 - val_loss: 2.9858 - val_categorical_accuracy: 0.1621\n",
            "Epoch 18/50\n",
            "51/51 [==============================] - 6s 118ms/step - loss: 2.9838 - categorical_accuracy: 0.1529 - val_loss: 2.9852 - val_categorical_accuracy: 0.1621\n",
            "Epoch 19/50\n",
            "51/51 [==============================] - 6s 117ms/step - loss: 2.9855 - categorical_accuracy: 0.1536 - val_loss: 2.9846 - val_categorical_accuracy: 0.1621\n",
            "Epoch 20/50\n",
            "51/51 [==============================] - 6s 117ms/step - loss: 2.9807 - categorical_accuracy: 0.1532 - val_loss: 2.9840 - val_categorical_accuracy: 0.1621\n",
            "Epoch 21/50\n",
            "51/51 [==============================] - 6s 117ms/step - loss: 2.9856 - categorical_accuracy: 0.1532 - val_loss: 2.9835 - val_categorical_accuracy: 0.1621\n",
            "Epoch 22/50\n",
            "51/51 [==============================] - 6s 116ms/step - loss: 2.9793 - categorical_accuracy: 0.1532 - val_loss: 2.9829 - val_categorical_accuracy: 0.1621\n",
            "Epoch 23/50\n",
            "51/51 [==============================] - 6s 127ms/step - loss: 2.9835 - categorical_accuracy: 0.1526 - val_loss: 2.9824 - val_categorical_accuracy: 0.1621\n",
            "Epoch 24/50\n",
            "51/51 [==============================] - 6s 117ms/step - loss: 2.9774 - categorical_accuracy: 0.1532 - val_loss: 2.9818 - val_categorical_accuracy: 0.1621\n",
            "Epoch 25/50\n",
            "51/51 [==============================] - 6s 116ms/step - loss: 2.9769 - categorical_accuracy: 0.1532 - val_loss: 2.9813 - val_categorical_accuracy: 0.1621\n",
            "Epoch 26/50\n",
            "51/51 [==============================] - 6s 118ms/step - loss: 2.9857 - categorical_accuracy: 0.1532 - val_loss: 2.9808 - val_categorical_accuracy: 0.1621\n",
            "Epoch 27/50\n",
            "51/51 [==============================] - 6s 116ms/step - loss: 2.9800 - categorical_accuracy: 0.1532 - val_loss: 2.9803 - val_categorical_accuracy: 0.1621\n",
            "Epoch 28/50\n",
            "51/51 [==============================] - 6s 118ms/step - loss: 2.9780 - categorical_accuracy: 0.1532 - val_loss: 2.9798 - val_categorical_accuracy: 0.1621\n",
            "Epoch 29/50\n",
            "51/51 [==============================] - 6s 120ms/step - loss: 2.9749 - categorical_accuracy: 0.1532 - val_loss: 2.9793 - val_categorical_accuracy: 0.1621\n",
            "Epoch 30/50\n",
            "51/51 [==============================] - 6s 120ms/step - loss: 2.9721 - categorical_accuracy: 0.1536 - val_loss: 2.9787 - val_categorical_accuracy: 0.1621\n",
            "Epoch 31/50\n",
            "51/51 [==============================] - 6s 117ms/step - loss: 2.9759 - categorical_accuracy: 0.1536 - val_loss: 2.9782 - val_categorical_accuracy: 0.1621\n",
            "Epoch 32/50\n",
            "51/51 [==============================] - 6s 117ms/step - loss: 2.9735 - categorical_accuracy: 0.1529 - val_loss: 2.9778 - val_categorical_accuracy: 0.1621\n",
            "Epoch 33/50\n",
            "51/51 [==============================] - 6s 117ms/step - loss: 2.9706 - categorical_accuracy: 0.1539 - val_loss: 2.9773 - val_categorical_accuracy: 0.1621\n",
            "Epoch 34/50\n",
            "51/51 [==============================] - 6s 116ms/step - loss: 2.9695 - categorical_accuracy: 0.1536 - val_loss: 2.9768 - val_categorical_accuracy: 0.1621\n",
            "Epoch 35/50\n",
            "51/51 [==============================] - 6s 117ms/step - loss: 2.9691 - categorical_accuracy: 0.1532 - val_loss: 2.9763 - val_categorical_accuracy: 0.1621\n",
            "Epoch 36/50\n",
            "51/51 [==============================] - 6s 119ms/step - loss: 2.9703 - categorical_accuracy: 0.1532 - val_loss: 2.9758 - val_categorical_accuracy: 0.1621\n",
            "Epoch 37/50\n",
            "51/51 [==============================] - 6s 117ms/step - loss: 2.9669 - categorical_accuracy: 0.1532 - val_loss: 2.9753 - val_categorical_accuracy: 0.1621\n",
            "Epoch 38/50\n",
            "51/51 [==============================] - 6s 117ms/step - loss: 2.9688 - categorical_accuracy: 0.1532 - val_loss: 2.9749 - val_categorical_accuracy: 0.1621\n",
            "Epoch 39/50\n",
            "51/51 [==============================] - 6s 118ms/step - loss: 2.9675 - categorical_accuracy: 0.1532 - val_loss: 2.9743 - val_categorical_accuracy: 0.1621\n",
            "Epoch 40/50\n",
            "51/51 [==============================] - 7s 135ms/step - loss: 2.9689 - categorical_accuracy: 0.1532 - val_loss: 2.9738 - val_categorical_accuracy: 0.1621\n",
            "Epoch 41/50\n",
            "51/51 [==============================] - 6s 125ms/step - loss: 2.9650 - categorical_accuracy: 0.1529 - val_loss: 2.9734 - val_categorical_accuracy: 0.1621\n",
            "Epoch 42/50\n",
            "51/51 [==============================] - 6s 121ms/step - loss: 2.9631 - categorical_accuracy: 0.1536 - val_loss: 2.9729 - val_categorical_accuracy: 0.1621\n",
            "Epoch 43/50\n",
            "51/51 [==============================] - 6s 119ms/step - loss: 2.9644 - categorical_accuracy: 0.1536 - val_loss: 2.9724 - val_categorical_accuracy: 0.1621\n",
            "Epoch 44/50\n",
            "51/51 [==============================] - 6s 117ms/step - loss: 2.9629 - categorical_accuracy: 0.1536 - val_loss: 2.9719 - val_categorical_accuracy: 0.1621\n",
            "Epoch 45/50\n",
            "51/51 [==============================] - 6s 116ms/step - loss: 2.9653 - categorical_accuracy: 0.1532 - val_loss: 2.9714 - val_categorical_accuracy: 0.1621\n",
            "Epoch 46/50\n",
            "51/51 [==============================] - 6s 116ms/step - loss: 2.9614 - categorical_accuracy: 0.1536 - val_loss: 2.9710 - val_categorical_accuracy: 0.1621\n",
            "Epoch 47/50\n",
            "51/51 [==============================] - 6s 117ms/step - loss: 2.9611 - categorical_accuracy: 0.1532 - val_loss: 2.9705 - val_categorical_accuracy: 0.1621\n",
            "Epoch 48/50\n",
            "51/51 [==============================] - 6s 116ms/step - loss: 2.9662 - categorical_accuracy: 0.1536 - val_loss: 2.9701 - val_categorical_accuracy: 0.1621\n",
            "Epoch 49/50\n",
            "51/51 [==============================] - 6s 117ms/step - loss: 2.9606 - categorical_accuracy: 0.1536 - val_loss: 2.9696 - val_categorical_accuracy: 0.1621\n",
            "Epoch 50/50\n",
            "51/51 [==============================] - 6s 116ms/step - loss: 2.9575 - categorical_accuracy: 0.1536 - val_loss: 2.9691 - val_categorical_accuracy: 0.1621\n",
            "Predict on test\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:smac.intensification.intensification.Intensifier:Updated estimated cost of incumbent on 1 runs: 2.9396\n",
            "INFO:smac.intensification.intensification.Intensifier:Updated estimated cost of incumbent on 1 runs: 2.9396\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 1 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 3204 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 801 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 4006 validated image filenames belonging to 20 classes.\n",
            "feature extraction\n",
            "porco ziooooooooooo\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "51/51 [==============================] - 8s 157ms/step - loss: 5.0086 - categorical_accuracy: 0.1458 - val_loss: 1.7368 - val_categorical_accuracy: 0.4956\n",
            "Epoch 2/50\n",
            "51/51 [==============================] - 6s 108ms/step - loss: 2.9030 - categorical_accuracy: 0.2787 - val_loss: 1.5743 - val_categorical_accuracy: 0.5880\n",
            "Epoch 3/50\n",
            "51/51 [==============================] - 6s 118ms/step - loss: 2.3476 - categorical_accuracy: 0.3530 - val_loss: 1.4005 - val_categorical_accuracy: 0.6067\n",
            "Epoch 4/50\n",
            "51/51 [==============================] - 6s 120ms/step - loss: 2.1213 - categorical_accuracy: 0.4114 - val_loss: 1.2940 - val_categorical_accuracy: 0.6355\n",
            "Epoch 5/50\n",
            "51/51 [==============================] - 6s 118ms/step - loss: 1.8600 - categorical_accuracy: 0.4522 - val_loss: 1.2149 - val_categorical_accuracy: 0.6579\n",
            "Epoch 6/50\n",
            "51/51 [==============================] - 6s 118ms/step - loss: 1.7834 - categorical_accuracy: 0.4866 - val_loss: 1.1691 - val_categorical_accuracy: 0.6742\n",
            "Epoch 7/50\n",
            "51/51 [==============================] - 6s 117ms/step - loss: 1.6710 - categorical_accuracy: 0.4956 - val_loss: 1.0766 - val_categorical_accuracy: 0.6904\n",
            "Epoch 8/50\n",
            "51/51 [==============================] - 6s 117ms/step - loss: 1.6332 - categorical_accuracy: 0.5159 - val_loss: 1.0554 - val_categorical_accuracy: 0.7079\n",
            "Epoch 9/50\n",
            "51/51 [==============================] - 6s 116ms/step - loss: 1.5478 - categorical_accuracy: 0.5424 - val_loss: 1.0257 - val_categorical_accuracy: 0.7129\n",
            "Epoch 10/50\n",
            "51/51 [==============================] - 6s 116ms/step - loss: 1.4933 - categorical_accuracy: 0.5527 - val_loss: 0.9936 - val_categorical_accuracy: 0.7291\n",
            "Epoch 11/50\n",
            "51/51 [==============================] - 6s 117ms/step - loss: 1.4067 - categorical_accuracy: 0.5774 - val_loss: 1.0111 - val_categorical_accuracy: 0.7391\n",
            "Epoch 12/50\n",
            "51/51 [==============================] - 6s 117ms/step - loss: 1.3780 - categorical_accuracy: 0.5880 - val_loss: 0.9976 - val_categorical_accuracy: 0.7453\n",
            "Epoch 13/50\n",
            "51/51 [==============================] - 6s 117ms/step - loss: 1.3286 - categorical_accuracy: 0.5996 - val_loss: 0.9778 - val_categorical_accuracy: 0.7528\n",
            "Epoch 14/50\n",
            "51/51 [==============================] - 6s 118ms/step - loss: 1.3189 - categorical_accuracy: 0.5933 - val_loss: 0.9542 - val_categorical_accuracy: 0.7528\n",
            "Epoch 15/50\n",
            "51/51 [==============================] - 6s 118ms/step - loss: 1.2650 - categorical_accuracy: 0.6133 - val_loss: 0.9566 - val_categorical_accuracy: 0.7591\n",
            "Epoch 16/50\n",
            "51/51 [==============================] - 6s 117ms/step - loss: 1.1861 - categorical_accuracy: 0.6283 - val_loss: 0.9652 - val_categorical_accuracy: 0.7615\n",
            "Epoch 17/50\n",
            "51/51 [==============================] - 6s 120ms/step - loss: 1.1777 - categorical_accuracy: 0.6305 - val_loss: 0.9432 - val_categorical_accuracy: 0.7690\n",
            "Epoch 18/50\n",
            "51/51 [==============================] - 6s 118ms/step - loss: 1.1485 - categorical_accuracy: 0.6545 - val_loss: 0.9430 - val_categorical_accuracy: 0.7828\n",
            "Epoch 19/50\n",
            "51/51 [==============================] - 6s 116ms/step - loss: 1.0998 - categorical_accuracy: 0.6489 - val_loss: 0.9504 - val_categorical_accuracy: 0.7740\n",
            "Epoch 20/50\n",
            "51/51 [==============================] - 6s 117ms/step - loss: 1.1245 - categorical_accuracy: 0.6586 - val_loss: 0.9542 - val_categorical_accuracy: 0.7828\n",
            "Epoch 21/50\n",
            "51/51 [==============================] - 6s 123ms/step - loss: 1.0625 - categorical_accuracy: 0.6679 - val_loss: 0.9275 - val_categorical_accuracy: 0.7878\n",
            "Epoch 22/50\n",
            "51/51 [==============================] - 6s 119ms/step - loss: 1.0734 - categorical_accuracy: 0.6676 - val_loss: 0.9197 - val_categorical_accuracy: 0.7903\n",
            "Epoch 23/50\n",
            "51/51 [==============================] - 6s 116ms/step - loss: 1.0138 - categorical_accuracy: 0.6732 - val_loss: 0.9155 - val_categorical_accuracy: 0.7928\n",
            "Epoch 24/50\n",
            "51/51 [==============================] - 6s 116ms/step - loss: 0.9952 - categorical_accuracy: 0.6904 - val_loss: 0.8966 - val_categorical_accuracy: 0.7903\n",
            "Epoch 25/50\n",
            "51/51 [==============================] - 6s 116ms/step - loss: 1.0228 - categorical_accuracy: 0.6717 - val_loss: 0.8768 - val_categorical_accuracy: 0.7903\n",
            "Epoch 26/50\n",
            "51/51 [==============================] - 6s 117ms/step - loss: 0.9771 - categorical_accuracy: 0.6932 - val_loss: 0.8806 - val_categorical_accuracy: 0.7953\n",
            "Epoch 27/50\n",
            "51/51 [==============================] - 6s 117ms/step - loss: 0.9095 - categorical_accuracy: 0.7035 - val_loss: 0.8795 - val_categorical_accuracy: 0.7990\n",
            "Epoch 28/50\n",
            "51/51 [==============================] - 6s 117ms/step - loss: 0.9602 - categorical_accuracy: 0.6966 - val_loss: 0.8876 - val_categorical_accuracy: 0.7965\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 9.329351596534253e-05.\n",
            "Epoch 29/50\n",
            "51/51 [==============================] - 6s 118ms/step - loss: 0.9208 - categorical_accuracy: 0.7007 - val_loss: 0.8917 - val_categorical_accuracy: 0.7940\n",
            "Epoch 30/50\n",
            "51/51 [==============================] - 6s 118ms/step - loss: 0.9368 - categorical_accuracy: 0.6966 - val_loss: 0.8897 - val_categorical_accuracy: 0.7940\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00030: early stopping\n",
            "Predict on test\n",
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 2 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 3204 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 802 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 4005 validated image filenames belonging to 20 classes.\n",
            "feature extraction\n",
            "porco ziooooooooooo\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "51/51 [==============================] - 7s 138ms/step - loss: 4.8119 - categorical_accuracy: 0.1345 - val_loss: 1.9991 - val_categorical_accuracy: 0.3903\n",
            "Epoch 2/50\n",
            "51/51 [==============================] - 6s 109ms/step - loss: 2.9422 - categorical_accuracy: 0.2500 - val_loss: 1.7182 - val_categorical_accuracy: 0.4850\n",
            "Epoch 3/50\n",
            "51/51 [==============================] - 6s 116ms/step - loss: 2.4362 - categorical_accuracy: 0.3265 - val_loss: 1.4516 - val_categorical_accuracy: 0.5362\n",
            "Epoch 4/50\n",
            "51/51 [==============================] - 6s 117ms/step - loss: 2.2074 - categorical_accuracy: 0.3839 - val_loss: 1.2826 - val_categorical_accuracy: 0.5673\n",
            "Epoch 5/50\n",
            "51/51 [==============================] - 6s 117ms/step - loss: 2.0226 - categorical_accuracy: 0.4248 - val_loss: 1.1123 - val_categorical_accuracy: 0.6072\n",
            "Epoch 6/50\n",
            "51/51 [==============================] - 6s 117ms/step - loss: 1.8381 - categorical_accuracy: 0.4675 - val_loss: 1.0122 - val_categorical_accuracy: 0.6322\n",
            "Epoch 7/50\n",
            "51/51 [==============================] - 6s 127ms/step - loss: 1.7665 - categorical_accuracy: 0.4928 - val_loss: 0.9655 - val_categorical_accuracy: 0.6397\n",
            "Epoch 8/50\n",
            "51/51 [==============================] - 7s 132ms/step - loss: 1.6817 - categorical_accuracy: 0.5019 - val_loss: 0.9120 - val_categorical_accuracy: 0.6546\n",
            "Epoch 9/50\n",
            "51/51 [==============================] - 6s 115ms/step - loss: 1.6298 - categorical_accuracy: 0.5212 - val_loss: 0.8524 - val_categorical_accuracy: 0.6621\n",
            "Epoch 10/50\n",
            "51/51 [==============================] - 6s 117ms/step - loss: 1.5302 - categorical_accuracy: 0.5534 - val_loss: 0.7717 - val_categorical_accuracy: 0.6820\n",
            "Epoch 11/50\n",
            "51/51 [==============================] - 6s 118ms/step - loss: 1.5287 - categorical_accuracy: 0.5506 - val_loss: 0.8035 - val_categorical_accuracy: 0.6808\n",
            "Epoch 12/50\n",
            "51/51 [==============================] - 6s 120ms/step - loss: 1.4345 - categorical_accuracy: 0.5699 - val_loss: 0.7436 - val_categorical_accuracy: 0.6958\n",
            "Epoch 13/50\n",
            "51/51 [==============================] - 6s 117ms/step - loss: 1.3982 - categorical_accuracy: 0.5755 - val_loss: 0.6975 - val_categorical_accuracy: 0.6995\n",
            "Epoch 14/50\n",
            "51/51 [==============================] - 6s 115ms/step - loss: 1.3640 - categorical_accuracy: 0.5787 - val_loss: 0.6559 - val_categorical_accuracy: 0.7095\n",
            "Epoch 15/50\n",
            "51/51 [==============================] - 6s 117ms/step - loss: 1.2954 - categorical_accuracy: 0.6092 - val_loss: 0.6289 - val_categorical_accuracy: 0.7157\n",
            "Epoch 16/50\n",
            "51/51 [==============================] - 6s 117ms/step - loss: 1.2472 - categorical_accuracy: 0.6127 - val_loss: 0.5898 - val_categorical_accuracy: 0.7195\n",
            "Epoch 17/50\n",
            "51/51 [==============================] - 6s 116ms/step - loss: 1.2365 - categorical_accuracy: 0.6189 - val_loss: 0.5431 - val_categorical_accuracy: 0.7332\n",
            "Epoch 18/50\n",
            "51/51 [==============================] - 6s 116ms/step - loss: 1.2103 - categorical_accuracy: 0.6283 - val_loss: 0.5741 - val_categorical_accuracy: 0.7357\n",
            "Epoch 19/50\n",
            "51/51 [==============================] - 6s 118ms/step - loss: 1.2212 - categorical_accuracy: 0.6330 - val_loss: 0.5454 - val_categorical_accuracy: 0.7444\n",
            "Epoch 20/50\n",
            "51/51 [==============================] - 6s 118ms/step - loss: 1.1135 - categorical_accuracy: 0.6464 - val_loss: 0.5114 - val_categorical_accuracy: 0.7431\n",
            "Epoch 21/50\n",
            "51/51 [==============================] - 6s 115ms/step - loss: 1.1214 - categorical_accuracy: 0.6361 - val_loss: 0.5061 - val_categorical_accuracy: 0.7344\n",
            "Epoch 22/50\n",
            "51/51 [==============================] - 6s 116ms/step - loss: 1.1242 - categorical_accuracy: 0.6570 - val_loss: 0.4754 - val_categorical_accuracy: 0.7481\n",
            "Epoch 23/50\n",
            "51/51 [==============================] - 6s 115ms/step - loss: 1.0443 - categorical_accuracy: 0.6704 - val_loss: 0.4628 - val_categorical_accuracy: 0.7469\n",
            "Epoch 24/50\n",
            "51/51 [==============================] - 6s 116ms/step - loss: 1.0576 - categorical_accuracy: 0.6745 - val_loss: 0.4587 - val_categorical_accuracy: 0.7506\n",
            "Epoch 25/50\n",
            "51/51 [==============================] - 6s 118ms/step - loss: 1.0242 - categorical_accuracy: 0.6732 - val_loss: 0.4454 - val_categorical_accuracy: 0.7531\n",
            "Epoch 26/50\n",
            "51/51 [==============================] - 6s 118ms/step - loss: 0.9956 - categorical_accuracy: 0.6745 - val_loss: 0.4310 - val_categorical_accuracy: 0.7556\n",
            "Epoch 27/50\n",
            "51/51 [==============================] - 6s 117ms/step - loss: 0.9741 - categorical_accuracy: 0.6916 - val_loss: 0.4187 - val_categorical_accuracy: 0.7544\n",
            "Epoch 28/50\n",
            "51/51 [==============================] - 6s 115ms/step - loss: 0.9643 - categorical_accuracy: 0.6957 - val_loss: 0.3930 - val_categorical_accuracy: 0.7544\n",
            "Epoch 29/50\n",
            "51/51 [==============================] - 6s 116ms/step - loss: 0.9505 - categorical_accuracy: 0.7054 - val_loss: 0.4100 - val_categorical_accuracy: 0.7531\n",
            "Epoch 30/50\n",
            "51/51 [==============================] - 6s 118ms/step - loss: 0.9526 - categorical_accuracy: 0.7013 - val_loss: 0.3823 - val_categorical_accuracy: 0.7606\n",
            "Epoch 31/50\n",
            "51/51 [==============================] - 6s 118ms/step - loss: 0.9437 - categorical_accuracy: 0.6957 - val_loss: 0.3666 - val_categorical_accuracy: 0.7569\n",
            "Epoch 32/50\n",
            "51/51 [==============================] - 6s 116ms/step - loss: 0.8939 - categorical_accuracy: 0.7013 - val_loss: 0.3502 - val_categorical_accuracy: 0.7594\n",
            "Epoch 33/50\n",
            "51/51 [==============================] - 6s 116ms/step - loss: 0.8948 - categorical_accuracy: 0.7200 - val_loss: 0.3539 - val_categorical_accuracy: 0.7643\n",
            "Epoch 34/50\n",
            "51/51 [==============================] - 6s 118ms/step - loss: 0.8769 - categorical_accuracy: 0.7191 - val_loss: 0.3539 - val_categorical_accuracy: 0.7631\n",
            "Epoch 35/50\n",
            "51/51 [==============================] - 6s 118ms/step - loss: 0.9038 - categorical_accuracy: 0.7135 - val_loss: 0.3682 - val_categorical_accuracy: 0.7606\n",
            "\n",
            "Epoch 00035: ReduceLROnPlateau reducing learning rate to 9.329351596534253e-05.\n",
            "Epoch 36/50\n",
            "51/51 [==============================] - 6s 118ms/step - loss: 0.8511 - categorical_accuracy: 0.7378 - val_loss: 0.3680 - val_categorical_accuracy: 0.7631\n",
            "Epoch 37/50\n",
            "51/51 [==============================] - 6s 118ms/step - loss: 0.8660 - categorical_accuracy: 0.7272 - val_loss: 0.3657 - val_categorical_accuracy: 0.7643\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00037: early stopping\n",
            "Predict on test\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:smac.intensification.intensification.Intensifier:Challenger (1.2671) is better than incumbent (2.9396) on 1 runs.\n",
            "INFO:smac.intensification.intensification.Intensifier:Changes in incumbent:\n",
            "INFO:smac.intensification.intensification.Intensifier:  cut_layer : 14 -> 18\n",
            "INFO:smac.intensification.intensification.Intensifier:  dropout_first_layer : 0.27018626697982884 -> 0.3479008468380841\n",
            "INFO:smac.intensification.intensification.Intensifier:  dropout_second_layer : 0.3099927883619568 -> 0.22536676367917013\n",
            "INFO:smac.intensification.intensification.Intensifier:  dropout_third_layer : None -> 0.3264191256300771\n",
            "INFO:smac.intensification.intensification.Intensifier:  learning_rate : 0.0007279980174341478 -> 0.0009329351308489859\n",
            "INFO:smac.intensification.intensification.Intensifier:  momentum : 0.16231298330980526 -> 0.6334631176114999\n",
            "INFO:smac.intensification.intensification.Intensifier:  nestorov : False -> True\n",
            "INFO:smac.intensification.intensification.Intensifier:  number_first_layer : 805 -> 103\n",
            "INFO:smac.intensification.intensification.Intensifier:  number_layer : 2 -> 3\n",
            "INFO:smac.intensification.intensification.Intensifier:  number_second_layer : 68 -> 875\n",
            "INFO:smac.intensification.intensification.Intensifier:  number_third_layer : None -> 302\n",
            "INFO:smac.intensification.intensification.Intensifier:Wallclock time limit for intensification reached (used: 1060.118645 sec, available: 0.000010 sec)\n",
            "INFO:smac.intensification.intensification.Intensifier:Updated estimated cost of incumbent on 1 runs: 1.2671\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 1 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 3204 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 801 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 4006 validated image filenames belonging to 20 classes.\n",
            "feature extraction\n",
            "porco ziooooooooooo\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "51/51 [==============================] - 8s 162ms/step - loss: 5.4513 - categorical_accuracy: 0.1411 - val_loss: 2.2538 - val_categorical_accuracy: 0.4345\n",
            "Epoch 2/50\n",
            "51/51 [==============================] - 6s 110ms/step - loss: 3.2372 - categorical_accuracy: 0.2890 - val_loss: 1.8039 - val_categorical_accuracy: 0.5356\n",
            "Epoch 3/50\n",
            "51/51 [==============================] - 6s 117ms/step - loss: 2.6235 - categorical_accuracy: 0.3642 - val_loss: 1.5865 - val_categorical_accuracy: 0.5830\n",
            "Epoch 4/50\n",
            "51/51 [==============================] - 6s 117ms/step - loss: 2.2188 - categorical_accuracy: 0.4254 - val_loss: 1.4227 - val_categorical_accuracy: 0.6042\n",
            "Epoch 5/50\n",
            "51/51 [==============================] - 6s 116ms/step - loss: 1.9845 - categorical_accuracy: 0.4547 - val_loss: 1.3532 - val_categorical_accuracy: 0.6317\n",
            "Epoch 6/50\n",
            "51/51 [==============================] - 6s 117ms/step - loss: 1.8159 - categorical_accuracy: 0.4994 - val_loss: 1.2691 - val_categorical_accuracy: 0.6529\n",
            "Epoch 7/50\n",
            "51/51 [==============================] - 6s 117ms/step - loss: 1.7211 - categorical_accuracy: 0.5259 - val_loss: 1.2036 - val_categorical_accuracy: 0.6679\n",
            "Epoch 8/50\n",
            "51/51 [==============================] - 6s 116ms/step - loss: 1.6530 - categorical_accuracy: 0.5384 - val_loss: 1.1632 - val_categorical_accuracy: 0.6692\n",
            "Epoch 9/50\n",
            "51/51 [==============================] - 6s 117ms/step - loss: 1.5359 - categorical_accuracy: 0.5574 - val_loss: 1.1324 - val_categorical_accuracy: 0.6929\n",
            "Epoch 10/50\n",
            "51/51 [==============================] - 6s 115ms/step - loss: 1.4843 - categorical_accuracy: 0.5671 - val_loss: 1.1184 - val_categorical_accuracy: 0.6966\n",
            "Epoch 11/50\n",
            "51/51 [==============================] - 6s 116ms/step - loss: 1.4115 - categorical_accuracy: 0.5765 - val_loss: 1.0566 - val_categorical_accuracy: 0.7091\n",
            "Epoch 12/50\n",
            "51/51 [==============================] - 6s 117ms/step - loss: 1.3693 - categorical_accuracy: 0.6074 - val_loss: 1.0590 - val_categorical_accuracy: 0.7066\n",
            "Epoch 13/50\n",
            "51/51 [==============================] - 6s 120ms/step - loss: 1.3307 - categorical_accuracy: 0.6099 - val_loss: 1.0289 - val_categorical_accuracy: 0.7216\n",
            "Epoch 14/50\n",
            "51/51 [==============================] - 6s 118ms/step - loss: 1.3057 - categorical_accuracy: 0.6199 - val_loss: 1.0076 - val_categorical_accuracy: 0.7216\n",
            "Epoch 15/50\n",
            "51/51 [==============================] - 6s 117ms/step - loss: 1.2540 - categorical_accuracy: 0.6317 - val_loss: 1.0181 - val_categorical_accuracy: 0.7203\n",
            "Epoch 16/50\n",
            "51/51 [==============================] - 6s 116ms/step - loss: 1.1788 - categorical_accuracy: 0.6461 - val_loss: 1.0086 - val_categorical_accuracy: 0.7241\n",
            "Epoch 17/50\n",
            "51/51 [==============================] - 6s 117ms/step - loss: 1.1481 - categorical_accuracy: 0.6458 - val_loss: 0.9875 - val_categorical_accuracy: 0.7328\n",
            "Epoch 18/50\n",
            "51/51 [==============================] - 6s 117ms/step - loss: 1.1824 - categorical_accuracy: 0.6501 - val_loss: 0.9364 - val_categorical_accuracy: 0.7366\n",
            "Epoch 19/50\n",
            "51/51 [==============================] - 6s 123ms/step - loss: 1.1012 - categorical_accuracy: 0.6726 - val_loss: 0.9668 - val_categorical_accuracy: 0.7403\n",
            "Epoch 20/50\n",
            "51/51 [==============================] - 7s 133ms/step - loss: 1.0813 - categorical_accuracy: 0.6720 - val_loss: 0.9500 - val_categorical_accuracy: 0.7478\n",
            "Epoch 21/50\n",
            "51/51 [==============================] - 6s 118ms/step - loss: 1.0759 - categorical_accuracy: 0.6888 - val_loss: 0.9350 - val_categorical_accuracy: 0.7516\n",
            "Epoch 22/50\n",
            "51/51 [==============================] - 6s 116ms/step - loss: 1.0329 - categorical_accuracy: 0.6826 - val_loss: 0.9079 - val_categorical_accuracy: 0.7491\n",
            "Epoch 23/50\n",
            "51/51 [==============================] - 6s 115ms/step - loss: 1.0152 - categorical_accuracy: 0.6938 - val_loss: 0.9025 - val_categorical_accuracy: 0.7566\n",
            "Epoch 24/50\n",
            "51/51 [==============================] - 6s 117ms/step - loss: 0.9744 - categorical_accuracy: 0.6957 - val_loss: 0.8969 - val_categorical_accuracy: 0.7516\n",
            "Epoch 25/50\n",
            "51/51 [==============================] - 6s 117ms/step - loss: 0.9665 - categorical_accuracy: 0.7110 - val_loss: 0.8823 - val_categorical_accuracy: 0.7615\n",
            "Epoch 26/50\n",
            "51/51 [==============================] - 6s 119ms/step - loss: 0.9999 - categorical_accuracy: 0.7047 - val_loss: 0.8731 - val_categorical_accuracy: 0.7665\n",
            "Epoch 27/50\n",
            "51/51 [==============================] - 6s 118ms/step - loss: 0.9441 - categorical_accuracy: 0.7076 - val_loss: 0.8719 - val_categorical_accuracy: 0.7690\n",
            "Epoch 28/50\n",
            "51/51 [==============================] - 6s 116ms/step - loss: 0.8971 - categorical_accuracy: 0.7216 - val_loss: 0.8368 - val_categorical_accuracy: 0.7715\n",
            "Epoch 29/50\n",
            "51/51 [==============================] - 6s 117ms/step - loss: 0.9266 - categorical_accuracy: 0.7169 - val_loss: 0.8558 - val_categorical_accuracy: 0.7765\n",
            "Epoch 30/50\n",
            "51/51 [==============================] - 6s 118ms/step - loss: 0.9347 - categorical_accuracy: 0.7175 - val_loss: 0.8672 - val_categorical_accuracy: 0.7665\n",
            "Epoch 31/50\n",
            "51/51 [==============================] - 6s 116ms/step - loss: 0.8879 - categorical_accuracy: 0.7328 - val_loss: 0.8462 - val_categorical_accuracy: 0.7678\n",
            "\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 6.459322175942362e-05.\n",
            "Epoch 32/50\n",
            "51/51 [==============================] - 6s 118ms/step - loss: 0.8606 - categorical_accuracy: 0.7266 - val_loss: 0.8466 - val_categorical_accuracy: 0.7715\n",
            "Epoch 33/50\n",
            "51/51 [==============================] - 6s 117ms/step - loss: 0.8915 - categorical_accuracy: 0.7434 - val_loss: 0.8446 - val_categorical_accuracy: 0.7728\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00033: early stopping\n",
            "Predict on test\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUM7BP6b6U_K",
        "colab_type": "text"
      },
      "source": [
        "## Plot loss for configurations tried and best seen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCQeT4Ff58RB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def losses_optimization(run_history)\n",
        "  losses = []\n",
        "  c = run_history.get_all_configs()\n",
        "  for conf in c:\n",
        "    temp = conf.get_dictionary()\n",
        "    losses.append(run_history.get_cost(conf))\n",
        "  return losses\n",
        "\n",
        "def plot_loss_configurations(run_history):\n",
        "  losses = losses_optimization(run_history)\n",
        "  plt.plot(losses)\n",
        "  plt.title('History losses for configurations tried')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xlabel('Number iteration')\n",
        "  plt.show()\n",
        "\n",
        "def plot_best_seen(run_history):\n",
        "  final = []\n",
        "  init = 100000000\n",
        "  losses = losses_optimization(run_history)\n",
        "  for i in losses:\n",
        "    if i < init:\n",
        "      final.append(i)\n",
        "      init = i\n",
        "    else:\n",
        "      final.append(init)\n",
        "  plt.plot(final, marker=\"o\")\n",
        "  plt.title('Best seen')\n",
        "  plt.xlabel('Number iteration')\n",
        "  plt.ylabel('Best seen')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEbfrZ_d6ZzM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if to_optimize == 'feature_extraction':\n",
        "  with open(PATH_OPTIMIZATION + 'history_feature_extraction.pkl', 'rb') as f:\n",
        "      run_history = pickle.load(f)\n",
        "if to_optimize == 'fine_tuning':\n",
        "  with open(PATH_OPTIMIZATION + 'history_fine_tuning.pkl', 'rb') as f:\n",
        "      run_history = pickle.load(f)\n",
        "\n",
        "plot_loss_configurations(run_history)\n",
        "plot_best_seen(run_history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-ZHDD5C4Ksp",
        "colab_type": "text"
      },
      "source": [
        "## Train model on incumbent on all train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IN__wfa4unN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model_all_train(to_optimize):\n",
        "  if to_optimize == 'feature_extraction':\n",
        "    with open(PATH_OPTIMIZATION + 'incubent_feature_extraction.pkl', 'rb') as f:\n",
        "        incumbent = pickle.load(f)\n",
        "    # create model with incumbent configuration\n",
        "    # train model on all train\n",
        "    # plot loss and accuracy on train and val\n",
        "    return model\n",
        "    \n",
        "  if to_optimize == 'fine_tuning':\n",
        "    with open(PATH_OPTIMIZATION + 'incumbent_fine_tuning.pkl', 'rb') as f:\n",
        "        incumbent = pickle.load(f)\n",
        "    # create model with incumbent configuration\n",
        "    # train model on all train\n",
        "    # plot loss and accuracy on train and val\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngqgVtqrIsD-",
        "colab_type": "text"
      },
      "source": [
        "## Performance of model on test set, after AutoML"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2GI2JyjIvBU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def performance_on_test(model, test_generator):\n",
        "  predictions = model.predict_generator(test_generator, \n",
        "                          steps=ceil(test_generator.n / test_generator.batch_size))\n",
        "  predict_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "  performance = classification_report(  y_true = test_generator.classes, \n",
        "                                        y_pred = predict_classes,\n",
        "                                        target_names=LABELS\n",
        "  )\n",
        "\n",
        "  return performance\n",
        "\n",
        "df_test = pd.read_csv(PATH_OPTIMIZATION + 'test.csv')\n",
        "\n",
        "# create test_gen from flow_dataframe\n",
        "model = train_model_all_train(to_optimize)\n",
        "test_generator = ImageDataGenerator(preprocessing_function=preprocess_input_vgg16)\n",
        "test_gen = test_generator.flow_from_dataframe(\n",
        "  dataframe = df_test,\n",
        "  x_col='absolute_path',\n",
        "  y_col='class',\n",
        "  shuffle=False,\n",
        "  class_mode='categorical',\n",
        "  target_size=IM_SIZE,\n",
        "  batch_size=BATCH_SIZE,\n",
        "  classes=LABELS,\n",
        ")\n",
        "result = performance_on_test(model, test_gen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h2eF2vMiYyG",
        "colab_type": "code",
        "outputId": "5e52a1ea-52a2-4d73-d471-5f47cd7020ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "print(result)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "   aeroplane       0.95      0.91      0.93       196\n",
            "     bicycle       0.90      0.72      0.80       175\n",
            "        bird       0.83      0.95      0.89       189\n",
            "        boat       0.91      0.75      0.82       102\n",
            "      bottle       0.77      0.89      0.83        81\n",
            "         bus       0.92      0.92      0.92       144\n",
            "         car       0.90      0.86      0.88       245\n",
            "         cat       0.80      0.86      0.83       391\n",
            "       chair       0.75      0.60      0.67       422\n",
            "         cow       0.60      0.73      0.66       118\n",
            " diningtable       0.65      0.69      0.67       186\n",
            "         dog       0.83      0.81      0.82       435\n",
            "       horse       0.79      0.75      0.77       185\n",
            "   motorbike       0.65      0.89      0.75       183\n",
            "      person       0.93      0.84      0.89      1613\n",
            " pottedplant       0.88      0.85      0.86       117\n",
            "       sheep       0.66      0.91      0.77       116\n",
            "        sofa       0.47      0.66      0.55       226\n",
            "       train       0.83      0.93      0.87       175\n",
            "   tvmonitor       0.85      0.93      0.89       133\n",
            "\n",
            "    accuracy                           0.82      5432\n",
            "   macro avg       0.79      0.82      0.80      5432\n",
            "weighted avg       0.83      0.82      0.82      5432\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}