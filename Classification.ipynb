{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "bwTcVNKCAoJt",
        "AHm9t-_NGYDa",
        "l7fUcTZApdLB",
        "pNm8VPigbjay",
        "fYnY8Dp84Utn"
      ],
      "authorship_tag": "ABX9TyPGLyPK05iBpG3Z1LObohIz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sbarbagnem/AdvancedProject/blob/master/Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGjYNEjPFiCE",
        "colab_type": "text"
      },
      "source": [
        "# Module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJr3vEODFZE1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import os\n",
        "import xml.etree.ElementTree as ET\n",
        "from math import ceil\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications import VGG16, ResNet50, MobileNet\n",
        "from keras.applications.vgg16 import preprocess_input as preprocess_input_vgg16\n",
        "from keras.applications.resnet50 import preprocess_input as preprocess_input_resnet50\n",
        "from keras.applications.mobilenet import preprocess_input as preprocess_input_mobilenet\n",
        "from keras.layers import Dense, Dropout, GlobalAveragePooling2D, Flatten\n",
        "from keras import Model\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eD9R4p_JGS8j",
        "colab_type": "text"
      },
      "source": [
        "# Costant"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jui_q9SGGXsi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "PATH_ANNOTATIONS = '/content/drive/My Drive/Laurea Magistrale/Adavanched Machine Learning/Progetto/Dataset/Train/Annotations/'\n",
        "PATH_MAIN = '/content/drive/My Drive/Laurea Magistrale/Adavanched Machine Learning/Progetto/Dataset/Train/Main/'\n",
        "PATH_IMAGES = '/content/drive/My Drive/Laurea Magistrale/Adavanched Machine Learning/Progetto/Dataset/Train/Images/'\n",
        "PATH_IMAGES_CROPPED_TRAIN = '/content/drive/My Drive/Laurea Magistrale/Adavanched Machine Learning/Progetto/Dataset/Train/Cropped_train/'\n",
        "PATH_IMAGES_CROPPED_VAL_TEST = '/content/drive/My Drive/Laurea Magistrale/Adavanched Machine Learning/Progetto/Dataset/Train/Cropped_val_test/'\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "IM_SIZE = (256, 256)\n",
        "\n",
        "LABELS = ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', \n",
        "          'car', 'cat', 'chair', 'cow', 'diningtable',\n",
        "          'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa',\n",
        "          'train', 'tvmonitor']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwTcVNKCAoJt",
        "colab_type": "text"
      },
      "source": [
        "# Directory for cropped images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shqDlbvsAsmi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creo 20 directory, una per ogni classe, per fare classificazione (flow_from_datframe) sulle immagini croppate\n",
        "def create_directories(path, labels):\n",
        "  for label in labels:\n",
        "    os.mkdir(os.path.join(path, label))\n",
        "\n",
        "create_folder_classes = False\n",
        "\n",
        "if create_folder_classes == True:\n",
        "  create_directories(PATH_IMAGES_CROPPED_TRAIN, LABELS)\n",
        "  create_directories(PATH_IMAGES_CROPPED_VAL_TEST, LABELS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awq2lzbxBXgj",
        "colab_type": "text"
      },
      "source": [
        "# Util function for mapping from label to #class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NN3ewyc_BenA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_dict_mapping(labels):\n",
        "  mapping = {}\n",
        "  for label,i in zip(labels, range(len(labels))):\n",
        "    mapping[label] = i\n",
        "  return mapping\n",
        "\n",
        "def from_label_to_number(mapping, label):\n",
        "  return mapping[label]\n",
        "\n",
        "def from_number_to_label(mapping, number):\n",
        "  for key, val in mapping.items(): \n",
        "    if val == number: \n",
        "      return key \n",
        "  return \"key doesn't exist\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EGlorv3K-NG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "c23ee80c-485f-432c-c688-fe9df73b1fd4"
      },
      "source": [
        "mapping = create_dict_mapping(LABELS)\n",
        "print(mapping)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'aeroplane': 0, 'bicycle': 1, 'bird': 2, 'boat': 3, 'bottle': 4, 'bus': 5, 'car': 6, 'cat': 7, 'chair': 8, 'cow': 9, 'diningtable': 10, 'dog': 11, 'horse': 12, 'motorbike': 13, 'person': 14, 'pottedplant': 15, 'sheep': 16, 'sofa': 17, 'train': 18, 'tvmonitor': 19}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHm9t-_NGYDa",
        "colab_type": "text"
      },
      "source": [
        "# Crop and save image for class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5P1zAHxAGdTJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def crop_image(img, x_min, y_min, x_max, y_max):\n",
        "  crop_img = img[y_min:y_max, x_min:x_max]\n",
        "  return crop_img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QkTnM8cPtA_",
        "colab_type": "code",
        "outputId": "e49e0040-c93b-4b11-e11e-e4e0c9612f85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "def list_images(path_images, path_txt, file_txt):\n",
        "  '''\n",
        "  creo liste immagini presenti in file_txt\n",
        "  '''\n",
        "  temp = []\n",
        "  f = open(os.path.join(path_txt, file_txt), \"r\")\n",
        "  for line in f.readlines():\n",
        "    temp.append(line.split('\\n')[0] + '.jpg')\n",
        "  list_images = [os.path.join(path_images, name) for name in temp]\n",
        "  print('Ho trovato ', len(list_images), 'per il file ', file_txt)\n",
        "  return list_images\n",
        "\n",
        "# creo liste immagini da train.txt e val.txt\n",
        "list_images_train = list_images(PATH_IMAGES, PATH_MAIN, 'train.txt')\n",
        "list_images_val = list_images(PATH_IMAGES, PATH_MAIN, 'val.txt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Laurea Magistrale/Adavanched Machine Learning/Progetto/Dataset/Train/Images/2008_000008.jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCnf_0tOQ687",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_and_crop(list_images, annotation_dir, path_to_save):\n",
        "  '''\n",
        "  leggo annotations dei file presenti nelle due liste di immagini \n",
        "  '''\n",
        "  for path in list_images:\n",
        "    image_name = path.split('/')[-1].split('.')[0] \n",
        "    print(image_name)\n",
        "    with open(os.path.join(annotation_dir, image_name + '.xml')) as f:\n",
        "      read_xml(f.read(), path, path_to_save)\n",
        "  return\n",
        "\n",
        "def read_xml(file_xml, path_image, path_to_save):\n",
        "  '''\n",
        "  leggo xml e per ogni box che trovo croppo e salvo\n",
        "  '''\n",
        "  img = cv2.imread(path_image)  \n",
        "  root = ET.XML(file_xml)\n",
        "  for _, child in enumerate(root):\n",
        "    if child.tag == 'object':\n",
        "      x_min = None\n",
        "      y_min = None\n",
        "      x_max = None\n",
        "      y_max = None\n",
        "      for subchild in child:\n",
        "        if subchild.tag == 'name':\n",
        "          name_object = subchild.text\n",
        "          #print(name_object)\n",
        "        if subchild.tag == 'bndbox':\n",
        "          for bndbox in subchild:\n",
        "            if bndbox.tag == 'xmin':\n",
        "              x_min = int(bndbox.text)\n",
        "              #print('x_min ', x_min)\n",
        "            if bndbox.tag == 'ymin':\n",
        "              y_min = int(bndbox.text)\n",
        "              #print('y_min ', y_min)\n",
        "            if bndbox.tag == 'xmax':\n",
        "              x_max = int(bndbox.text)\n",
        "              #print('x_max ', x_max)\n",
        "            if bndbox.tag == 'ymax':\n",
        "              y_max = int(bndbox.text)\n",
        "              #print('y_max ', y_max)\n",
        "        if(x_min!=None and y_min!=None and x_max!=None and y_max!=None):\n",
        "          image_cropped = crop_image(img, x_min, y_min, x_max, y_max)\n",
        "          x_min = None\n",
        "          y_min = None\n",
        "          x_max = None\n",
        "          y_max = None\n",
        "          #cv2_imshow(image_cropped)\n",
        "          save_image_cropped(name_object, image_cropped, path_to_save)\n",
        "  return \n",
        "\n",
        "def save_image_cropped(obj, img, path_to_save):\n",
        "  '''\n",
        "  salvo immagine croppata con numero progressivo in base \n",
        "  all'ultima presente nella cartella\n",
        "  '''\n",
        "  list_dir_classes = os.listdir(path_to_save)\n",
        "  for dir_class in list_dir_classes:\n",
        "    if dir_class == obj:\n",
        "      dir_temp = os.path.join(path_to_save,dir_class)\n",
        "      list_temp = os.listdir(dir_temp)\n",
        "      if list_temp == []:\n",
        "        cv2.imwrite(os.path.join(dir_temp, obj + '_1.jpg'), img)\n",
        "      else:\n",
        "        number_file = int(list_temp[-1].split('.')[0].split('_')[1]) + 1\n",
        "        cv2.imwrite(os.path.join(dir_temp, obj + '_' + str(number_file) + '.jpg'), img)\n",
        "  return\n",
        "\n",
        "read_and_crop(list_images=list_images_val, annotation_dir=PATH_ANNOTATIONS, path_to_save=PATH_IMAGES_CROPPED_VAL_TEST)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgzdNcHZGgch",
        "colab_type": "text"
      },
      "source": [
        "# Util function to create generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMtwpZ_kGfsx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_generator( batch_size, im_size, base_net, train_directory, val_directory, labels,\n",
        "                      validation_split, aug, augment_params):\n",
        "  \n",
        "  if base_net == 'vgg16':\n",
        "    preprocess_function = preprocess_input_vgg16\n",
        "  elif base_net == 'mobilenet':\n",
        "    preprocess_function = preprocess_input_mobilenet\n",
        "  elif base_net == 'resnet':\n",
        "    preprocess_function = preprocess_input_resnet50\n",
        "\n",
        "  if not(aug):\n",
        "    img_gen_train = ImageDataGenerator(rescale=1./255)\n",
        "  elif aug:\n",
        "    img_gen_train = ImageDataGenerator( rescale=1./255,\n",
        "                                        **augment_params)  \n",
        "\n",
        "\n",
        "  img_gen_val = ImageDataGenerator( rescale=1./255,\n",
        "                                    validation_split = validation_split\n",
        "                                  )\n",
        "    \n",
        "  train_gen = img_gen_train.flow_from_directory(\n",
        "      directory = train_directory,\n",
        "      shuffle=True,\n",
        "      class_mode='categorical',\n",
        "      target_size=im_size,\n",
        "      batch_size=batch_size,\n",
        "      classes=labels,\n",
        "      seed=42\n",
        "  )\n",
        "\n",
        "  val_gen = img_gen_val.flow_from_directory(\n",
        "      directory = val_directory,\n",
        "      shuffle=False,\n",
        "      class_mode='categorical',\n",
        "      target_size=im_size,\n",
        "      batch_size=batch_size,\n",
        "      subset='training',\n",
        "      classes=labels,\n",
        "      seed=42\n",
        "  )\n",
        "\n",
        "  test_gen = img_gen_val.flow_from_directory(\n",
        "      directory = val_directory,\n",
        "      shuffle=False,\n",
        "      class_mode='categorical',\n",
        "      target_size=im_size,\n",
        "      batch_size=batch_size,\n",
        "      subset='validation',\n",
        "      classes=labels,\n",
        "      seed=42\n",
        "  )\n",
        "\n",
        "  return train_gen, val_gen, test_gen"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDw9W32GaThb",
        "colab_type": "text"
      },
      "source": [
        "# Util function for frequency classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teCq1HPhaXTg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_frequency_from_generator(generator):\n",
        "  mapping = generator.class_indices\n",
        "  classes, count = np.unique(generator.labels, return_counts=True)\n",
        "  labels = [from_number_to_label(mapping, label) for label in classes]\n",
        "  freq = count\n",
        "  return labels, freq\n",
        "\n",
        "def normalize_frequency(freq):\n",
        "  min_freq = np.min(freq)\n",
        "  weigh = [min_freq/x for x in freq]\n",
        "  return weigh * freq, weigh\n",
        "\n",
        "def plot_label_frequency(labels, freq):\n",
        "  freq = freq / np.sum(freq)\n",
        "  l = list(range(1, len(labels)+1))\n",
        "  plt.barh(l, width=freq, height=0.5)\n",
        "  plt.yticks(l, labels, rotation='horizontal')\n",
        "  plt.show()\n",
        "\n",
        "def plot_stacked_bar_freq(labels, freq, freq_normalize):\n",
        "  N = len(labels)\n",
        "  ind = np.arange(N)    # the x locations for the groups\n",
        "  width = 0.35       # the width of the bars: can also be len(x) sequence\n",
        "\n",
        "  p1 = plt.bar(ind, freq, width)\n",
        "  p2 = plt.bar(ind, freq_normalize, width)\n",
        "\n",
        "  plt.ylabel('Frequency')\n",
        "  plt.legend((p1[0], p2[0]), ('Freq', 'Freq_normalize'))\n",
        "\n",
        "  plt.show() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXwxrNdCShlc",
        "colab_type": "text"
      },
      "source": [
        "# Util function to create model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etwOZM2GSjL_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(base_net, freeze_all, freeze_to, dense, dropout, im_size):\n",
        "  if base_net == 'vgg16':\n",
        "    base_model = VGG16(weights = 'imagenet', input_shape=im_size + (3,), include_top = False)\n",
        "  elif base_net == 'resnet':\n",
        "    base_model = ResNet50(weights = 'imagenet', input_shape=im_size + (3,), include_top = False)\n",
        "  elif base_net == 'mobilenet':\n",
        "    base_model = MobileNet(weights = 'imagenet', input_shape=im_size + (3,), include_top = False)\n",
        "\n",
        "  #x = base_model.output\n",
        "  x = base_model.layers[-1].output\n",
        "  x = Flatten()(x)\n",
        "  #x = GlobalAveragePooling2D()(x)\n",
        "  for layer in dense:\n",
        "    x = Dense(layer, activation='relu')(x)\n",
        "    #x = Dropout(dropout)(x)\n",
        "  predictions = Dense(20, activation = 'softmax')(x)\n",
        "  model = Model(input = base_model.input, output = predictions)\n",
        "\n",
        "  if freeze_all == True:\n",
        "    for layer in base_model.layers:\n",
        "      layer.trainable=False\n",
        "  else:\n",
        "    for layer in base_model.layers[:freeze_to]:\n",
        "      layer.trainable=False\n",
        "    for layer in model.layers[freeze_to:]:\n",
        "      layer.trainable = True\n",
        "      \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7fUcTZApdLB",
        "colab_type": "text"
      },
      "source": [
        "# Util function to plot performance epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXAGRUyipg-R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_performance(history):\n",
        "  plt.plot(history.history['categorical_accuracy'])\n",
        "  plt.plot(history.history['val_categorical_accuracy'])\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'val'], loc='upper left')\n",
        "  plt.show()\n",
        "  # summarize history for loss\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'val'], loc='upper left')\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22ReLsjTEeCZ",
        "colab_type": "text"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "se7bKuAJbulU",
        "colab_type": "text"
      },
      "source": [
        "## Set up generator and model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDNTdRNpGBpI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "d3aeb177-4851-4005-af6a-d46914d11049"
      },
      "source": [
        "base_net = 'vgg16'\n",
        "\n",
        "augment_params = dict(  rotation_range=30,\n",
        "                        width_shift_range=0.2,\n",
        "                        height_shift_range=0.2,\n",
        "                        horizontal_flip=True,\n",
        "                        #zoom_range=0.1\n",
        "                      )\n",
        "\n",
        "train_gen, val_gen, test_gen = create_generator(batch_size=BATCH_SIZE, \n",
        "                                                im_size=IM_SIZE, \n",
        "                                                base_net=base_net, \n",
        "                                                train_directory=PATH_IMAGES_CROPPED_TRAIN, \n",
        "                                                val_directory=PATH_IMAGES_CROPPED_VAL_TEST, \n",
        "                                                labels=LABELS,\n",
        "                                                validation_split=0.5,\n",
        "                                                aug=False,\n",
        "                                                augment_params=augment_params\n",
        "                                                )"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 15774 images belonging to 20 classes.\n",
            "Found 7899 images belonging to 20 classes.\n",
            "Found 7888 images belonging to 20 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oR4bO3SF_-EN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# show random image from generator\n",
        "from skimage import io\n",
        "\n",
        "def imshow(image_RGB):\n",
        "  io.imshow(image_RGB)\n",
        "  io.t\n",
        "  io.show()\n",
        "\n",
        "x,y = train_gen.next()\n",
        "for i in range(0,63):\n",
        "    image = x[i]\n",
        "    plt.figure()\n",
        "    plt.imshow(image)\n",
        "    plt.title(from_number_to_label(mapping,np.where(y[i] == 1)[0][0]))\n",
        "    plt.show\n",
        "\n",
        "train_gen.reset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNm8VPigbjay",
        "colab_type": "text"
      },
      "source": [
        "## Analyze frequency label in generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7QYMSTWbaof",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "outputId": "a9930695-9078-4380-86f4-6cebaf39a929"
      },
      "source": [
        "labels, freq = get_frequency_from_generator(train_gen)\n",
        "plot_label_frequency(labels, freq)\n",
        "freq_normalize, weigh = normalize_frequency(freq)\n",
        "#plot_label_frequency(labels, freq_normalize)\n",
        "plot_stacked_bar_freq(labels, freq, freq_normalize)\n",
        "\n",
        "# dict for weigh model\n",
        "weigh_class = {}\n",
        "for label,i in zip(labels, weigh):\n",
        "  weigh_class[from_label_to_number(mapping, label)] = i\n",
        "\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "weigh_class_sklearn = class_weight.compute_class_weight('balanced',\n",
        "                                                 np.unique(train_gen.classes),\n",
        "                                                 train_gen.classes)\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ4AAAD4CAYAAADcpoD8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAD4CAYAAADLhBA1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWSacUMzMLBNOKGZmlgknFDMzy8T/BzfHShP7jHwXAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYnY8Dp84Utn",
        "colab_type": "text"
      },
      "source": [
        "## Focal loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kf8VcmVF4Xn_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def focal_loss(gamma=2., alpha=4.):\n",
        "\n",
        "    gamma = float(gamma)\n",
        "    alpha = float(alpha)\n",
        "\n",
        "    def focal_loss_fixed(y_true, y_pred):\n",
        "        \"\"\"Focal loss for multi-classification\n",
        "        FL(p_t)=-alpha(1-p_t)^{gamma}ln(p_t)\n",
        "        Notice: y_pred is probability after softmax\n",
        "        gradient is d(Fl)/d(p_t) not d(Fl)/d(x) as described in paper\n",
        "        d(Fl)/d(p_t) * [p_t(1-p_t)] = d(Fl)/d(x)\n",
        "        Focal Loss for Dense Object Detection\n",
        "        https://arxiv.org/abs/1708.02002\n",
        "\n",
        "        Arguments:\n",
        "            y_true {tensor} -- ground truth labels, shape of [batch_size, num_cls]\n",
        "            y_pred {tensor} -- model's output, shape of [batch_size, num_cls]\n",
        "\n",
        "        Keyword Arguments:\n",
        "            gamma {float} -- (default: {2.0})\n",
        "            alpha {float} -- (default: {4.0})\n",
        "\n",
        "        Returns:\n",
        "            [tensor] -- loss.\n",
        "        \"\"\"\n",
        "        epsilon = 1.e-9\n",
        "        y_true = tf.convert_to_tensor(y_true, tf.float32)\n",
        "        y_pred = tf.convert_to_tensor(y_pred, tf.float32)\n",
        "\n",
        "        model_out = tf.add(y_pred, epsilon)\n",
        "        ce = tf.multiply(y_true, -tf.log(model_out))\n",
        "        weight = tf.multiply(y_true, tf.pow(tf.subtract(1., model_out), gamma))\n",
        "        fl = tf.multiply(alpha, tf.multiply(weight, ce))\n",
        "        reduced_fl = tf.reduce_max(fl, axis=1)\n",
        "        return tf.reduce_mean(reduced_fl)\n",
        "    return focal_loss_fixed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAg57-_ib21t",
        "colab_type": "text"
      },
      "source": [
        "## Set up iper-parameter for fit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QSFiR2Ab6v1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss = 'categorical_crossentropy'\n",
        "optimizer = 'adam'\n",
        "reduce_on_plateau = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, \n",
        "                                      verbose=1, mode='auto', min_delta=0.0001, \n",
        "                                      cooldown=0, min_lr=0.00001)\n",
        "early_stop = EarlyStopping(monitor='val_loss', \n",
        "                           patience=3, \n",
        "                           verbose=1\n",
        "                           )\n",
        "metrics = ['categorical_accuracy']\n",
        "epochs = 30\n",
        "steps_per_epoch = ceil(train_gen.n/BATCH_SIZE)\n",
        "validation_steps = ceil(val_gen.n/BATCH_SIZE)\n",
        "class_weight = weigh_class\n",
        "dense = [256, 256]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9HmoEWOUkkU",
        "colab_type": "code",
        "outputId": "093afaed-edd0-4bc9-aef8-0cbe2614d3ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "model = create_model( base_net=base_net, \n",
        "                      freeze_all=True, \n",
        "                      freeze_to=0, \n",
        "                      dense=dense,\n",
        "                      dropout=0.3,\n",
        "                      im_size=IM_SIZE\n",
        "                     )\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss=loss, \n",
        "              optimizer=optimizer, \n",
        "              metrics=metrics\n",
        "              )\n",
        "\n",
        "net_history = model.fit_generator(train_gen, epochs=epochs, verbose=1,\n",
        "                                  validation_data = val_gen,\n",
        "                                  steps_per_epoch = steps_per_epoch,\n",
        "                                  validation_steps = validation_steps,\n",
        "                                  callbacks = [early_stop],\n",
        "                                  #class_weight = class_weight\n",
        "                                  )"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "247/247 [==============================] - 197s 796ms/step - loss: 1.4561 - categorical_accuracy: 0.5710 - val_loss: 1.1792 - val_categorical_accuracy: 0.6400\n",
            "Epoch 2/30\n",
            "247/247 [==============================] - 181s 733ms/step - loss: 0.8215 - categorical_accuracy: 0.7467 - val_loss: 1.0382 - val_categorical_accuracy: 0.6870\n",
            "Epoch 3/30\n",
            "247/247 [==============================] - 181s 732ms/step - loss: 0.5699 - categorical_accuracy: 0.8179 - val_loss: 1.1313 - val_categorical_accuracy: 0.6722\n",
            "Epoch 4/30\n",
            "247/247 [==============================] - 181s 733ms/step - loss: 0.4137 - categorical_accuracy: 0.8692 - val_loss: 1.3497 - val_categorical_accuracy: 0.6459\n",
            "Epoch 5/30\n",
            "247/247 [==============================] - 181s 733ms/step - loss: 0.3052 - categorical_accuracy: 0.9035 - val_loss: 1.3188 - val_categorical_accuracy: 0.6759\n",
            "Epoch 00005: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}