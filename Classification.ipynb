{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "bGjYNEjPFiCE",
        "eD9R4p_JGS8j",
        "bwTcVNKCAoJt",
        "awq2lzbxBXgj",
        "AHm9t-_NGYDa",
        "WgzdNcHZGgch",
        "ISzDSogyQ12u",
        "dDw9W32GaThb",
        "XXwxrNdCShlc",
        "oXNhkv10FLgG",
        "l7fUcTZApdLB",
        "1gpMHcy2SpB7",
        "22ReLsjTEeCZ",
        "se7bKuAJbulU",
        "pNm8VPigbjay",
        "oAg57-_ib21t",
        "Pz6Y1ic80Rz2",
        "8U2BptkC0JcN",
        "mrlxqfdszCbf",
        "-KaAO8omzV73",
        "yJrfI4v50nnV"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sbarbagnem/AdvancedProject/blob/master/Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGjYNEjPFiCE",
        "colab_type": "text"
      },
      "source": [
        "# Module\n",
        "**caricare prima librerie di smac e poi il resto**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_AUBPWzkgSI",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "7a812fd4-99be-4003-a54f-2851c608400d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#!pip uninstall scikit-learn -y\n",
        "#!pip install scikit-learn==0.21.3\n",
        "!apt-get install swig\n",
        "!pip install smac\n",
        "!pip install smac[all]\n",
        "!pip install smt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "swig is already the newest version (3.0.12-1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 43 not upgraded.\n",
            "Requirement already satisfied: smac in /usr/local/lib/python3.6/dist-packages (0.12.2)\n",
            "Requirement already satisfied: scikit-learn>=0.22.0 in /usr/local/lib/python3.6/dist-packages (from smac) (0.22.2.post1)\n",
            "Requirement already satisfied: pynisher>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from smac) (0.5.0)\n",
            "Requirement already satisfied: ConfigSpace<0.5,>=0.4.9 in /usr/local/lib/python3.6/dist-packages (from smac) (0.4.13)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (from smac) (5.4.8)\n",
            "Requirement already satisfied: pyrfr>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from smac) (0.8.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from smac) (0.15.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from smac) (1.4.1)\n",
            "Requirement already satisfied: lazy-import in /usr/local/lib/python3.6/dist-packages (from smac) (0.2.2)\n",
            "Requirement already satisfied: sobol-seq in /usr/local/lib/python3.6/dist-packages (from smac) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.7.1 in /usr/local/lib/python3.6/dist-packages (from smac) (1.18.5)\n",
            "Requirement already satisfied: docutils>=0.3 in /usr/local/lib/python3.6/dist-packages (from pynisher>=0.4.1->smac) (0.15.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pynisher>=0.4.1->smac) (47.1.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from ConfigSpace<0.5,>=0.4.9->smac) (0.29.19)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from ConfigSpace<0.5,>=0.4.9->smac) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from lazy-import->smac) (1.12.0)\n",
            "Requirement already satisfied: smac[all] in /usr/local/lib/python3.6/dist-packages (0.12.2)\n",
            "Requirement already satisfied: pyrfr>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from smac[all]) (0.8.0)\n",
            "Requirement already satisfied: lazy-import in /usr/local/lib/python3.6/dist-packages (from smac[all]) (0.2.2)\n",
            "Requirement already satisfied: ConfigSpace<0.5,>=0.4.9 in /usr/local/lib/python3.6/dist-packages (from smac[all]) (0.4.13)\n",
            "Requirement already satisfied: numpy>=1.7.1 in /usr/local/lib/python3.6/dist-packages (from smac[all]) (1.18.5)\n",
            "Requirement already satisfied: scikit-learn>=0.22.0 in /usr/local/lib/python3.6/dist-packages (from smac[all]) (0.22.2.post1)\n",
            "Requirement already satisfied: sobol-seq in /usr/local/lib/python3.6/dist-packages (from smac[all]) (0.2.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from smac[all]) (0.15.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (from smac[all]) (5.4.8)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from smac[all]) (1.4.1)\n",
            "Requirement already satisfied: pynisher>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from smac[all]) (0.5.0)\n",
            "Requirement already satisfied: emcee>=2.1.0; extra == \"all\" in /usr/local/lib/python3.6/dist-packages (from smac[all]) (3.0.2)\n",
            "Requirement already satisfied: sphinx-rtd-theme; extra == \"all\" in /usr/local/lib/python3.6/dist-packages (from smac[all]) (0.4.3)\n",
            "Requirement already satisfied: sphinx-gallery==0.5.0; extra == \"all\" in /usr/local/lib/python3.6/dist-packages (from smac[all]) (0.5.0)\n",
            "Requirement already satisfied: scikit-optimize; extra == \"all\" in /usr/local/lib/python3.6/dist-packages (from smac[all]) (0.7.4)\n",
            "Requirement already satisfied: pyDOE; extra == \"all\" in /usr/local/lib/python3.6/dist-packages (from smac[all]) (0.3.8)\n",
            "Requirement already satisfied: sphinx; extra == \"all\" in /usr/local/lib/python3.6/dist-packages (from smac[all]) (1.8.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from lazy-import->smac[all]) (1.12.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from ConfigSpace<0.5,>=0.4.9->smac[all]) (0.29.19)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from ConfigSpace<0.5,>=0.4.9->smac[all]) (2.4.7)\n",
            "Requirement already satisfied: docutils>=0.3 in /usr/local/lib/python3.6/dist-packages (from pynisher>=0.4.1->smac[all]) (0.15.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pynisher>=0.4.1->smac[all]) (47.1.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from sphinx-gallery==0.5.0; extra == \"all\"->smac[all]) (3.2.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from sphinx-gallery==0.5.0; extra == \"all\"->smac[all]) (7.0.0)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize; extra == \"all\"->smac[all]) (20.4.0)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx; extra == \"all\"->smac[all]) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from sphinx; extra == \"all\"->smac[all]) (20.4)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx; extra == \"all\"->smac[all]) (1.2.0)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx; extra == \"all\"->smac[all]) (1.2.2)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from sphinx; extra == \"all\"->smac[all]) (2.23.0)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx; extra == \"all\"->smac[all]) (2.8.0)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.6/dist-packages (from sphinx; extra == \"all\"->smac[all]) (2.11.2)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx; extra == \"all\"->smac[all]) (2.1.3)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx; extra == \"all\"->smac[all]) (0.7.12)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->sphinx-gallery==0.5.0; extra == \"all\"->smac[all]) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->sphinx-gallery==0.5.0; extra == \"all\"->smac[all]) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->sphinx-gallery==0.5.0; extra == \"all\"->smac[all]) (2.8.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from pyaml>=16.9->scikit-optimize; extra == \"all\"->smac[all]) (3.13)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx; extra == \"all\"->smac[all]) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx; extra == \"all\"->smac[all]) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx; extra == \"all\"->smac[all]) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx; extra == \"all\"->smac[all]) (2.9)\n",
            "Requirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.6/dist-packages (from babel!=2.0,>=1.3->sphinx; extra == \"all\"->smac[all]) (2018.9)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.3->sphinx; extra == \"all\"->smac[all]) (1.1.1)\n",
            "Requirement already satisfied: smt in /usr/local/lib/python3.6/dist-packages (0.5.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from smt) (0.22.2.post1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from smt) (20.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from smt) (1.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from smt) (3.2.1)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from smt) (1.12.0)\n",
            "Requirement already satisfied: numpydoc in /usr/local/lib/python3.6/dist-packages (from smt) (1.0.0)\n",
            "Requirement already satisfied: pyDOE2 in /usr/local/lib/python3.6/dist-packages (from smt) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->smt) (1.18.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->smt) (0.15.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->smt) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->smt) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->smt) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->smt) (1.2.0)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.6/dist-packages (from numpydoc->smt) (2.11.2)\n",
            "Requirement already satisfied: sphinx>=1.6.5 in /usr/local/lib/python3.6/dist-packages (from numpydoc->smt) (1.8.5)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.3->numpydoc->smt) (1.1.1)\n",
            "Requirement already satisfied: docutils>=0.11 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc->smt) (0.15.2)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc->smt) (2.0.0)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc->smt) (2.1.3)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc->smt) (2.23.0)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc->smt) (2.8.0)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc->smt) (1.2.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc->smt) (47.1.1)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc->smt) (1.2.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc->smt) (0.7.12)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx>=1.6.5->numpydoc->smt) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx>=1.6.5->numpydoc->smt) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx>=1.6.5->numpydoc->smt) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx>=1.6.5->numpydoc->smt) (1.24.3)\n",
            "Requirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.6/dist-packages (from babel!=2.0,>=1.3->sphinx>=1.6.5->numpydoc->smt) (2018.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJr3vEODFZE1",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "81a3d90f-01e0-4937-e10e-104bc8297dc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from math import ceil\n",
        "import xml.etree.ElementTree as ET\n",
        "from google.colab import drive\n",
        "from google.colab.patches import cv2_imshow\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import gc\n",
        "\n",
        "from keras.models import Model, load_model\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications import VGG16, ResNet50, MobileNet\n",
        "from keras.applications.vgg16 import preprocess_input as preprocess_input_vgg16\n",
        "from keras.applications.resnet50 import preprocess_input as preprocess_input_resnet50\n",
        "from keras.applications.mobilenet import preprocess_input as preprocess_input_mobilenet\n",
        "from keras.layers import Dense, Dropout, GlobalAveragePooling2D, Flatten, BatchNormalization, Activation\n",
        "from keras import Model\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras import backend as K\n",
        "\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8YYvM7UbCMd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from smac.configspace import ConfigurationSpace \n",
        "from smac.configspace import UniformFloatHyperparameter, UniformIntegerHyperparameter, CategoricalHyperparameter, InCondition\n",
        "from smac.configspace import Configuration # serve per costruire le configurazioni iniziali da passare alla funzione di minimizzazione\n",
        "from smac.scenario.scenario import Scenario\n",
        "from smac.facade.smac_bo_facade import SMAC4BO # bayesian optimization with GaussianProcess\n",
        "from smac.facade.smac_hpo_facade import SMAC4HPO # bayesian optimization with RandomForest\n",
        "from smac.optimizer.acquisition import EI, LCB, PI\n",
        "from smac.runhistory.runhistory import RunHistory # util class to collect all data of optimization\n",
        "from smac.initial_design.latin_hypercube_design import LHDesign\n",
        "from smac.initial_design.random_configuration_design import RandomConfigurations\n",
        "from smac.stats.stats import Stats # util class to save history on directory\n",
        "from smac.utils.io.traj_logging import TrajLogger"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eD9R4p_JGS8j",
        "colab_type": "text"
      },
      "source": [
        "# Costant"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jui_q9SGGXsi",
        "colab_type": "code",
        "outputId": "297a8e37-eb1d-4042-ed5e-01aafe462cd8",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "PATH_ANNOTATIONS = '/content/drive/My Drive/Laurea Magistrale/Adavanched Machine Learning/Progetto/Dataset/Train/Annotations/'\n",
        "PATH_MAIN = '/content/drive/My Drive/Laurea Magistrale/Adavanched Machine Learning/Progetto/Dataset/Train/Main/'\n",
        "PATH_IMAGES = '/content/drive/My Drive/Laurea Magistrale/Adavanched Machine Learning/Progetto/Dataset/Train/Images/'\n",
        "PATH_IMAGES_CROPPED_TRAIN_BALANCED = '/content/drive/My Drive/Laurea Magistrale/Adavanched Machine Learning/Progetto/Dataset/Train/Cropped_train_balanced/'\n",
        "PATH_IMAGES_CROPPED_VAL_TEST_BALANCED = '/content/drive/My Drive/Laurea Magistrale/Adavanched Machine Learning/Progetto/Dataset/Train/Cropped_val_test_balanced/'\n",
        "PATH_OPTIMIZATION = '/content/drive/My Drive/Laurea Magistrale/Adavanched Machine Learning/Progetto/Ottimizzazione/'\n",
        "PATH_PICKLE = '/content/drive/My Drive/Laurea Magistrale/Adavanched Machine Learning/Progetto/Pickles/'\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "IM_SIZE = (128, 128)\n",
        "\n",
        "LABELS = ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', \n",
        "          'car', 'cat', 'chair', 'cow', 'diningtable',\n",
        "          'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa',\n",
        "          'train', 'tvmonitor']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwTcVNKCAoJt",
        "colab_type": "text"
      },
      "source": [
        "# Directory for cropped images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shqDlbvsAsmi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creo 20 directory, una per ogni classe, per fare classificazione (flow_from_datframe) sulle immagini croppate\n",
        "def create_directories(path, labels):\n",
        "  for label in labels:\n",
        "    os.mkdir(os.path.join(path, label))\n",
        "\n",
        "create_folder_classes = True\n",
        "\n",
        "if create_folder_classes == True:\n",
        "  create_directories(PATH_IMAGES_CROPPED_TRAIN_BALANCED, LABELS)\n",
        "  create_directories(PATH_IMAGES_CROPPED_VAL_TEST_BALANCED, LABELS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awq2lzbxBXgj",
        "colab_type": "text"
      },
      "source": [
        "# Util function for mapping from label to #class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NN3ewyc_BenA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_dict_mapping(labels):\n",
        "  mapping = {}\n",
        "  for label,i in zip(labels, range(len(labels))):\n",
        "    mapping[label] = i\n",
        "  return mapping\n",
        "\n",
        "def from_label_to_number(mapping, label):\n",
        "  return mapping[label]\n",
        "\n",
        "def from_number_to_label(mapping, number):\n",
        "  for key, val in mapping.items(): \n",
        "    if val == number: \n",
        "      return key \n",
        "  return \"key doesn't exist\"\n",
        "\n",
        "def from_onehot_to_label(mapping, one_hot):\n",
        "  return from_number_to_label(mapping,np.where(one_hot == 1)[0][0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EGlorv3K-NG",
        "colab_type": "code",
        "outputId": "441d02f2-a8c2-4ae3-f046-828529710ded",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "mapping = create_dict_mapping(LABELS)\n",
        "print(mapping)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'aeroplane': 0, 'bicycle': 1, 'bird': 2, 'boat': 3, 'bottle': 4, 'bus': 5, 'car': 6, 'cat': 7, 'chair': 8, 'cow': 9, 'diningtable': 10, 'dog': 11, 'horse': 12, 'motorbike': 13, 'person': 14, 'pottedplant': 15, 'sheep': 16, 'sofa': 17, 'train': 18, 'tvmonitor': 19}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHm9t-_NGYDa",
        "colab_type": "text"
      },
      "source": [
        "# Crop and save image for class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5P1zAHxAGdTJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def crop_image(img, x_min, y_min, x_max, y_max):\n",
        "  crop_img = img[y_min:y_max, x_min:x_max]\n",
        "  if (y_max-y_min) >= 100 and (x_max-x_min) >= 100:\n",
        "    crop_img = cv2.resize(crop_img, IM_SIZE)\n",
        "    return crop_img\n",
        "  else:\n",
        "    return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QkTnM8cPtA_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def list_images(path_images, path_txt, file_txt):\n",
        "  # creo liste immagini presenti in file_txt\n",
        "  temp = []\n",
        "  f = open(os.path.join(path_txt, file_txt), \"r\")\n",
        "  for line in f.readlines():\n",
        "    temp.append(line.split('\\n')[0] + '.jpg')\n",
        "  list_images = [os.path.join(path_images, name) for name in temp]\n",
        "  print('Ho trovato ', len(list_images), 'per il file ', file_txt)\n",
        "  return list_images\n",
        "\n",
        "# creo liste immagini da train.txt e val.txt\n",
        "list_images_train = list_images(PATH_IMAGES, PATH_MAIN, 'train.txt')\n",
        "list_images_val = list_images(PATH_IMAGES, PATH_MAIN, 'val.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gt_3vjWp7sUu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_count = {}\n",
        "\n",
        "# dict to count number of image for every class\n",
        "for label in LABELS:\n",
        "  train_count[label] = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCnf_0tOQ687",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_and_crop(list_images, annotation_dir, path_to_save, train=False):\n",
        "  '''\n",
        "  leggo annotations dei file presenti nelle due liste di immagini \n",
        "  '''\n",
        "  for path in list_images:\n",
        "    image_name = path.split('/')[-1].split('.')[0] \n",
        "    print(image_name)\n",
        "    with open(os.path.join(annotation_dir, image_name + '.xml')) as f:\n",
        "      read_xml(f.read(), path, path_to_save, train)\n",
        "  return\n",
        "\n",
        "def read_xml(file_xml, path_image, path_to_save, train=False):\n",
        "  '''\n",
        "  leggo xml e per ogni box che trovo croppo e salvo\n",
        "  '''\n",
        "  #print(path_image)\n",
        "  img = cv2.imread(path_image)  \n",
        "  root = ET.XML(file_xml)\n",
        "  for _, child in enumerate(root):\n",
        "    if child.tag == 'object':\n",
        "      x_min = None\n",
        "      y_min = None\n",
        "      x_max = None\n",
        "      y_max = None\n",
        "      for subchild in child:\n",
        "        if subchild.tag == 'name':\n",
        "          name_object = subchild.text\n",
        "          #print(name_object)\n",
        "        if subchild.tag == 'bndbox':\n",
        "          for bndbox in subchild:\n",
        "            if bndbox.tag == 'xmin':\n",
        "              x_min = int(bndbox.text)\n",
        "              #print('x_min ', x_min)\n",
        "            if bndbox.tag == 'ymin':\n",
        "              y_min = int(bndbox.text)\n",
        "              #print('y_min ', y_min)\n",
        "            if bndbox.tag == 'xmax':\n",
        "              x_max = int(bndbox.text)\n",
        "              #print('x_max ', x_max)\n",
        "            if bndbox.tag == 'ymax':\n",
        "              y_max = int(bndbox.text)\n",
        "              #print('y_max ', y_max)\n",
        "        if(x_min!=None and y_min!=None and x_max!=None and y_max!=None):\n",
        "          if(train_count[name_object] < 500 and train) or (not train):\n",
        "            image_cropped = crop_image(img, x_min, y_min, x_max, y_max)\n",
        "            x_min = None\n",
        "            y_min = None\n",
        "            x_max = None\n",
        "            y_max = None\n",
        "            #cv2_imshow(image_cropped)\n",
        "            if image_cropped is not None:\n",
        "              save_image_cropped(name_object, image_cropped, path_to_save, train)\n",
        "  return \n",
        "\n",
        "def save_image_cropped(obj, img, path_to_save, train=False):\n",
        "  '''\n",
        "  salvo immagine croppata con numero progressivo in base \n",
        "  all'ultima presente nella cartella\n",
        "  '''\n",
        "  list_dir_classes = os.listdir(path_to_save)\n",
        "  for dir_class in list_dir_classes:\n",
        "    if dir_class == obj:\n",
        "      dir_temp = os.path.join(path_to_save,dir_class)\n",
        "      list_temp = os.listdir(dir_temp)\n",
        "      if list_temp == []:\n",
        "        cv2.imwrite(os.path.join(dir_temp, obj + '_1.jpg'), img)\n",
        "        if train:\n",
        "          train_count[obj] += 1\n",
        "      else:\n",
        "        number_file = int(list_temp[-1].split('.')[0].split('_')[1]) + 1\n",
        "        cv2.imwrite(os.path.join(dir_temp, obj + '_' + str(number_file) + '.jpg'), img)\n",
        "        if train:\n",
        "          train_count[obj] += 1\n",
        "  return\n",
        "\n",
        "#read_and_crop(list_images=list_images_train, annotation_dir=PATH_ANNOTATIONS, path_to_save=PATH_IMAGES_CROPPED_TRAIN_BALANCED, train=True)\n",
        "read_and_crop(list_images=list_images_val, annotation_dir=PATH_ANNOTATIONS, path_to_save=PATH_IMAGES_CROPPED_VAL_TEST_BALANCED, train=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJ8TWz7E5pKU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_count\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgzdNcHZGgch",
        "colab_type": "text"
      },
      "source": [
        "# Generator from directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMtwpZ_kGfsx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_generator( batch_size, im_size, train_directory, val_directory, labels,\n",
        "                      validation_split, aug, augment_params):\n",
        "    \n",
        "  preprocess_function = preprocess_input_vgg16\n",
        "\n",
        "  if not(aug):\n",
        "    img_gen_train = ImageDataGenerator(#rescale=1./255, \n",
        "                                       preprocessing_function=preprocess_function)\n",
        "  elif aug:\n",
        "    img_gen_train = ImageDataGenerator( preprocessing_function=preprocess_function,\n",
        "                                        #rescale=1./255,\n",
        "                                        **augment_params)  \n",
        "\n",
        "\n",
        "  img_gen_val = ImageDataGenerator( preprocessing_function=preprocess_function,\n",
        "                                    validation_split = validation_split\n",
        "                                  )\n",
        "    \n",
        "  train_gen = img_gen_train.flow_from_directory(\n",
        "      directory = train_directory,\n",
        "      shuffle=True,\n",
        "      class_mode='categorical',\n",
        "      target_size=im_size,\n",
        "      batch_size=batch_size,\n",
        "      classes=labels,\n",
        "      seed=42\n",
        "  )\n",
        "\n",
        "  val_gen = img_gen_val.flow_from_directory(\n",
        "      directory = val_directory,\n",
        "      shuffle=False,\n",
        "      class_mode='categorical',\n",
        "      target_size=im_size,\n",
        "      batch_size=batch_size,\n",
        "      subset='training',\n",
        "      classes=labels,\n",
        "      seed=42\n",
        "  )\n",
        "\n",
        "  test_gen = img_gen_val.flow_from_directory(\n",
        "      directory = val_directory,\n",
        "      shuffle=False,\n",
        "      class_mode='categorical',\n",
        "      target_size=im_size,\n",
        "      batch_size=batch_size,\n",
        "      subset='validation',\n",
        "      classes=labels,\n",
        "      seed=42\n",
        "  )\n",
        "\n",
        "  return train_gen, val_gen, test_gen"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISzDSogyQ12u",
        "colab_type": "text"
      },
      "source": [
        "# Generator from image in memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z27ENn5qQ9AR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_generator_for_pickle( images_train, labels_train, images_val, labels_val, batch_size, base_net,\n",
        "                      validation_split, aug, augment_params):\n",
        "  \n",
        "  if base_net == 'vgg16':\n",
        "    preprocess_function = preprocess_input_vgg16\n",
        "  elif base_net == 'mobilenet':\n",
        "    preprocess_function = preprocess_input_mobilenet\n",
        "  elif base_net == 'resnet':\n",
        "    preprocess_function = preprocess_input_resnet50\n",
        "\n",
        "  if not(aug):\n",
        "    img_gen_train = ImageDataGenerator(preprocessing_function=preprocess_function)\n",
        "  elif aug:\n",
        "    img_gen_train = ImageDataGenerator( preprocessing_function=preprocess_function,\n",
        "                                        **augment_params)  \n",
        "\n",
        "\n",
        "  img_gen_val = ImageDataGenerator( preprocessing_function=preprocess_function,\n",
        "                                    validation_split = validation_split\n",
        "                                  )\n",
        "    \n",
        "  train_gen = img_gen_train.flow(\n",
        "      x = images_train,\n",
        "      y = labels_train,\n",
        "      shuffle=True,\n",
        "      batch_size=batch_size,\n",
        "      seed=42\n",
        "  )\n",
        "\n",
        "  val_gen = img_gen_val.flow(\n",
        "      x = images_val,\n",
        "      y = labels_val,\n",
        "      shuffle=False,\n",
        "      batch_size=batch_size,\n",
        "      subset='training',\n",
        "      seed=42\n",
        "  )\n",
        "\n",
        "  test_gen = img_gen_val.flow(\n",
        "      x = images_val,\n",
        "      y = labels_val,\n",
        "      shuffle=False,\n",
        "      batch_size=batch_size,\n",
        "      subset='validation',\n",
        "      seed=42\n",
        "  )\n",
        "\n",
        "  return train_gen, val_gen, test_gen"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDw9W32GaThb",
        "colab_type": "text"
      },
      "source": [
        "# Frequency classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teCq1HPhaXTg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_frequency_from_generator(generator):\n",
        "  mapping = generator.class_indices\n",
        "  classes, count = np.unique(generator.labels, return_counts=True)\n",
        "  labels = [from_number_to_label(mapping, label) for label in classes]\n",
        "  freq = count\n",
        "  return labels, freq\n",
        "\n",
        "def normalize_frequency(freq):\n",
        "  min_freq = np.min(freq)\n",
        "  weigh = [min_freq/x for x in freq]\n",
        "  return weigh * freq, weigh\n",
        "\n",
        "def plot_label_frequency(labels, freq):\n",
        "  freq = freq / np.sum(freq)\n",
        "  l = list(range(1, len(labels)+1))\n",
        "  plt.barh(l, width=freq, height=0.5)\n",
        "  plt.yticks(l, labels, rotation='horizontal')\n",
        "  plt.show()\n",
        "\n",
        "def plot_stacked_bar_freq(labels, freq, freq_normalize):\n",
        "  N = len(labels)\n",
        "  ind = np.arange(N)    # the x locations for the groups\n",
        "  width = 0.35       # the width of the bars: can also be len(x) sequence\n",
        "\n",
        "  p1 = plt.bar(ind, freq, width)\n",
        "  p2 = plt.bar(ind, freq_normalize, width)\n",
        "\n",
        "  plt.ylabel('Frequency')\n",
        "  plt.legend((p1[0], p2[0]), ('Freq', 'Freq_normalize'))\n",
        "\n",
        "  plt.show() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BR99sl1tKkgt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels, freq = get_frequency_from_generator(train_gen)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hg48KrlvhswU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(labels)\n",
        "print(freq)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWJR49amiauf",
        "colab_type": "code",
        "outputId": "4da348a9-4b5b-4033-bd2c-dc49b45741ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "plot_label_frequency(labels, freq)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jo-t0MmYjmXY",
        "colab_type": "code",
        "outputId": "c46ac3fb-bd96-472a-ad36-4a8c6ab5344d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "freq_normalize = normalize_frequency(freq)\n",
        "print(freq_normalize)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(array([317., 317., 317., 317., 317., 317., 317., 317., 317., 317., 317.,\n",
            "       317., 317., 317., 317., 317., 317., 317., 317., 317.]), [0.674468085106383, 0.7731707317073171, 0.535472972972973, 0.6240157480314961, 0.4232309746328438, 1.0, 0.2661628883291352, 0.5205254515599343, 0.2175703500343171, 0.8929577464788733, 0.8498659517426274, 0.4127604166666667, 0.8408488063660478, 0.8453333333333334, 0.06315999203028491, 0.5691202872531418, 0.6227897838899804, 0.7944862155388471, 0.9694189602446484, 0.7694174757281553])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvIqlKPijrTx",
        "colab_type": "code",
        "outputId": "f531e9b0-7078-4bd4-9f6d-b69b79294d6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "plot_stacked_bar_freq(labels, freq, freq_normalize[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAD4CAYAAADLhBA1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAbOElEQVR4nO3de5QV5Znv8e+PS0SFgJeOwwFjo0GjCXKxUTxRx8sIaBK8JCZ6nBEZjySCWWadkzXKmCjxcpY5cdRJ4uRERw6QEK+jiIk5AY2XmEShwRZQlEYDsYkKA15gFLXjc/7Yb/dsoLvZNFV796Z/n7X22lVvvbXr6erqevp9691VigjMzMx2VY9KB2BmZrsHJxQzM8uEE4qZmWXCCcXMzDLhhGJmZpnoVekA8rD//vtHbW1tpcMwM6sqixcv/veIqOns+rtlQqmtraW+vr7SYZiZVRVJa3ZlfXd5mZlZJpxQzMwsE04oZmaWid3yGkpbPvzwQ5qamtiyZUulQ7Gd1KdPHwYPHkzv3r0rHYqZdaDbJJSmpib69etHbW0tkiodjpUoItiwYQNNTU0MGTKk0uGYWQe6TZfXli1b2G+//ZxMqowk9ttvP7cszapArglF0mpJyyQ1SKpPZftKWiCpMb3vk8ol6QeSVklaKmlU0edMTPUbJU3chXh2/YeysvPvzaw6lKOFclJEjIiIujR/BfBoRAwFHk3zAKcBQ9NrMvBjKCQg4GrgGOBo4OqWJGRmZl1HJa6hnAGcmKZnAY8Dl6fy2VF4QMvTkgZIGpjqLoiIjQCSFgDjgTt3JYjaK365K6tvZ/UNn99hnZ49ezJs2LDW+blz5+Jv9JvZ7iLvhBLAfEkB/CQibgMOiIjX0vLXgQPS9CDg1aJ1m1JZe+VbkTSZQsuGT37yk1n+DJnZc889aWhoaHNZRBAR9OjRbS5rmQFt/3NXyj9o1vXkffY6LiJGUejOmirphOKFqTWSySMjI+K2iKiLiLqamk7fiqasVq9ezWGHHcYFF1zAZz/7WV599VW+//3vM3r0aI488kiuvvrq1rrXX389hx56KMcddxznnXceN954YwUjNzPbXq4JJSLWpvd1wAMUroG8kbqySO/rUvW1wIFFqw9OZe2VV5333nuPESNGMGLECM466ywAGhsbmTJlCs8//zwvvfQSjY2NLFy4kIaGBhYvXsyTTz7J4sWLueuuu2hoaODhhx9m0aJFFf5JzMy2l1uXl6S9gR4RsSlNjwWuAeYBE4Eb0vuDaZV5wKWS7qJwAf7tiHhN0q+B/1V0IX4sMC2vuPO0bZfX6tWrOeiggxgzZgwA8+fPZ/78+YwcORKAzZs309jYyKZNmzjrrLPYa6+9AJgwYUL5gzcz24E8r6EcADyQhnz2An4eEf9P0iLgHkkXAWuAr6T6DwOnA6uAd4FJABGxUdK1QMu/5de0XKDfHey9996t0xHBtGnT+NrXvrZVnVtuuaXcYZmZ7bTcurwi4pWIGJ5en4mI61P5hog4JSKGRsTftCSHKJgaEYdExLCIqC/6rBkR8an0+r95xVxp48aNY8aMGWzevBmAtWvXsm7dOk444QTmzp3Le++9x6ZNm3jooYcqHKmZ2fa6za1XttUVR5GMHTuWFStWcOyxxwLQt29ffvaznzFq1Ci++tWvMnz4cD7xiU8wevToCkdqZrY9j1Eto5aWR4va2lqWL1++Vdlll13GsmXLWLZsGX/4wx845JBDALjyyitZuXIlTz31FIceemjZYjYzK5UTipmZZaLbdnlVs+nTp1c6BDOz7biFYmZmmXBCMTOzTDihmJlZJpxQzMwsE933ovz0/hl/3tvZfp6ZWZVxC6WMevbs2XpzyBEjRrB69epKh5SL6dOnt94N+aqrruKRRx6pcERmVg7dt4VSAdXwPJTm5mZ69crusLjmmmsy+ywz69rcQqmgvJ6HcuKJJ3L55Zdz9NFHc+ihh/Lb3/4WgC1btjBp0iSGDRvGyJEjeeyxxwCYOXMmEyZM4OSTT+aUU05h5syZnHnmmZx66qnU1tbyox/9iJtuuomRI0cyZswYNm4s3Jvz9ttvZ/To0QwfPpwvfelLvPvuu9vFcuGFF3LfffdRX1/f2jIbNmxY63PiX375ZcaPH89RRx3F8ccfz4svvpjZ/jWz8nJCKaNyPg+lubmZhQsXcsstt/Dd734XgFtvvRVJLFu2jDvvvJOJEyeyZcsWAJYsWcJ9993HE088AcDy5cu5//77WbRoEVdeeSV77bUXzz77LMceeyyzZ88G4Oyzz2bRokU899xzHH744dxxxx3txlNXV0dDQwMNDQ2MHz+eb33rWwBMnjyZH/7whyxevJgbb7yRKVOmdH4Hm1lFucurjMr5PJSzzz4bgKOOOqr1Ws1TTz3FN77xDQA+/elPc9BBB7Fy5UoATj31VPbdd9/W9U866ST69etHv3796N+/P1/84hcBGDZsGEuXLgUKSefb3/42b731Fps3b2bcuHE7jOvuu+9myZIlzJ8/n82bN/P73/+ec845p3X5+++/v8PPMLOuyQmlwvJ6Hsoee+wBFAYCNDc371QcxesD9OjRo3W+R48erZ934YUXMnfuXIYPH87MmTN5/PHHO9zG8uXLmT59Ok8++SQ9e/bko48+YsCAAe1eVzKz6tJ9E0oXHOY7btw4vvOd73D++efTt29f1q5dS+/evTnhhBO48MILmTZtGs3NzTz00EPbJZ1SHH/88cyZM4eTTz6ZlStX8qc//YnDDjuMJUuWdCreTZs2MXDgQD788EPmzJnDoEGD2q371ltvcd555zF79mxqamoA+PjHP86QIUO49957Oeecc4gIli5dyvDhwzsVj5lVVvdNKF1Q3s9DmTJlCpdccgnDhg2jV69ezJw5c6uWyM669tprOeaYY6ipqeGYY45h06ZN7dZ98MEHWbNmDRdffHFrWUNDA3PmzOGSSy7huuuu48MPP+Tcc891QjGrUoqISseQubq6uqivr9+qbMWKFRx++OEViihb06dPp2/fvq0XtruD3en3Z1urveKX25V1xQfgdQeSFkdEXWfX9ygvMzPLhLu8qlDL81CmTp3K7373u62WXXbZZUyaNKkCUZlZd9etEkpEtH6hbndw6623VjqEstgdu2XNdkfdpsurT58+bNiwwSenKhMRbNiwgT59+lQ6FDPbgW7TQhk8eDBNTU2sX7++0qHYTurTpw+DBw+udBhmtgPdJqH07t2bIUOGVDoMM7PdVrfp8jIzs3w5oZiZWSacUMzMLBNOKGZmlgknFDMzy4QTipmZZcIJxczMMuGEYmZmmXBCMTOzTOSeUCT1lPSspF+k+SGSnpG0StLdkj6WyvdI86vS8tqiz5iWyl+StOMHl5uZWdmVo4VyGbCiaP57wM0R8SngTeCiVH4R8GYqvznVQ9IRwLnAZ4DxwL9I6lmGuM3MbCfkmlAkDQY+D/xrmhdwMnBfqjILODNNn5HmSctPSfXPAO6KiPcj4o/AKuDoPOM2M7Odl3cL5RbgH4CP0vx+wFsR0Zzmm4BBaXoQ8CpAWv52qt9a3sY6rSRNllQvqd53FDYzK7/cEoqkLwDrImJxXtsoFhG3RURdRNTV1NSUY5NmZlYkz9vXfw6YIOl0oA/wceCfgQGSeqVWyGBgbaq/FjgQaJLUC+gPbCgqb1G8jpmZdRG5tVAiYlpEDI6IWgoX1X8TEecDjwFfTtUmAg+m6XlpnrT8N1F4vOI84Nw0CmwIMBRYmFfcZmbWOZV4wNblwF2SrgOeBe5I5XcAP5W0CthIIQkREc9Lugd4AWgGpkbEX8oftpmZdaQsCSUiHgceT9Ov0MYorYjYApzTzvrXA9fnF6GZme0qf1PezMwy4YRiZmaZcEIxM7NMOKGYmVkmnFDMzCwTTihmZpYJJxQzM8uEE4qZmWXCCcXMzDLhhGJmZplwQjEzs0w4oZiZWSacUMzMLBNOKGZmlgknFDMzy4QTipmZZcIJxczMMuGEYmZmmXBCMTOzTDihmJlZJpxQzMwsE04oZmaWCScUMzPLhBOKmZllwgnFzMwy4YRiZmaZcEIxM7NMlJRQJA3LOxAzM6tupbZQ/kXSQklTJPXPNSIzM6tKJSWUiDgeOB84EFgs6eeSTs01MjMzqyolX0OJiEbg28DlwF8DP5D0oqSz8wrOzMyqR6nXUI6UdDOwAjgZ+GJEHJ6mb84xPjMzqxKltlB+CCwBhkfE1IhYAhARf6bQatmOpD7pustzkp6X9N1UPkTSM5JWSbpb0sdS+R5pflVaXlv0WdNS+UuSxnX+xzUzs7yUmlA+D/w8It4DkNRD0l4AEfHTdtZ5Hzg5IoYDI4DxksYA3wNujohPAW8CF6X6FwFvpvKbUz0kHQGcC3wGGE9hgEDPnfsxzcwsb6UmlEeAPYvm90pl7YqCzWm2d3oFhW6y+1L5LODMNH1GmictP0WSUvldEfF+RPwRWAUcXWLcZmZWJqUmlD5FyYE0vdeOVpLUU1IDsA5YALwMvBURzalKEzAoTQ8CXk2f3wy8DexXXN7GOsXbmiypXlL9+vXrS/yxzMwsK6UmlP+QNKplRtJRwHs7Wiki/hIRI4DBFFoVn+5UlCWIiNsioi4i6mpqavLajJmZtaNXifW+Cdwr6c+AgL8CvlrqRiLiLUmPAccCAyT1Sq2QwcDaVG0the+5NEnqBfQHNhSVtyhex8zMuohSv9i4iELr4hLg68DhEbG4o3Uk1UgakKb3BE6lMOz4MeDLqdpE4ME0PS/Nk5b/JiIilZ+bRoENAYYCC0v78czMrFxKbaEAjAZq0zqjJBERszuoPxCYlUZk9QDuiYhfSHoBuEvSdcCzwB2p/h3ATyWtAjZSGNlFRDwv6R7gBaAZmBoRf9mJuM3MrAxKSiiSfgocAjQALSfzANpNKBGxFBjZRvkrtDFKKyK2AOe081nXA9eXEquZmVVGqS2UOuCI1AVlZma2nVJHeS2ncCHezMysTaW2UPYHXpC0kMI34AGIiAm5RGVmZlWn1IQyPc8gzMys+pWUUCLiCUkHAUMj4pF0Hy/fT8vMzFqVevv6iyncX+snqWgQMDevoMzMrPqUelF+KvA54B1ofdjWJ/IKyszMqk+pCeX9iPigZSbdGsVDiM3MrFWpCeUJSf8I7JmeJX8v8FB+YZmZWbUpNaFcAawHlgFfAx6mnSc1mplZ91TqKK+PgNvTy8zMbDul3svrj7RxzSQiDs48IjMzq0o7cy+vFn0o3MRx3+zDMTOzalXq81A2FL3WRsQtwOdzjs3MzKpIqV1eo4pme1BosezMs1TMzGw3V2pS+Kei6WZgNfCVzKMxM7OqVeoor5PyDsTMzKpbqV1e/6Oj5RFxUzbhmJlZtdqZUV6jgXlp/ovAQqAxj6DMzKz6lJpQBgOjImITgKTpwC8j4m/zCszMzKpLqbdeOQD4oGj+g1RmZmYGlN5CmQ0slPRAmj8TmJVPSGZmVo1KHeV1vaRfAcenokkR8Wx+YZmZWbUptcsLYC/gnYj4Z6BJ0pCcYjIzsypU6iOArwYuB6alot7Az/IKyszMqk+pLZSzgAnAfwBExJ+BfnkFZWZm1afUhPJBRATpFvaS9s4vJDMzq0alJpR7JP0EGCDpYuAR/LAtMzMrssNRXpIE3A18GngHOAy4KiIW5BybmZlVkR0mlIgISQ9HxDDAScTMzNpUapfXEkmjc43EzMyqWqnflD8G+FtJqymM9BKFxsuReQVmZmbVpcMWiqRPpslxwMHAyRTuNPyF9N7RugdKekzSC5Kel3RZKt9X0gJJjel9n1QuST+QtErS0uKnREqamOo3SprY+R/XzMzysqMur7kAEbEGuCki1hS/drBuM/A/I+IIYAwwVdIRwBXAoxExFHg0zQOcBgxNr8nAj6GQgICrKbSSjgaubklCZmbWdewooaho+uCd+eCIeC0ilqTpTcAKYBBwBv95Y8lZFG40SSqfHQVPUxiiPJBC62hBRGyMiDcpDAwYvzOxmJlZ/naUUKKd6Z0iqRYYCTwDHBARr6VFr/Oft8EfBLxatFpTKmuvfNttTJZUL6l+/fr1nQ3VzMw6aUcJZbikdyRtAo5M0+9I2iTpnVI2IKkv8G/ANyNiq3WKv32/qyLitoioi4i6mpqaLD7SzMx2QoejvCKi5658uKTeFJLJnIi4PxW/IWlgRLyWurTWpfK1wIFFqw9OZWuBE7cpf3xX4jIzs+ztzO3rd0r6hv0dwIqIuKlo0TygZaTWRODBovIL0mivMcDbqWvs18BYSfuki/FjU5mZmXUhpX4PpTM+B/wdsExSQyr7R+AGCvcGuwhYA3wlLXsYOB1YBbwLTAKIiI2SrgUWpXrXRMTGHOM2M7NOyC2hRMRTbD1KrNgpbdQPYGo7nzUDmJFddGZmlrXcurzMzKx7cUIxM7NMOKGYmVkmnFDMzCwTTihmZpYJJxQzM8uEE4qZmWXCCcXMzDLhhGJmZplwQjEzs0w4oZiZWSacUMzMLBNOKGZmlgknFDMzy4QTipmZZcIJxczMMuGEYmZmmcjzEcBmVaP2il+2Wb76hs+XORKz6uUWipmZZcIJxczMMuGEYmZmmXBCMTOzTDihmJlZJpxQzMwsE04oZmaWCScUMzPLhBOKmZllwgnFzMwy4VuvWJfR1u1PfOsTs+rhFoqZmWXCCcXMzDLhhGJmZpnILaFImiFpnaTlRWX7SlogqTG975PKJekHklZJWippVNE6E1P9RkkT84rXzMx2TZ4tlJnA+G3KrgAejYihwKNpHuA0YGh6TQZ+DIUEBFwNHAMcDVzdkoTMzKxryS2hRMSTwMZtis8AZqXpWcCZReWzo+BpYICkgcA4YEFEbIyIN4EFbJ+kzMysCyj3NZQDIuK1NP06cECaHgS8WlSvKZW1V25mZl1MxS7KR0QAkdXnSZosqV5S/fr167P6WDMzK1G5E8obqSuL9L4ula8FDiyqNziVtVe+nYi4LSLqIqKupqYm88DNzKxj5U4o84CWkVoTgQeLyi9Io73GAG+nrrFfA2Ml7ZMuxo9NZWZm1sXkdusVSXcCJwL7S2qiMFrrBuAeSRcBa4CvpOoPA6cDq4B3gUkAEbFR0rXAolTvmojY9kK/mZl1AbkllIg4r51Fp7RRN4Cp7XzODGBGhqGZmVkO/E15MzPLhBOKmZllwgnFzMwy4YRiZmaZ8AO2zHYDfjiZdQVOKNaqrZMS+MRkZqVxl5eZmWXCCcXMzDLhhGJmZplwQjEzs0w4oZiZWSY8yquL8UirzvPQWbPKcgvFzMwy4RaKmVUtt+i7FrdQzMwsE26h7GZ8HcGsdG7hZMsJpQ0+KZuZ7Tx3eZmZWSacUMzMLBPu8sqY+2TNuo9Kdo93xXONE4pZF1DNJ6aueGKzynBCMcuAT6pmTihmZhWzu40odUJpw+o+/62N0rd3Yd1Kr1+ebTO9fzvl+ce+q+tX8+9tV9ev5p+9mmPf1fV3+e81Bx7lZWZmmXBCMTOzTDihmJlZJpxQzMwsE04oZmaWCScUMzPLhBOKmZllwgnFzMwy4YRiZmaZqJqEImm8pJckrZJ0RaXjMTOzrVVFQpHUE7gVOA04AjhP0hGVjcrMzIpVRUIBjgZWRcQrEfEBcBdwRoVjMjOzIoqISsewQ5K+DIyPiP+e5v8OOCYiLi2qMxmYnGYPA17KYNP7A/+ewefkpSvH59g6ryvH59g6ryvH1xLbQRFR09kP2W3uNhwRtwG3ZfmZkuojoi7Lz8xSV47PsXVeV47PsXVeV44vq9iqpctrLXBg0fzgVGZmZl1EtSSURcBQSUMkfQw4F5hX4ZjMzKxIVXR5RUSzpEuBXwM9gRkR8XwZNp1pF1oOunJ8jq3zunJ8jq3zunJ8mcRWFRflzcys66uWLi8zM+vinFDMzCwTTijs+LYukvaQdHda/oyk2jLFdaCkxyS9IOl5SZe1UedESW9Lakivq8oRW9H2V0talrZd38ZySfpB2ndLJY0qU1yHFe2TBknvSPrmNnXKuu8kzZC0TtLyorJ9JS2Q1Jje92ln3YmpTqOkiWWK7fuSXky/twckDWhn3Q6PgZximy5pbdHv7vR21s31lk3txHZ3UVyrJTW0s26u+y1to81zSG7HXUR06xeFi/wvAwcDHwOeA47Yps4U4P+k6XOBu8sU20BgVJruB6xsI7YTgV9UcP+tBvbvYPnpwK8AAWOAZyr0O36dwpe2KrbvgBOAUcDyorL/DVyRpq8AvtfGevsCr6T3fdL0PmWIbSzQK01/r63YSjkGcoptOvCtEn7vHf5t5xHbNsv/CbiqEvstbaPNc0hex51bKKXd1uUMYFaavg84RZLyDiwiXouIJWl6E7ACGJT3djN2BjA7Cp4GBkgaWOYYTgFejog1Zd7uViLiSWDjNsXFx9Ys4Mw2Vh0HLIiIjRHxJrAAGJ93bBExPyKa0+zTFL7/VXbt7LdS5H7Lpo5iS+eIrwB3ZrnNndHBOSSX484JpbBzXy2ab2L7k3ZrnfQH9jawX1miS1I320jgmTYWHyvpOUm/kvSZcsYFBDBf0uJ0+5ttlbJ/83Yu7f9RV3LfARwQEa+l6deBA9qo0xX24d9TaGm2ZUfHQF4uTd1xM9rpsqn0fjseeCMiGttZXtb9ts05JJfjzgmlCkjqC/wb8M2IeGebxUsodOUMB34IzC1zeMdFxCgKd4KeKumEMm+/Qyp8EXYCcG8biyu977YShX6GLjeOX9KVQDMwp50qlTgGfgwcAowAXqPQtdTVnEfHrZOy7beOziFZHndOKKXd1qW1jqReQH9gQzmCk9SbwoEwJyLu33Z5RLwTEZvT9MNAb0n7lyO2tM216X0d8ACFboZilb5tzmnAkoh4Y9sFld53yRstXYDpfV0bdSq2DyVdCHwBOD+deLZTwjGQuYh4IyL+EhEfAbe3s81K7rdewNnA3e3VKdd+a+cckstx54RS2m1d5gEtIxy+DPymvT+uLKU+2DuAFRFxUzt1/qrleo6koyn8TsuV7PaW1K9lmsJF3OXbVJsHXKCCMcDbRU3tcmj3v8RK7rsixcfWRODBNur8GhgraZ/UtTM2leVK0njgH4AJEfFuO3VKOQbyiK34OtxZ7Wyzkrds+hvgxYhoamthufZbB+eQfI67PEcYVMuLwkiklRRGhFyZyq6h8IcE0IdCl8kqYCFwcJniOo5CU3Qp0JBepwNfB76e6lwKPE9hBMvTwH8t4347OG33uRRDy74rjk8UHo72MrAMqCtjfHtTSBD9i8oqtu8oJLbXgA8p9EdfROFa3KNAI/AIsG+qWwf8a9G6f5+Ov1XApDLFtopCH3rLsdcy0vG/AA93dAyUIbafpuNpKYWT48BtY0vz2/1t5x1bKp/ZcpwV1S3rfkvbae8ckstx51uvmJlZJtzlZWZmmXBCMTOzTDihmJlZJpxQzMwsE04oZmaWCScUMzPLhBOKmZll4v8DYYplxUSESrEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXwxrNdCShlc",
        "colab_type": "text"
      },
      "source": [
        "# Model feature extractor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etwOZM2GSjL_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def feature_extraction(cut_layer, dense, dropouts, im_size, loss, optimizer, metrics, verbose=False):\n",
        "  '''\n",
        "    viene definito il layer di cut della vgg e vengono aggiunti una serie di layer densi prima dell'output\n",
        "  '''\n",
        "  \n",
        "  base_model = VGG16(weights = 'imagenet', input_shape=im_size + (3,), include_top = False)\n",
        "\n",
        "  x = base_model.layers[cut_layer].output\n",
        "  x = GlobalAveragePooling2D()(x)\n",
        "  for layer,dropout in zip(dense,dropouts):\n",
        "    x = Dense(layer, activation='relu')(x)\n",
        "    x = Dropout(dropout)(x)\n",
        "  predictions = Dense(20, activation = 'softmax')(x)\n",
        "  model = Model(input = base_model.input, output = predictions)\n",
        "\n",
        "  for layer in base_model.layers:\n",
        "    layer.trainable=False\n",
        "    \n",
        "  if verbose:\n",
        "    model.summary()\n",
        "\n",
        "  model.compile(loss=loss, \n",
        "                optimizer=optimizer, \n",
        "                metrics=metrics\n",
        "  )\n",
        "      \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXNhkv10FLgG",
        "colab_type": "text"
      },
      "source": [
        "# Model fine tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8EhFgVcFTa7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fine_tuning(freeze_to, dense, dropouts, im_size, loss, optimizer, metrics, verbose=False):\n",
        "  '''\n",
        "    vengono definiti i layer della vgg trainabili e i dense layer prima dell'output\n",
        "  '''\n",
        "  \n",
        "  base_model = VGG16(weights = 'imagenet', input_shape=im_size + (3,), include_top = False)\n",
        "\n",
        "  x = base_model.layers[-1].output\n",
        "  x = GlobalAveragePooling2D()(x) \n",
        "\n",
        "  for layer,dropout in zip(dense,dropouts): \n",
        "    x = Dense(layer, activation='relu')(x)\n",
        "    x = Dropout(dropout)(x)\n",
        "  predictions = Dense(20, activation = 'softmax')(x)\n",
        "  model = Model(input = base_model.input, output = predictions)\n",
        "\n",
        "  for layer in base_model.layers[:freeze_to]:\n",
        "    layer.trainable=False\n",
        "  for layer in model.layers[freeze_to:]: # layer su cui applicare fine tuning\n",
        "    layer.trainable = True\n",
        "\n",
        "  if verbose:\n",
        "    model.summary()\n",
        "\n",
        "  model.compile(loss=loss, \n",
        "                optimizer=optimizer, \n",
        "                metrics=metrics\n",
        "  )\n",
        "      \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7fUcTZApdLB",
        "colab_type": "text"
      },
      "source": [
        "# Plot performance epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXAGRUyipg-R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_performance(history):\n",
        "  plt.plot(history.history['categorical_accuracy'])\n",
        "  plt.plot(history.history['val_categorical_accuracy'])\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'val'], loc='upper left')\n",
        "  plt.show()\n",
        "  # summarize history for loss\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'val'], loc='upper left')\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gpMHcy2SpB7",
        "colab_type": "text"
      },
      "source": [
        "# Util to load train/val pickle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSiSo2KHSzmo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_pickle(path_file, one_hot=True):\n",
        "\n",
        "  file = open(path_file, 'rb')\n",
        "  data = pickle.load(file)\n",
        "  file.close()\n",
        "  images = data[0]\n",
        "  labels = data[1]\n",
        "\n",
        "  if one_hot:\n",
        "    enc = OneHotEncoder(handle_unknown='ignore')\n",
        "    enc.fit(labels.reshape(-1,1))\n",
        "    labels_one_hot = enc.transform(labels.reshape(-1,1)).toarray()\n",
        "\n",
        "  return images, labels_one_hot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22ReLsjTEeCZ",
        "colab_type": "text"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "se7bKuAJbulU",
        "colab_type": "text"
      },
      "source": [
        "## Set up generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDNTdRNpGBpI",
        "colab_type": "code",
        "outputId": "408ec0a0-e309-4300-f132-64d5ce4045e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#base_net = 'vgg16'\n",
        "\n",
        "augment_params = dict(  rotation_range=30,\n",
        "                        horizontal_flip=True,\n",
        "                        zoom_range=0.1\n",
        "                      )\n",
        "\n",
        "train_gen, val_gen, test_gen = create_generator(batch_size=BATCH_SIZE, \n",
        "                                                im_size=IM_SIZE, \n",
        "                                                train_directory=PATH_IMAGES_CROPPED_TRAIN_BALANCED, \n",
        "                                                val_directory=PATH_IMAGES_CROPPED_VAL_TEST_BALANCED, \n",
        "                                                labels=LABELS,\n",
        "                                                validation_split=0.7,\n",
        "                                                aug=True,\n",
        "                                                augment_params=augment_params\n",
        "                                                )\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 5680 images belonging to 20 classes.\n",
            "Found 2341 images belonging to 20 classes.\n",
            "Found 5432 images belonging to 20 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oR4bO3SF_-EN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x,y = train_gen.next()\n",
        "for i in range(0,10):\n",
        "    image = x[i]\n",
        "    plt.figure()\n",
        "    plt.imshow(image)\n",
        "    plt.title(from_onehot_to_label(mapping, y[i]))\n",
        "    plt.show\n",
        "\n",
        "train_gen.reset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNm8VPigbjay",
        "colab_type": "text"
      },
      "source": [
        "## Analyze frequency label in generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7QYMSTWbaof",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels, freq = get_frequency_from_generator(train_gen)\n",
        "plot_label_frequency(labels, freq)\n",
        "freq_normalize, weigh = normalize_frequency(freq)\n",
        "#plot_label_frequency(labels, freq_normalize)\n",
        "plot_stacked_bar_freq(labels, freq, freq_normalize)\n",
        "'''\n",
        "# dict for weigh model\n",
        "weigh_class = {}\n",
        "for label,i in zip(labels, weigh):\n",
        "  weigh_class[from_label_to_number(mapping, label)] = i\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqmfekWbn-jf",
        "colab_type": "code",
        "outputId": "2b9c4842-ba68-44a6-f120-148988d54c32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "weigh_class"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 0.674468085106383,\n",
              " 1: 0.7731707317073171,\n",
              " 2: 0.535472972972973,\n",
              " 3: 0.6240157480314961,\n",
              " 4: 0.4232309746328438,\n",
              " 5: 1.0,\n",
              " 6: 0.2661628883291352,\n",
              " 7: 0.5205254515599343,\n",
              " 8: 0.2175703500343171,\n",
              " 9: 0.8929577464788733,\n",
              " 10: 0.8498659517426274,\n",
              " 11: 0.4127604166666667,\n",
              " 12: 0.8408488063660478,\n",
              " 13: 0.8453333333333334,\n",
              " 14: 0.06315999203028491,\n",
              " 15: 0.5691202872531418,\n",
              " 16: 0.6227897838899804,\n",
              " 17: 0.7944862155388471,\n",
              " 18: 0.9694189602446484,\n",
              " 19: 0.7694174757281553}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAg57-_ib21t",
        "colab_type": "text"
      },
      "source": [
        "## Set up iper-parameter for fit and model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QSFiR2Ab6v1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss = 'categorical_crossentropy'\n",
        "optimizer = 'adam'\n",
        "reduce_on_plateau = ReduceLROnPlateau(monitor='val_loss', factor=0.01, patience=3, \n",
        "                                      verbose=1, mode='auto', min_delta=0.0001, \n",
        "                                      cooldown=0, min_lr=0.000001)\n",
        "early_stop = EarlyStopping(monitor='val_loss', \n",
        "                           patience=5, \n",
        "                           verbose=1,\n",
        "                           restore_best_weights=True\n",
        "                           )\n",
        "model_checkpoint = ModelCheckpoint(filepath='best_weights.h5', \n",
        "                                   monitor='val_loss', \n",
        "                                   save_best_only=True)\n",
        "metrics = ['categorical_accuracy']\n",
        "epochs = 50\n",
        "steps_per_epoch = ceil(train_gen.n / train_gen.batch_size)\n",
        "validation_steps = ceil(val_gen.n / val_gen.batch_size)\n",
        "test_steps = ceil(test_gen.n / test_gen.batch_size)\n",
        "dense = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pz6Y1ic80Rz2",
        "colab_type": "text"
      },
      "source": [
        "## Create model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQ2obP6f0UDz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model feauture extraction\n",
        "model = feature_extraction(cut_layer=-1,\n",
        "                           dense=[512, 256, 128, 64],\n",
        "                           dropouts=[0.2,0.2,0.2,0.2],\n",
        "                           im_size=IM_SIZE,\n",
        "                           loss='categorical_crossentropy',\n",
        "                           optimizer=optimizer,\n",
        "                           metrics=metrics,\n",
        "                           verbose=False\n",
        "                           )\n",
        "# TODO model fine tuning\n",
        "'''\n",
        "model = fine_tuning(freeze_to=15,\n",
        "                    dense=[128, 64],\n",
        "                    dropouts=[0.2,0.2],\n",
        "                    im_size=IM_SIZE,\n",
        "                    loss='categorical_crossentropy,\n",
        "                    optimizer=optimizer,\n",
        "                    metrics=metrics,\n",
        "                    verbose=False)\n",
        "'''\n",
        "for index,layer in enumerate(model.layers):\n",
        "  print(index, ' ', layer.name, ' ', layer.trainable)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8U2BptkC0JcN",
        "colab_type": "text"
      },
      "source": [
        "## Fit prova"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9HmoEWOUkkU",
        "colab_type": "code",
        "outputId": "d605aac0-b9f7-412f-cb71-0fc4b33c1d9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "net_history = model.fit_generator(train_gen, epochs=2, verbose=1,\n",
        "                                  validation_data = val_gen,\n",
        "                                  steps_per_epoch = steps_per_epoch,\n",
        "                                  validation_steps = validation_steps,\n",
        "                                  callbacks = [early_stop, reduce_on_plateau]                 \n",
        "                                  )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "89/89 [==============================] - 3891s 44s/step - loss: 2.7898 - categorical_accuracy: 0.2632 - val_loss: 0.6715 - val_categorical_accuracy: 0.6886\n",
            "Epoch 2/2\n",
            "89/89 [==============================] - 27s 300ms/step - loss: 1.5000 - categorical_accuracy: 0.5521 - val_loss: 0.2958 - val_categorical_accuracy: 0.7715\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqNkpcddZLB0",
        "colab_type": "code",
        "outputId": "ef00aed0-cd17-437d-be1b-dfb09fb1c277",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        }
      },
      "source": [
        "plot_performance(net_history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GnbuGQ72sMR",
        "colab_type": "code",
        "outputId": "a7ae14e1-2466-42e7-cd9b-85c2e3669582",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "loss, accuracy = model.evaluate(test_gen, steps=test_steps)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "37/37 [==============================] - 4s 115ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3njsQpf7MTvn",
        "colab_type": "code",
        "outputId": "043bff4b-5d67-48b1-9efa-1d3849dfd561",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(f'Loss: {loss}')\n",
        "print(f'Accuracy: {accuracy}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 0.295815646648407\n",
            "Accuracy: 0.7714651823043823\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwte93dQmMzJ",
        "colab_type": "text"
      },
      "source": [
        "# AutoML"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrlxqfdszCbf",
        "colab_type": "text"
      },
      "source": [
        "## Create dataframe for k-cross-fold optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9T3GwCMTMVtZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dataframe_path_class(path_images):\n",
        "  df = pd.DataFrame(columns=['absolute_path', 'class'])\n",
        "  list_dir = os.listdir(path_images)\n",
        "  for sub_dir in list_dir:\n",
        "    list_images = os.listdir(os.path.join(path_images, sub_dir))  \n",
        "    for image in list_images:\n",
        "      df = df.append({'absolute_path': os.path.join(path_images, sub_dir, image),\n",
        "                      'class' : str(sub_dir)} , \n",
        "                     ignore_index=True)\n",
        "  return df\n",
        "\n",
        "df_train = dataframe_path_class(PATH_IMAGES_CROPPED_TRAIN_BALANCED)\n",
        "df_val_test = dataframe_path_class(PATH_IMAGES_CROPPED_VAL_TEST_BALANCED)\n",
        "\n",
        "# creo df_test che uso solo dopo ottimizzazione iperparametri\n",
        "df_val, df_test = train_test_split(df_val_test, test_size=0.7)\n",
        "\n",
        "# unisco df_val a df_train per avere un train generale su cui eseguire k-cross-fold validation\n",
        "df_train = df_train.append(df_val, ignore_index=True)\n",
        "\n",
        "# salvo i due dataframe\n",
        "df_train.to_csv(PATH_OPTIMIZATION + 'train_k_cross.csv')\n",
        "df_test.to_csv(PATH_OPTIMIZATION + 'test.csv')\n",
        "\n",
        "print(df_train.shape, df_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KaAO8omzV73",
        "colab_type": "text"
      },
      "source": [
        "## k-cross-fold to optimize "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YRPDR-AKNLB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cross_validation_optimization(cfg):\n",
        "\n",
        "  kf = KFold(n_splits=3, random_state=None, shuffle=True) \n",
        "\n",
        "  #accuracy_on_fold = []\n",
        "  losses = []\n",
        "\n",
        "  i = 1\n",
        "  for train_index, val_index in kf.split(df_train):\n",
        "\n",
        "    #gc.collect()\n",
        "    K.clear_session()\n",
        "\n",
        "    # creo modello sulla base delle configurazioni in cfg\n",
        "    if cfg['number_layer'] == 1:\n",
        "      dense = [cfg['number_first_layer']]\n",
        "      dropouts = [cfg['dropout_first_layer']]\n",
        "    elif cfg['number_layer'] == 2:\n",
        "      dense = [cfg['number_first_layer'], \n",
        "               cfg['number_second_layer']]\n",
        "      dropouts = [cfg['dropout_first_layer'],\n",
        "                 cfg['dropout_second_layer']]\n",
        "    elif cfg['number_layer'] == 3:\n",
        "      dense = [cfg['number_first_layer'], \n",
        "               cfg['number_second_layer'], \n",
        "               cfg['number_third_layer']]\n",
        "      dropouts = [cfg['dropout_first_layer'],\n",
        "                 cfg['dropout_second_layer'],\n",
        "                 cfg['dropout_third_layer']]\n",
        "    elif cfg['number_layer'] == 4:\n",
        "      dense = [cfg['number_first_layer'], \n",
        "               cfg['number_second_layer'], \n",
        "               cfg['number_third_layer'], \n",
        "               cfg['number_fourth_layer']]\n",
        "      dropouts = [cfg['dropout_first_layer'],\n",
        "                 cfg['dropout_second_layer'],\n",
        "                 cfg['dropout_third_layer'],\n",
        "                 cfg['dropout_third_layer']]\n",
        "    '''\n",
        "    if cfg['optimizier'] == 'SGD':\n",
        "      optimizer = SGD(learning_rate=cfg['learning_rate'], momentum=cfg['momentum'])\n",
        "      if cfg['nestorov'] == True:\n",
        "        optimizer.nestorov = True\n",
        "    elif cfg['optimizier'] == 'adam':\n",
        "      optimizer = Adam(learning_rate=cfg['learning_rate'])\n",
        "    '''\n",
        "    optimizer = Adam(learning_rate=cfg['learning_rate'])\n",
        "    print('create generator')\n",
        "    trainGenerator = ImageDataGenerator(preprocessing_function=preprocess_input_vgg16,\n",
        "                                        rotation_range=30,\n",
        "                                        horizontal_flip=True,\n",
        "                                        zoom_range=0.1\n",
        "                                        )\n",
        "    valGenerator = ImageDataGenerator(preprocessing_function=preprocess_input_vgg16)\n",
        "    \n",
        "    print('create train val and test')\n",
        "    trainData = df_train.iloc[train_index,:]\n",
        "    trainData, valData = train_test_split(trainData, test_size=0.2)\n",
        "    testData = df_train.iloc[val_index,:]\n",
        "\n",
        "    print(\"=========================================\")\n",
        "    print(\"====== K Fold Validation step => %d =======\" % (i))\n",
        "    print(\"=========================================\")\n",
        "\n",
        "    print('flow from train dataframe')\n",
        "    train_gen = valGenerator.flow_from_dataframe(\n",
        "      dataframe = trainData,\n",
        "      x_col='absolute_path',\n",
        "      y_col='class',\n",
        "      shuffle=True,\n",
        "      class_mode='categorical',\n",
        "      target_size=IM_SIZE,\n",
        "      batch_size=BATCH_SIZE,\n",
        "      classes=LABELS,\n",
        "    )\n",
        "\n",
        "    print('flow from val dataframe')\n",
        "    val_gen = valGenerator.flow_from_dataframe(\n",
        "      dataframe = valData,\n",
        "      x_col='absolute_path',\n",
        "      y_col='class',\n",
        "      shuffle=False,\n",
        "      class_mode='categorical',\n",
        "      target_size=IM_SIZE,\n",
        "      batch_size=BATCH_SIZE,\n",
        "      classes=LABELS,\n",
        "    )\n",
        "\n",
        "    print('flow from test dataframe')\n",
        "    test_gen = valGenerator.flow_from_dataframe(\n",
        "      dataframe = testData,\n",
        "      x_col='absolute_path',\n",
        "      y_col='class',\n",
        "      shuffle=False,\n",
        "      class_mode='categorical',\n",
        "      target_size=IM_SIZE,\n",
        "      batch_size=BATCH_SIZE,\n",
        "      classes=LABELS,\n",
        "    ) \n",
        "    \n",
        "    step_train = ceil(train_gen.n/train_gen.batch_size)\n",
        "    step_val = ceil(val_gen.n/val_gen.batch_size)\n",
        "    step_test = ceil(test_gen.n/test_gen.batch_size)\n",
        "\n",
        "    reduce_on_plateau = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, \n",
        "                                          verbose=1, mode='auto', min_delta=0.0001, \n",
        "                                          cooldown=0, min_lr=0.00001)\n",
        "    early_stop = EarlyStopping(monitor='val_loss', \n",
        "                              patience=5, \n",
        "                              verbose=1,\n",
        "                              restore_best_weights=True\n",
        "                              )\n",
        "\n",
        "    if to_optimize == 'feature_extraction':\n",
        "      print('feature extraction')\n",
        "      model = feature_extraction(cut_layer=cfg['cut_layer'],\n",
        "                                dense=dense,\n",
        "                                dropouts=dropouts,\n",
        "                                im_size=IM_SIZE,\n",
        "                                loss='categorical_crossentropy',\n",
        "                                optimizer=optimizer,\n",
        "                                metrics=['categorical_accuracy'],\n",
        "                                verbose=False\n",
        "                                )\n",
        "    if to_optimize == 'fine_tuning':\n",
        "      model = fine_tuning(freeze_to=cfg['freeze_to'],\n",
        "                                        dense=dense,\n",
        "                                        dropouts=dropouts,\n",
        "                                        im_size=IM_SIZE,\n",
        "                                        loss='categorical_crossentropy',\n",
        "                                        optimizer=optimizer,\n",
        "                                        metrics=['categorical_accuracy'],\n",
        "                                        verbose=False) \n",
        "    \n",
        "    print(\"Inizio fit\")\n",
        "    \n",
        "    history = model.fit_generator(generator=train_gen, \n",
        "                                  epochs=50, \n",
        "                                  verbose=1,\n",
        "                                  validation_data = val_gen,\n",
        "                                  steps_per_epoch = step_train,\n",
        "                                  validation_steps = step_val,\n",
        "                                  callbacks = [early_stop, reduce_on_plateau],\n",
        "                                  )\n",
        "\n",
        "\n",
        "    # evaluate test and return loss\n",
        "    loss, _ = model.evaluate_generator(test_gen, steps=step_test)\n",
        "    print('loss fold ', i, loss)\n",
        "    losses.append(loss)\n",
        "    i+=1\n",
        "  print('loss media configurazione: ', np.mean(losses))\n",
        "  return np.mean(losses) # minimizzo loss media sulla 3-cross validation\n",
        "  #return 1 - np.mean(accuracy_on_fold)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJrfI4v50nnV",
        "colab_type": "text"
      },
      "source": [
        "## Configuration space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgQ0AAgxmP7i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_configuration_space(to_optimize):\n",
        "\n",
        "  cs = ConfigurationSpace()\n",
        "\n",
        "  # iper-parametri da ottimizzare\n",
        "  '''\n",
        "    Optimizier: categorical ('adam', 'SGD')\n",
        "    Learning_rate: uniform_float [0.01, 0.00001]\n",
        "    Momentum: categorical and condition (True,False) if 'SGD'\n",
        "    Nestorov: uniform_float and condition [0.9, 0.8] if 'SGD'\n",
        "    Dropout: uniform_float [0.2,0.4]\n",
        "    Number_fully_connected: categorical (1,2)\n",
        "    Number_neurons_first_layer: uniform_integer [64, 1024]\n",
        "    Numebr_neurons_second_layer: uniform_integer and condition [64,1024] if number_fully_connected==2\n",
        "  '''\n",
        "  '''\n",
        "  optimizier_param = CategoricalHyperparameter(name='optimizier',\n",
        "                                              choices=['adam', 'SGD'],\n",
        "                                              default_value='SGD')\n",
        "  '''\n",
        "  lr_param = UniformFloatHyperparameter(name=\"learning_rate\", \n",
        "                                        lower=0.000001, \n",
        "                                        upper=0.001, \n",
        "                                        default_value=0.001)\n",
        "  '''\n",
        "  momentum_param = UniformFloatHyperparameter(name='momentum',\n",
        "                                              lower=0.1,\n",
        "                                              upper=0.9,\n",
        "                                              default_value=0.5\n",
        "                                              )\n",
        "  cond_momentum = InCondition(child=momentum_param, \n",
        "                              parent=optimizier_param, \n",
        "                              values=['SGD'])\n",
        "\n",
        "  nestorov_param = CategoricalHyperparameter(name=\"nestorov\", \n",
        "                                            choices=[True, False], \n",
        "                                            default_value=False)\n",
        "  cond_nestorov = InCondition(child=nestorov_param, \n",
        "                              parent=optimizier_param, \n",
        "                              values=['SGD'])\n",
        "  '''\n",
        "  \n",
        "  number_layer_param = UniformIntegerHyperparameter(name='number_layer',\n",
        "                                          lower=1,\n",
        "                                          upper=4,\n",
        "                                          default_value=2)\n",
        "  \n",
        "  dropout_first_layer = UniformFloatHyperparameter(name='dropout_first_layer',\n",
        "                                            lower=0.2,\n",
        "                                            upper=0.4,\n",
        "                                            default_value=0.2)\n",
        "  dropout_second_layer = UniformFloatHyperparameter(name='dropout_second_layer',\n",
        "                                            lower=0.2,\n",
        "                                            upper=0.4,\n",
        "                                            default_value=0.2)\n",
        "  dropout_third_layer = UniformFloatHyperparameter(name='dropout_third_layer',\n",
        "                                            lower=0.2,\n",
        "                                            upper=0.4,\n",
        "                                            default_value=0.2)\n",
        "  dropout_fourth_layer = UniformFloatHyperparameter(name='dropout_fourth_layer',\n",
        "                                            lower=0.2,\n",
        "                                            upper=0.4,\n",
        "                                            default_value=0.2)\n",
        "\n",
        "  neurons_first_layer_param = UniformIntegerHyperparameter(name='number_first_layer',\n",
        "                                                    lower=64,\n",
        "                                                    upper=1024,\n",
        "                                                    default_value=512)\n",
        "\n",
        "  neurons_second_layer_param = UniformIntegerHyperparameter(name='number_second_layer',\n",
        "                                                    lower=64,\n",
        "                                                    upper=1024,\n",
        "                                                    default_value=512)\n",
        "  neurons_third_layer_param = UniformIntegerHyperparameter(name='number_third_layer',\n",
        "                                                  lower=64,\n",
        "                                                  upper=1024,\n",
        "                                                  default_value=512)\n",
        "\n",
        "  neurons_fourth_layer_param = UniformIntegerHyperparameter(name='number_fourth_layer',\n",
        "                                                    lower=64,\n",
        "                                                    upper=1024,\n",
        "                                                    default_value=512)\n",
        "  cond_first_layer = InCondition(child=neurons_first_layer_param,\n",
        "                                parent=number_layer_param,\n",
        "                                values=[1,2,3,4])\n",
        "  cond_second_layer = InCondition(child=neurons_second_layer_param,\n",
        "                                  parent=number_layer_param,\n",
        "                                  values=[2,3,4])\n",
        "  cond_third_layer = InCondition(child=neurons_third_layer_param,\n",
        "                                  parent=number_layer_param,\n",
        "                                  values=[3,4])\n",
        "  cond_fourth_layer = InCondition(child=neurons_fourth_layer_param,\n",
        "                                  parent=number_layer_param,\n",
        "                                  values=[4])\n",
        "  \n",
        "  cond__dropout_first_layer = InCondition(child=dropout_first_layer,\n",
        "                                parent=number_layer_param,\n",
        "                                values=[1,2,3,4])\n",
        "  cond_dropout_second_layer = InCondition(child=dropout_second_layer,\n",
        "                                  parent=number_layer_param,\n",
        "                                  values=[2,3,4])\n",
        "  cond_dropout_third_layer = InCondition(child=dropout_third_layer,\n",
        "                                  parent=number_layer_param,\n",
        "                                  values=[3,4])\n",
        "  cond_dropout_fourth_layer = InCondition(child=dropout_fourth_layer,\n",
        "                                  parent=number_layer_param,\n",
        "                                  values=[4])\n",
        "  \n",
        "  if to_optimize == 'feature_extraction':\n",
        "    cut_layer = CategoricalHyperparameter(name='cut_layer',\n",
        "                                          choices=[6,10,14,18],\n",
        "                                          default_value=14)\n",
        "    cs.add_hyperparameters([cut_layer])\n",
        "  if to_optimize == 'fine_tuning':\n",
        "    freeze_to = UniformIntegerHyperparameter(name='freeze_to',\n",
        "                                             lower=7,\n",
        "                                             upper=19,\n",
        "                                             default_value=15)\n",
        "    cs.add_hyperparameters([freeze_to])\n",
        "  cs.add_hyperparameters([#optimizier_param, \n",
        "                          lr_param,\n",
        "                          #momentum_param,\n",
        "                          #nestorov_param,\n",
        "                          number_layer_param,\n",
        "                          neurons_first_layer_param,\n",
        "                          neurons_second_layer_param,\n",
        "                          neurons_third_layer_param,\n",
        "                          neurons_fourth_layer_param,\n",
        "                          dropout_first_layer,\n",
        "                          dropout_second_layer,\n",
        "                          dropout_third_layer,\n",
        "                          dropout_fourth_layer\n",
        "                          ])\n",
        "  cs.add_conditions([#cond_momentum,\n",
        "                    #cond_nestorov,\n",
        "                    cond_first_layer,\n",
        "                    cond_second_layer,\n",
        "                    cond_third_layer,\n",
        "                    cond_fourth_layer,\n",
        "                    cond__dropout_first_layer,\n",
        "                    cond_dropout_second_layer,\n",
        "                    cond_dropout_third_layer,\n",
        "                    cond_dropout_fourth_layer\n",
        "                    ])\n",
        "\n",
        "  return cs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTVlUukw3Fcl",
        "colab_type": "text"
      },
      "source": [
        "## Scenario"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gIlpKtp984l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#to_optimize = 'feature_extraction'\n",
        "to_optimize = 'fine_tuning'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-YMAPtn3HM0",
        "colab_type": "code",
        "outputId": "c4942712-16f7-4a1c-af4f-7640f098ca4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cs = create_configuration_space(to_optimize)\n",
        "scenario = Scenario({\"run_obj\": \"quality\",  \n",
        "                     \"runcount-limit\": 20, # 15 + 5 configurazioni iniziali   \n",
        "                     \"cs\": cs,               \n",
        "                     \"deterministic\": \"true\"\n",
        "                     })\n",
        "# All statistics collected during configuration run. Written to output-directory to be restored\n",
        "stat = Stats(scenario=scenario)\n",
        "\n",
        "\n",
        "traj_logger  = TrajLogger(output_dir=PATH_OPTIMIZATION + 'traj_logger/',\n",
        "                          stats=stat)\n",
        "\n",
        "run_history = RunHistory()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:smac.utils.io.cmd_reader.CMDReader:Output to smac3-output_2020-06-15_14:24:27_072073\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_91uOB4kAOqz",
        "colab_type": "text"
      },
      "source": [
        "## Optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfqB26ESAQP5",
        "colab_type": "code",
        "outputId": "b0ced60b-3e3a-4b9e-efec-26e7693f7344",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "rng = np.random.RandomState(seed=42)\n",
        "\n",
        "df_train = pd.read_csv(PATH_OPTIMIZATION + 'train_k_cross.csv')\n",
        "\n",
        "smac = SMAC4HPO(scenario=scenario,\n",
        "                rng=rng,\n",
        "                runhistory=run_history,\n",
        "                tae_runner=cross_validation_optimization,\n",
        "                initial_design=RandomConfigurations, # LHDesign\n",
        "                initial_design_kwargs={'n_configs_x_params': 5},\n",
        "                acquisition_function=EI\n",
        "                )\n",
        "\n",
        "incumbent = smac.optimize()\n",
        "\n",
        "# save optim value and history confinguration tried\n",
        "if to_optimize == 'feature_extraction':\n",
        "  with open(PATH_OPTIMIZATION + 'incubent_feature_extraction.pkl', 'wb') as f:  \n",
        "      pickle.dump(incumbent, f)\n",
        "  with open(PATH_OPTIMIZATION + 'history_feature_extraction.pkl', 'wb') as f:  \n",
        "      pickle.dump(run_history, f)\n",
        "if to_optimize == 'fine_tuning':\n",
        "  with open(PATH_OPTIMIZATION + 'incumbent_fine_tuning.pkl', 'wb') as f:  \n",
        "      pickle.dump(incumbent, f)\n",
        "  with open(PATH_OPTIMIZATION + 'history_fine_tuning.pkl', 'wb') as f:  \n",
        "      pickle.dump(run_history, f)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:smac.facade.smac_hpo_facade.SMAC4HPO:Optimizing a deterministic scenario for quality without a tuner timeout - will make SMAC deterministic and only evaluate one configuration per iteration!\n",
            "ERROR:smac.utils.io.output_writer.OutputWriter:Could not write pcs file to disk. ConfigSpace not compatible with (new) pcs format.\n",
            "INFO:smac.initial_design.random_configuration_design.RandomConfigurations:Running initial design for 5 configurations\n",
            "INFO:smac.facade.smac_hpo_facade.SMAC4HPO:<class 'smac.facade.smac_hpo_facade.SMAC4HPO'>\n",
            "INFO:smac.optimizer.smbo.SMBO:Running initial design\n",
            "INFO:smac.intensification.intensification.Intensifier:First run, no incumbent provided; challenger is assumed to be the incumbent\n",
            "INFO:smac.intensification.intensification.Intensifier:Updated estimated cost of incumbent on 1 runs: 2147483647.0000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 1 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 4272 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 1068 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 2671 validated image filenames belonging to 20 classes.\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 11s 158ms/step - loss: 2.5583 - categorical_accuracy: 0.3493 - val_loss: 1.5246 - val_categorical_accuracy: 0.4878\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 8s 116ms/step - loss: 1.5267 - categorical_accuracy: 0.5438 - val_loss: 1.3964 - val_categorical_accuracy: 0.5946\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 8s 125ms/step - loss: 1.1112 - categorical_accuracy: 0.6529 - val_loss: 1.0612 - val_categorical_accuracy: 0.6348\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 8s 123ms/step - loss: 0.8502 - categorical_accuracy: 0.7210 - val_loss: 1.3569 - val_categorical_accuracy: 0.6938\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 0.6200 - categorical_accuracy: 0.7938 - val_loss: 1.0042 - val_categorical_accuracy: 0.7210\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 0.4634 - categorical_accuracy: 0.8478 - val_loss: 1.4330 - val_categorical_accuracy: 0.7425\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 8s 124ms/step - loss: 0.3133 - categorical_accuracy: 0.8956 - val_loss: 1.3515 - val_categorical_accuracy: 0.7360\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 9s 129ms/step - loss: 0.2274 - categorical_accuracy: 0.9298 - val_loss: 1.0021 - val_categorical_accuracy: 0.7556\n",
            "Epoch 9/50\n",
            "67/67 [==============================] - 8s 125ms/step - loss: 0.1712 - categorical_accuracy: 0.9462 - val_loss: 1.1051 - val_categorical_accuracy: 0.7678\n",
            "Epoch 10/50\n",
            "67/67 [==============================] - 8s 124ms/step - loss: 0.1136 - categorical_accuracy: 0.9644 - val_loss: 0.8751 - val_categorical_accuracy: 0.7697\n",
            "Epoch 11/50\n",
            "67/67 [==============================] - 8s 124ms/step - loss: 0.1153 - categorical_accuracy: 0.9637 - val_loss: 1.3747 - val_categorical_accuracy: 0.7406\n",
            "Epoch 12/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 0.0956 - categorical_accuracy: 0.9738 - val_loss: 2.1924 - val_categorical_accuracy: 0.7491\n",
            "Epoch 13/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 0.0871 - categorical_accuracy: 0.9754 - val_loss: 1.4040 - val_categorical_accuracy: 0.7725\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 3.5158041282556954e-05.\n",
            "Epoch 14/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 0.0264 - categorical_accuracy: 0.9930 - val_loss: 1.3961 - val_categorical_accuracy: 0.7921\n",
            "Epoch 15/50\n",
            "67/67 [==============================] - 8s 124ms/step - loss: 0.0116 - categorical_accuracy: 0.9974 - val_loss: 1.5196 - val_categorical_accuracy: 0.7949\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00015: early stopping\n",
            "loss fold  1 0.9120532274246216\n",
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 2 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 4272 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 1069 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 2670 validated image filenames belonging to 20 classes.\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 9s 139ms/step - loss: 2.5533 - categorical_accuracy: 0.3593 - val_loss: 1.5378 - val_categorical_accuracy: 0.5482\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 8s 117ms/step - loss: 1.4178 - categorical_accuracy: 0.5702 - val_loss: 0.9560 - val_categorical_accuracy: 0.6735\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 9s 137ms/step - loss: 0.9273 - categorical_accuracy: 0.7069 - val_loss: 0.9014 - val_categorical_accuracy: 0.6960\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 9s 129ms/step - loss: 0.6407 - categorical_accuracy: 0.7949 - val_loss: 0.6145 - val_categorical_accuracy: 0.7381\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 0.4681 - categorical_accuracy: 0.8518 - val_loss: 0.5921 - val_categorical_accuracy: 0.7465\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 8s 123ms/step - loss: 0.2942 - categorical_accuracy: 0.9043 - val_loss: 0.4762 - val_categorical_accuracy: 0.7643\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 0.2010 - categorical_accuracy: 0.9349 - val_loss: 0.6359 - val_categorical_accuracy: 0.7568\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 8s 123ms/step - loss: 0.1408 - categorical_accuracy: 0.9544 - val_loss: 0.7021 - val_categorical_accuracy: 0.7446\n",
            "Epoch 9/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 0.1769 - categorical_accuracy: 0.9448 - val_loss: 0.7754 - val_categorical_accuracy: 0.7624\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 3.5158041282556954e-05.\n",
            "Epoch 10/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.0491 - categorical_accuracy: 0.9860 - val_loss: 0.7412 - val_categorical_accuracy: 0.7783\n",
            "Epoch 11/50\n",
            "67/67 [==============================] - 8s 123ms/step - loss: 0.0310 - categorical_accuracy: 0.9930 - val_loss: 0.8074 - val_categorical_accuracy: 0.7792\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00011: early stopping\n",
            "loss fold  2 1.3802989721298218\n",
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 3 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 4272 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 1069 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 2670 validated image filenames belonging to 20 classes.\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 9s 137ms/step - loss: 2.7163 - categorical_accuracy: 0.3205 - val_loss: 1.6788 - val_categorical_accuracy: 0.4677\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 1.6599 - categorical_accuracy: 0.5051 - val_loss: 1.3834 - val_categorical_accuracy: 0.6015\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 8s 123ms/step - loss: 1.1597 - categorical_accuracy: 0.6358 - val_loss: 0.8455 - val_categorical_accuracy: 0.6296\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 0.8284 - categorical_accuracy: 0.7306 - val_loss: 0.7787 - val_categorical_accuracy: 0.6876\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 0.6142 - categorical_accuracy: 0.7842 - val_loss: 0.8187 - val_categorical_accuracy: 0.6773\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 0.4459 - categorical_accuracy: 0.8443 - val_loss: 0.9159 - val_categorical_accuracy: 0.7147\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 0.2905 - categorical_accuracy: 0.9010 - val_loss: 0.8751 - val_categorical_accuracy: 0.7521\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 3.5158041282556954e-05.\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 0.1550 - categorical_accuracy: 0.9506 - val_loss: 0.7563 - val_categorical_accuracy: 0.7540\n",
            "Epoch 9/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 0.1191 - categorical_accuracy: 0.9647 - val_loss: 0.7482 - val_categorical_accuracy: 0.7615\n",
            "Epoch 10/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 0.1039 - categorical_accuracy: 0.9644 - val_loss: 0.6820 - val_categorical_accuracy: 0.7717\n",
            "Epoch 11/50\n",
            "67/67 [==============================] - 8s 123ms/step - loss: 0.0926 - categorical_accuracy: 0.9698 - val_loss: 0.6979 - val_categorical_accuracy: 0.7689\n",
            "Epoch 12/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.0749 - categorical_accuracy: 0.9785 - val_loss: 0.6920 - val_categorical_accuracy: 0.7746\n",
            "Epoch 13/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 0.0704 - categorical_accuracy: 0.9803 - val_loss: 0.6596 - val_categorical_accuracy: 0.7689\n",
            "Epoch 14/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 0.0559 - categorical_accuracy: 0.9864 - val_loss: 0.6628 - val_categorical_accuracy: 0.7736\n",
            "Epoch 15/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.0504 - categorical_accuracy: 0.9860 - val_loss: 0.6795 - val_categorical_accuracy: 0.7699\n",
            "Epoch 16/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 0.0431 - categorical_accuracy: 0.9885 - val_loss: 0.6506 - val_categorical_accuracy: 0.7736\n",
            "Epoch 17/50\n",
            "67/67 [==============================] - 8s 124ms/step - loss: 0.0351 - categorical_accuracy: 0.9916 - val_loss: 0.6763 - val_categorical_accuracy: 0.7764\n",
            "Epoch 18/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.0359 - categorical_accuracy: 0.9909 - val_loss: 0.6596 - val_categorical_accuracy: 0.7717\n",
            "Epoch 19/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.0298 - categorical_accuracy: 0.9927 - val_loss: 0.6503 - val_categorical_accuracy: 0.7717\n",
            "Epoch 20/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 0.0245 - categorical_accuracy: 0.9960 - val_loss: 0.7499 - val_categorical_accuracy: 0.7736\n",
            "Epoch 21/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.0211 - categorical_accuracy: 0.9960 - val_loss: 0.6822 - val_categorical_accuracy: 0.7755\n",
            "Epoch 22/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.0208 - categorical_accuracy: 0.9960 - val_loss: 0.6530 - val_categorical_accuracy: 0.7811\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Epoch 23/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.0182 - categorical_accuracy: 0.9965 - val_loss: 0.6760 - val_categorical_accuracy: 0.7792\n",
            "Epoch 24/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.0176 - categorical_accuracy: 0.9967 - val_loss: 0.6809 - val_categorical_accuracy: 0.7792\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00024: early stopping\n",
            "loss fold  3 1.6423346996307373\n",
            "loss media configurazione:  1.3115622997283936\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:smac.intensification.intensification.Intensifier:Challenger (1.3116) is better than incumbent (2147483647.0000) on 1 runs.\n",
            "INFO:smac.intensification.intensification.Intensifier:Changes in incumbent:\n",
            "INFO:smac.intensification.intensification.Intensifier:  dropout_first_layer : 0.333365779402875 -> 0.33654945057688657\n",
            "INFO:smac.intensification.intensification.Intensifier:  freeze_to : 12 -> 16\n",
            "INFO:smac.intensification.intensification.Intensifier:  learning_rate : 0.0004147045138530529 -> 0.0003515804035642449\n",
            "INFO:smac.intensification.intensification.Intensifier:  number_first_layer : 982 -> 802\n",
            "INFO:smac.intensification.intensification.Intensifier:  number_layer : 3 -> 1\n",
            "INFO:smac.intensification.intensification.Intensifier:Wallclock time limit for intensification reached (used: 440.914534 sec, available: 0.000010 sec)\n",
            "INFO:smac.intensification.intensification.Intensifier:Updated estimated cost of incumbent on 1 runs: 1.3116\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 1 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 4272 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 1068 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 2671 validated image filenames belonging to 20 classes.\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 10s 151ms/step - loss: 6.1130 - categorical_accuracy: 0.3471 - val_loss: 1.9642 - val_categorical_accuracy: 0.6049\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 8s 113ms/step - loss: 1.6570 - categorical_accuracy: 0.5702 - val_loss: 1.7001 - val_categorical_accuracy: 0.6760\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 1.2391 - categorical_accuracy: 0.6503 - val_loss: 1.4330 - val_categorical_accuracy: 0.7032\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 9s 134ms/step - loss: 0.9935 - categorical_accuracy: 0.7154 - val_loss: 1.2904 - val_categorical_accuracy: 0.7453\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 8s 123ms/step - loss: 0.8217 - categorical_accuracy: 0.7570 - val_loss: 1.2293 - val_categorical_accuracy: 0.7444\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.6889 - categorical_accuracy: 0.7891 - val_loss: 1.3587 - val_categorical_accuracy: 0.7678\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.6006 - categorical_accuracy: 0.8083 - val_loss: 1.3136 - val_categorical_accuracy: 0.7734\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.5196 - categorical_accuracy: 0.8326 - val_loss: 1.2179 - val_categorical_accuracy: 0.7800\n",
            "Epoch 9/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.4553 - categorical_accuracy: 0.8485 - val_loss: 1.3458 - val_categorical_accuracy: 0.7828\n",
            "Epoch 10/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.4151 - categorical_accuracy: 0.8647 - val_loss: 1.2926 - val_categorical_accuracy: 0.7903\n",
            "Epoch 11/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.3849 - categorical_accuracy: 0.8750 - val_loss: 1.3312 - val_categorical_accuracy: 0.8024\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 7.39764713216573e-05.\n",
            "Epoch 12/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.3293 - categorical_accuracy: 0.8933 - val_loss: 1.3046 - val_categorical_accuracy: 0.8043\n",
            "Epoch 13/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.3238 - categorical_accuracy: 0.8954 - val_loss: 1.3009 - val_categorical_accuracy: 0.8024\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00013: early stopping\n",
            "loss fold  1 0.9939068555831909\n",
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 2 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 4272 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 1069 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 2670 validated image filenames belonging to 20 classes.\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 9s 140ms/step - loss: 5.8001 - categorical_accuracy: 0.3621 - val_loss: 1.1950 - val_categorical_accuracy: 0.6221\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 8s 112ms/step - loss: 1.6074 - categorical_accuracy: 0.5714 - val_loss: 0.8418 - val_categorical_accuracy: 0.6651\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 1.1953 - categorical_accuracy: 0.6524 - val_loss: 0.6728 - val_categorical_accuracy: 0.7138\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.9685 - categorical_accuracy: 0.7100 - val_loss: 0.6532 - val_categorical_accuracy: 0.7343\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.7862 - categorical_accuracy: 0.7575 - val_loss: 0.6125 - val_categorical_accuracy: 0.7558\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.6763 - categorical_accuracy: 0.7804 - val_loss: 0.5397 - val_categorical_accuracy: 0.7671\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.5793 - categorical_accuracy: 0.8155 - val_loss: 0.5320 - val_categorical_accuracy: 0.7830\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 8s 117ms/step - loss: 0.4978 - categorical_accuracy: 0.8441 - val_loss: 0.5393 - val_categorical_accuracy: 0.7755\n",
            "Epoch 9/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 0.4460 - categorical_accuracy: 0.8507 - val_loss: 0.4670 - val_categorical_accuracy: 0.7783\n",
            "Epoch 10/50\n",
            "67/67 [==============================] - 8s 123ms/step - loss: 0.3963 - categorical_accuracy: 0.8687 - val_loss: 0.4516 - val_categorical_accuracy: 0.7858\n",
            "Epoch 11/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 0.3467 - categorical_accuracy: 0.8860 - val_loss: 0.4131 - val_categorical_accuracy: 0.8045\n",
            "Epoch 12/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.3169 - categorical_accuracy: 0.8914 - val_loss: 0.4340 - val_categorical_accuracy: 0.7923\n",
            "Epoch 13/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.2922 - categorical_accuracy: 0.8993 - val_loss: 0.4346 - val_categorical_accuracy: 0.7886\n",
            "Epoch 14/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.2618 - categorical_accuracy: 0.9108 - val_loss: 0.3957 - val_categorical_accuracy: 0.8045\n",
            "Epoch 15/50\n",
            "67/67 [==============================] - 8s 125ms/step - loss: 0.2408 - categorical_accuracy: 0.9157 - val_loss: 0.4651 - val_categorical_accuracy: 0.8064\n",
            "Epoch 16/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.2175 - categorical_accuracy: 0.9232 - val_loss: 0.4088 - val_categorical_accuracy: 0.8026\n",
            "Epoch 17/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.1924 - categorical_accuracy: 0.9298 - val_loss: 0.4244 - val_categorical_accuracy: 0.8007\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 7.39764713216573e-05.\n",
            "Epoch 18/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.1993 - categorical_accuracy: 0.9342 - val_loss: 0.4100 - val_categorical_accuracy: 0.8073\n",
            "Epoch 19/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 0.1622 - categorical_accuracy: 0.9469 - val_loss: 0.4093 - val_categorical_accuracy: 0.8110\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00019: early stopping\n",
            "loss fold  2 1.2225455045700073\n",
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 3 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 4272 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 1069 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 2670 validated image filenames belonging to 20 classes.\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 9s 133ms/step - loss: 5.7054 - categorical_accuracy: 0.3572 - val_loss: 1.3594 - val_categorical_accuracy: 0.6052\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 8s 117ms/step - loss: 1.5905 - categorical_accuracy: 0.5770 - val_loss: 1.0921 - val_categorical_accuracy: 0.6763\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 1.1846 - categorical_accuracy: 0.6643 - val_loss: 0.8002 - val_categorical_accuracy: 0.7100\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.9418 - categorical_accuracy: 0.7189 - val_loss: 0.7879 - val_categorical_accuracy: 0.7325\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.7884 - categorical_accuracy: 0.7638 - val_loss: 0.7530 - val_categorical_accuracy: 0.7437\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.7018 - categorical_accuracy: 0.7842 - val_loss: 0.6881 - val_categorical_accuracy: 0.7493\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.5903 - categorical_accuracy: 0.8207 - val_loss: 0.6342 - val_categorical_accuracy: 0.7577\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 9s 129ms/step - loss: 0.5293 - categorical_accuracy: 0.8303 - val_loss: 0.6916 - val_categorical_accuracy: 0.7680\n",
            "Epoch 9/50\n",
            "67/67 [==============================] - 8s 127ms/step - loss: 0.4638 - categorical_accuracy: 0.8478 - val_loss: 0.5951 - val_categorical_accuracy: 0.7727\n",
            "Epoch 10/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.4074 - categorical_accuracy: 0.8635 - val_loss: 0.6311 - val_categorical_accuracy: 0.7680\n",
            "Epoch 11/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 0.3739 - categorical_accuracy: 0.8755 - val_loss: 0.6295 - val_categorical_accuracy: 0.7671\n",
            "Epoch 12/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.3214 - categorical_accuracy: 0.8904 - val_loss: 0.4920 - val_categorical_accuracy: 0.7708\n",
            "Epoch 13/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.2972 - categorical_accuracy: 0.8982 - val_loss: 0.4947 - val_categorical_accuracy: 0.7830\n",
            "Epoch 14/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.2880 - categorical_accuracy: 0.9031 - val_loss: 0.5582 - val_categorical_accuracy: 0.7764\n",
            "Epoch 15/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.2398 - categorical_accuracy: 0.9150 - val_loss: 0.5135 - val_categorical_accuracy: 0.7877\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 7.39764713216573e-05.\n",
            "Epoch 16/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.2190 - categorical_accuracy: 0.9256 - val_loss: 0.5161 - val_categorical_accuracy: 0.7867\n",
            "Epoch 17/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.2134 - categorical_accuracy: 0.9284 - val_loss: 0.5107 - val_categorical_accuracy: 0.7886\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00017: early stopping\n",
            "loss fold  3 1.8516836166381836\n",
            "loss media configurazione:  1.3560453255971272\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:smac.intensification.intensification.Intensifier:Wallclock time limit for intensification reached (used: 423.217484 sec, available: 0.000010 sec)\n",
            "INFO:smac.intensification.intensification.Intensifier:Updated estimated cost of incumbent on 1 runs: 1.3116\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 1 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 4272 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 1068 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 2671 validated image filenames belonging to 20 classes.\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 11s 166ms/step - loss: 3.0992 - categorical_accuracy: 0.1100 - val_loss: 2.7968 - val_categorical_accuracy: 0.1582\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 8s 117ms/step - loss: 2.8893 - categorical_accuracy: 0.1257 - val_loss: 2.9595 - val_categorical_accuracy: 0.1582\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 2.9335 - categorical_accuracy: 0.1456 - val_loss: 2.8908 - val_categorical_accuracy: 0.1582\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 2.8825 - categorical_accuracy: 0.1442 - val_loss: 2.8395 - val_categorical_accuracy: 0.1582\n",
            "\n",
            "Epoch 00004: ReduceLROnPlateau reducing learning rate to 3.051588428206742e-05.\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 2.8631 - categorical_accuracy: 0.1442 - val_loss: 2.8367 - val_categorical_accuracy: 0.1582\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 2.8593 - categorical_accuracy: 0.1447 - val_loss: 2.8340 - val_categorical_accuracy: 0.1582\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00006: early stopping\n",
            "loss fold  1 2.6614460945129395\n",
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 2 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 4272 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 1069 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 2670 validated image filenames belonging to 20 classes.\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 9s 140ms/step - loss: 3.1427 - categorical_accuracy: 0.1114 - val_loss: 2.8858 - val_categorical_accuracy: 0.1384\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 8s 114ms/step - loss: 2.8092 - categorical_accuracy: 0.1482 - val_loss: 2.6724 - val_categorical_accuracy: 0.1384\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 2.6256 - categorical_accuracy: 0.1582 - val_loss: 2.5867 - val_categorical_accuracy: 0.1908\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 2.5388 - categorical_accuracy: 0.1838 - val_loss: 2.5410 - val_categorical_accuracy: 0.1777\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 8s 124ms/step - loss: 2.4957 - categorical_accuracy: 0.1889 - val_loss: 2.4780 - val_categorical_accuracy: 0.1852\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 2.4143 - categorical_accuracy: 0.2097 - val_loss: 2.4647 - val_categorical_accuracy: 0.2105\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 9s 127ms/step - loss: 2.3111 - categorical_accuracy: 0.2556 - val_loss: 2.3100 - val_categorical_accuracy: 0.2498\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 2.1856 - categorical_accuracy: 0.2797 - val_loss: 2.2101 - val_categorical_accuracy: 0.2872\n",
            "Epoch 9/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 2.0581 - categorical_accuracy: 0.3022 - val_loss: 2.1580 - val_categorical_accuracy: 0.2975\n",
            "Epoch 10/50\n",
            "67/67 [==============================] - 8s 123ms/step - loss: 1.9788 - categorical_accuracy: 0.3200 - val_loss: 2.1478 - val_categorical_accuracy: 0.2937\n",
            "Epoch 11/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 1.9199 - categorical_accuracy: 0.3352 - val_loss: 2.1087 - val_categorical_accuracy: 0.2862\n",
            "Epoch 12/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 1.7709 - categorical_accuracy: 0.3652 - val_loss: 1.8317 - val_categorical_accuracy: 0.3442\n",
            "Epoch 13/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 1.6963 - categorical_accuracy: 0.3769 - val_loss: 2.0421 - val_categorical_accuracy: 0.3386\n",
            "Epoch 14/50\n",
            "67/67 [==============================] - 8s 123ms/step - loss: 1.6334 - categorical_accuracy: 0.3993 - val_loss: 1.9180 - val_categorical_accuracy: 0.3340\n",
            "Epoch 15/50\n",
            "67/67 [==============================] - 8s 125ms/step - loss: 1.5536 - categorical_accuracy: 0.4087 - val_loss: 1.8344 - val_categorical_accuracy: 0.3798\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 3.051588428206742e-05.\n",
            "Epoch 16/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 1.3354 - categorical_accuracy: 0.4853 - val_loss: 1.7137 - val_categorical_accuracy: 0.4032\n",
            "Epoch 17/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 1.2707 - categorical_accuracy: 0.5019 - val_loss: 1.8055 - val_categorical_accuracy: 0.3957\n",
            "Epoch 18/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 1.2151 - categorical_accuracy: 0.5197 - val_loss: 1.7455 - val_categorical_accuracy: 0.4041\n",
            "Epoch 19/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 1.1821 - categorical_accuracy: 0.5323 - val_loss: 1.7958 - val_categorical_accuracy: 0.4079\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Epoch 20/50\n",
            "67/67 [==============================] - 8s 125ms/step - loss: 1.1376 - categorical_accuracy: 0.5487 - val_loss: 1.8127 - val_categorical_accuracy: 0.4107\n",
            "Epoch 21/50\n",
            "67/67 [==============================] - 9s 134ms/step - loss: 1.1226 - categorical_accuracy: 0.5574 - val_loss: 1.8068 - val_categorical_accuracy: 0.4191\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00021: early stopping\n",
            "loss fold  2 2.0308609008789062\n",
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 3 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 4272 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 1069 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 2670 validated image filenames belonging to 20 classes.\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 9s 132ms/step - loss: 3.0439 - categorical_accuracy: 0.1318 - val_loss: 2.9141 - val_categorical_accuracy: 0.1366\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 8s 123ms/step - loss: 2.6856 - categorical_accuracy: 0.1517 - val_loss: 2.7332 - val_categorical_accuracy: 0.1515\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 2.5818 - categorical_accuracy: 0.1774 - val_loss: 2.6946 - val_categorical_accuracy: 0.1787\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 2.4935 - categorical_accuracy: 0.2020 - val_loss: 2.6349 - val_categorical_accuracy: 0.1936\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 2.4745 - categorical_accuracy: 0.2158 - val_loss: 2.6020 - val_categorical_accuracy: 0.1833\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 2.3741 - categorical_accuracy: 0.2259 - val_loss: 2.5189 - val_categorical_accuracy: 0.1993\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 2.2573 - categorical_accuracy: 0.2472 - val_loss: 2.3650 - val_categorical_accuracy: 0.2170\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 2.2056 - categorical_accuracy: 0.2540 - val_loss: 2.3676 - val_categorical_accuracy: 0.2657\n",
            "Epoch 9/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 2.1459 - categorical_accuracy: 0.2795 - val_loss: 2.3174 - val_categorical_accuracy: 0.2563\n",
            "Epoch 10/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 2.0535 - categorical_accuracy: 0.3008 - val_loss: 2.2966 - val_categorical_accuracy: 0.2657\n",
            "Epoch 11/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 1.9947 - categorical_accuracy: 0.3127 - val_loss: 2.2745 - val_categorical_accuracy: 0.2619\n",
            "Epoch 12/50\n",
            "67/67 [==============================] - 8s 123ms/step - loss: 1.9611 - categorical_accuracy: 0.3207 - val_loss: 2.4183 - val_categorical_accuracy: 0.2591\n",
            "Epoch 13/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 1.9125 - categorical_accuracy: 0.3230 - val_loss: 2.5202 - val_categorical_accuracy: 0.2834\n",
            "Epoch 14/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 1.9109 - categorical_accuracy: 0.3251 - val_loss: 2.1775 - val_categorical_accuracy: 0.2451\n",
            "Epoch 15/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 1.8483 - categorical_accuracy: 0.3347 - val_loss: 2.4525 - val_categorical_accuracy: 0.2675\n",
            "Epoch 16/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 1.8198 - categorical_accuracy: 0.3434 - val_loss: 2.6530 - val_categorical_accuracy: 0.2142\n",
            "Epoch 17/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 1.8825 - categorical_accuracy: 0.3233 - val_loss: 2.2682 - val_categorical_accuracy: 0.2816\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.051588428206742e-05.\n",
            "Epoch 18/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 1.6418 - categorical_accuracy: 0.3874 - val_loss: 2.2901 - val_categorical_accuracy: 0.2909\n",
            "Epoch 19/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 1.6017 - categorical_accuracy: 0.3907 - val_loss: 2.2942 - val_categorical_accuracy: 0.3031\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00019: early stopping\n",
            "loss fold  3 2.2754714488983154\n",
            "loss media configurazione:  2.322592814763387\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:smac.intensification.intensification.Intensifier:Wallclock time limit for intensification reached (used: 407.685185 sec, available: 0.000010 sec)\n",
            "INFO:smac.intensification.intensification.Intensifier:Updated estimated cost of incumbent on 1 runs: 1.3116\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 1 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 4272 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 1068 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 2671 validated image filenames belonging to 20 classes.\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 10s 155ms/step - loss: 3.2909 - categorical_accuracy: 0.1255 - val_loss: 2.9417 - val_categorical_accuracy: 0.1582\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 2.9213 - categorical_accuracy: 0.1447 - val_loss: 2.8858 - val_categorical_accuracy: 0.1582\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 2.8776 - categorical_accuracy: 0.1447 - val_loss: 2.8444 - val_categorical_accuracy: 0.1582\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 2.8552 - categorical_accuracy: 0.1447 - val_loss: 2.8267 - val_categorical_accuracy: 0.1582\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 2.8505 - categorical_accuracy: 0.1447 - val_loss: 2.8192 - val_categorical_accuracy: 0.1582\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 2.8503 - categorical_accuracy: 0.1447 - val_loss: 2.8163 - val_categorical_accuracy: 0.1582\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 2.8493 - categorical_accuracy: 0.1447 - val_loss: 2.8163 - val_categorical_accuracy: 0.1582\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 2.8490 - categorical_accuracy: 0.1447 - val_loss: 2.8138 - val_categorical_accuracy: 0.1582\n",
            "Epoch 9/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 2.8498 - categorical_accuracy: 0.1447 - val_loss: 2.8140 - val_categorical_accuracy: 0.1582\n",
            "Epoch 10/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 2.8491 - categorical_accuracy: 0.1447 - val_loss: 2.8115 - val_categorical_accuracy: 0.1582\n",
            "Epoch 11/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 2.8499 - categorical_accuracy: 0.1447 - val_loss: 2.8146 - val_categorical_accuracy: 0.1582\n",
            "Epoch 12/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 2.8491 - categorical_accuracy: 0.1447 - val_loss: 2.8130 - val_categorical_accuracy: 0.1582\n",
            "Epoch 13/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 2.8505 - categorical_accuracy: 0.1447 - val_loss: 2.8127 - val_categorical_accuracy: 0.1582\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 6.481438176706434e-05.\n",
            "Epoch 14/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 2.8471 - categorical_accuracy: 0.1447 - val_loss: 2.8129 - val_categorical_accuracy: 0.1582\n",
            "Epoch 15/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 2.8487 - categorical_accuracy: 0.1447 - val_loss: 2.8127 - val_categorical_accuracy: 0.1582\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00015: early stopping\n",
            "loss fold  1 2.6170170307159424\n",
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 2 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 4272 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 1069 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 2670 validated image filenames belonging to 20 classes.\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 10s 143ms/step - loss: 3.2797 - categorical_accuracy: 0.1301 - val_loss: 2.9576 - val_categorical_accuracy: 0.1553\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 8s 113ms/step - loss: 2.9207 - categorical_accuracy: 0.1468 - val_loss: 2.9174 - val_categorical_accuracy: 0.1553\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 2.8760 - categorical_accuracy: 0.1468 - val_loss: 2.8957 - val_categorical_accuracy: 0.1553\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 2.8556 - categorical_accuracy: 0.1468 - val_loss: 2.8904 - val_categorical_accuracy: 0.1553\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 2.8475 - categorical_accuracy: 0.1468 - val_loss: 2.8945 - val_categorical_accuracy: 0.1553\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 2.8467 - categorical_accuracy: 0.1468 - val_loss: 2.8926 - val_categorical_accuracy: 0.1553\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 2.8468 - categorical_accuracy: 0.1468 - val_loss: 2.8928 - val_categorical_accuracy: 0.1553\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 6.481438176706434e-05.\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 2.8439 - categorical_accuracy: 0.1468 - val_loss: 2.8931 - val_categorical_accuracy: 0.1553\n",
            "Epoch 9/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 2.8443 - categorical_accuracy: 0.1468 - val_loss: 2.8928 - val_categorical_accuracy: 0.1553\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00009: early stopping\n",
            "loss fold  2 2.8646492958068848\n",
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 3 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 4272 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 1069 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 2670 validated image filenames belonging to 20 classes.\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 9s 130ms/step - loss: 3.2235 - categorical_accuracy: 0.1023 - val_loss: 2.9566 - val_categorical_accuracy: 0.1506\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 2.9212 - categorical_accuracy: 0.1472 - val_loss: 2.9158 - val_categorical_accuracy: 0.1506\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 2.8793 - categorical_accuracy: 0.1472 - val_loss: 2.8921 - val_categorical_accuracy: 0.1506\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 2.8609 - categorical_accuracy: 0.1472 - val_loss: 2.8824 - val_categorical_accuracy: 0.1506\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 2.8528 - categorical_accuracy: 0.1472 - val_loss: 2.8805 - val_categorical_accuracy: 0.1506\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 2.8508 - categorical_accuracy: 0.1472 - val_loss: 2.8767 - val_categorical_accuracy: 0.1506\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 2.8509 - categorical_accuracy: 0.1472 - val_loss: 2.8771 - val_categorical_accuracy: 0.1506\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 2.8515 - categorical_accuracy: 0.1472 - val_loss: 2.8750 - val_categorical_accuracy: 0.1506\n",
            "Epoch 9/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 2.8535 - categorical_accuracy: 0.1472 - val_loss: 2.8737 - val_categorical_accuracy: 0.1506\n",
            "Epoch 10/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 2.8514 - categorical_accuracy: 0.1472 - val_loss: 2.8739 - val_categorical_accuracy: 0.1506\n",
            "Epoch 11/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 2.8499 - categorical_accuracy: 0.1472 - val_loss: 2.8738 - val_categorical_accuracy: 0.1506\n",
            "Epoch 12/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 2.8511 - categorical_accuracy: 0.1472 - val_loss: 2.8741 - val_categorical_accuracy: 0.1506\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 6.481438176706434e-05.\n",
            "Epoch 13/50\n",
            "67/67 [==============================] - 8s 123ms/step - loss: 2.8515 - categorical_accuracy: 0.1472 - val_loss: 2.8741 - val_categorical_accuracy: 0.1506\n",
            "Epoch 14/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 2.8508 - categorical_accuracy: 0.1472 - val_loss: 2.8739 - val_categorical_accuracy: 0.1506\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00014: early stopping\n",
            "loss fold  3 2.6086108684539795\n",
            "loss media configurazione:  2.696759064992269\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:smac.intensification.intensification.Intensifier:Wallclock time limit for intensification reached (used: 335.874012 sec, available: 0.000010 sec)\n",
            "INFO:smac.intensification.intensification.Intensifier:Updated estimated cost of incumbent on 1 runs: 1.3116\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 1 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 4272 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 1068 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 2671 validated image filenames belonging to 20 classes.\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 11s 159ms/step - loss: 3.2064 - categorical_accuracy: 0.1561 - val_loss: 2.3920 - val_categorical_accuracy: 0.3137\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 7s 112ms/step - loss: 2.1820 - categorical_accuracy: 0.3518 - val_loss: 1.6724 - val_categorical_accuracy: 0.5047\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 1.5746 - categorical_accuracy: 0.5353 - val_loss: 1.0242 - val_categorical_accuracy: 0.6526\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 1.1492 - categorical_accuracy: 0.6601 - val_loss: 0.7927 - val_categorical_accuracy: 0.7069\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.8435 - categorical_accuracy: 0.7411 - val_loss: 0.8510 - val_categorical_accuracy: 0.7369\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.6255 - categorical_accuracy: 0.8034 - val_loss: 0.6484 - val_categorical_accuracy: 0.7743\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.4670 - categorical_accuracy: 0.8577 - val_loss: 0.5539 - val_categorical_accuracy: 0.7856\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.3414 - categorical_accuracy: 0.8968 - val_loss: 0.6762 - val_categorical_accuracy: 0.7809\n",
            "Epoch 9/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.2300 - categorical_accuracy: 0.9274 - val_loss: 0.5623 - val_categorical_accuracy: 0.7912\n",
            "Epoch 10/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 0.1908 - categorical_accuracy: 0.9405 - val_loss: 0.6466 - val_categorical_accuracy: 0.7949\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.0151414608117193e-05.\n",
            "Epoch 11/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.1171 - categorical_accuracy: 0.9672 - val_loss: 0.5768 - val_categorical_accuracy: 0.8155\n",
            "Epoch 12/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.0920 - categorical_accuracy: 0.9721 - val_loss: 0.5637 - val_categorical_accuracy: 0.8127\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00012: early stopping\n",
            "loss fold  1 0.9568665623664856\n",
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 2 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 4272 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 1069 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 2670 validated image filenames belonging to 20 classes.\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 9s 140ms/step - loss: 3.2894 - categorical_accuracy: 0.1461 - val_loss: 2.2505 - val_categorical_accuracy: 0.3340\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 7s 112ms/step - loss: 2.1142 - categorical_accuracy: 0.3848 - val_loss: 1.6333 - val_categorical_accuracy: 0.5585\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 1.4732 - categorical_accuracy: 0.5712 - val_loss: 1.0751 - val_categorical_accuracy: 0.6988\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 1.0164 - categorical_accuracy: 0.6861 - val_loss: 0.8164 - val_categorical_accuracy: 0.7933\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.7421 - categorical_accuracy: 0.7757 - val_loss: 0.7091 - val_categorical_accuracy: 0.7961\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 0.5097 - categorical_accuracy: 0.8432 - val_loss: 0.7513 - val_categorical_accuracy: 0.7933\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 0.4006 - categorical_accuracy: 0.8734 - val_loss: 0.7839 - val_categorical_accuracy: 0.8054\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.2962 - categorical_accuracy: 0.9078 - val_loss: 0.6716 - val_categorical_accuracy: 0.8335\n",
            "Epoch 9/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.2029 - categorical_accuracy: 0.9382 - val_loss: 0.7058 - val_categorical_accuracy: 0.8316\n",
            "Epoch 10/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.1542 - categorical_accuracy: 0.9537 - val_loss: 0.5783 - val_categorical_accuracy: 0.8485\n",
            "Epoch 11/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.0967 - categorical_accuracy: 0.9712 - val_loss: 0.7759 - val_categorical_accuracy: 0.8391\n",
            "Epoch 12/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.0832 - categorical_accuracy: 0.9740 - val_loss: 0.7216 - val_categorical_accuracy: 0.8428\n",
            "Epoch 13/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.0619 - categorical_accuracy: 0.9813 - val_loss: 0.7518 - val_categorical_accuracy: 0.8157\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.0151414608117193e-05.\n",
            "Epoch 14/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.0319 - categorical_accuracy: 0.9909 - val_loss: 0.6156 - val_categorical_accuracy: 0.8485\n",
            "Epoch 15/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.0270 - categorical_accuracy: 0.9930 - val_loss: 0.5822 - val_categorical_accuracy: 0.8531\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00015: early stopping\n",
            "loss fold  2 0.4878964126110077\n",
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 3 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 4272 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 1069 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 2670 validated image filenames belonging to 20 classes.\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 9s 134ms/step - loss: 3.0900 - categorical_accuracy: 0.2065 - val_loss: 2.2346 - val_categorical_accuracy: 0.3779\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 2.0014 - categorical_accuracy: 0.4139 - val_loss: 1.4040 - val_categorical_accuracy: 0.5267\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 1.3943 - categorical_accuracy: 0.5845 - val_loss: 0.8634 - val_categorical_accuracy: 0.6745\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.9695 - categorical_accuracy: 0.7081 - val_loss: 0.7369 - val_categorical_accuracy: 0.7156\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.7151 - categorical_accuracy: 0.7931 - val_loss: 0.6815 - val_categorical_accuracy: 0.7474\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 0.5509 - categorical_accuracy: 0.8326 - val_loss: 0.6076 - val_categorical_accuracy: 0.7624\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.3888 - categorical_accuracy: 0.8794 - val_loss: 0.5009 - val_categorical_accuracy: 0.7774\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.2846 - categorical_accuracy: 0.9141 - val_loss: 0.3132 - val_categorical_accuracy: 0.7858\n",
            "Epoch 9/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.2100 - categorical_accuracy: 0.9352 - val_loss: 0.3010 - val_categorical_accuracy: 0.7839\n",
            "Epoch 10/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.1589 - categorical_accuracy: 0.9485 - val_loss: 0.3815 - val_categorical_accuracy: 0.7764\n",
            "Epoch 11/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.1165 - categorical_accuracy: 0.9621 - val_loss: 0.2607 - val_categorical_accuracy: 0.8064\n",
            "Epoch 12/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.0952 - categorical_accuracy: 0.9714 - val_loss: 0.3035 - val_categorical_accuracy: 0.8026\n",
            "Epoch 13/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.0735 - categorical_accuracy: 0.9778 - val_loss: 0.2247 - val_categorical_accuracy: 0.7858\n",
            "Epoch 14/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.0466 - categorical_accuracy: 0.9878 - val_loss: 0.1999 - val_categorical_accuracy: 0.7989\n",
            "Epoch 15/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.0455 - categorical_accuracy: 0.9876 - val_loss: 0.3186 - val_categorical_accuracy: 0.7895\n",
            "Epoch 16/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.0831 - categorical_accuracy: 0.9764 - val_loss: 0.5568 - val_categorical_accuracy: 0.7820\n",
            "Epoch 17/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.0761 - categorical_accuracy: 0.9787 - val_loss: 0.3405 - val_categorical_accuracy: 0.7989\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.0151414608117193e-05.\n",
            "Epoch 18/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.0261 - categorical_accuracy: 0.9949 - val_loss: 0.2282 - val_categorical_accuracy: 0.8157\n",
            "Epoch 19/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.0159 - categorical_accuracy: 0.9970 - val_loss: 0.2176 - val_categorical_accuracy: 0.8195\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00019: early stopping\n",
            "loss fold  3 2.586662769317627\n",
            "loss media configurazione:  1.3438085814317067\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:smac.intensification.intensification.Intensifier:Wallclock time limit for intensification reached (used: 400.053924 sec, available: 0.000010 sec)\n",
            "INFO:smac.intensification.intensification.Intensifier:Updated estimated cost of incumbent on 1 runs: 1.3116\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 1 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 4272 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 1068 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 2671 validated image filenames belonging to 20 classes.\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 10s 153ms/step - loss: 2.9062 - categorical_accuracy: 0.5482 - val_loss: 0.8093 - val_categorical_accuracy: 0.7912\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 8s 125ms/step - loss: 0.9155 - categorical_accuracy: 0.7662 - val_loss: 0.6612 - val_categorical_accuracy: 0.8024\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.6158 - categorical_accuracy: 0.8158 - val_loss: 0.6796 - val_categorical_accuracy: 0.8258\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.4538 - categorical_accuracy: 0.8546 - val_loss: 0.6495 - val_categorical_accuracy: 0.8230\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 8s 117ms/step - loss: 0.3605 - categorical_accuracy: 0.8787 - val_loss: 0.7110 - val_categorical_accuracy: 0.8343\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.2564 - categorical_accuracy: 0.9178 - val_loss: 0.6548 - val_categorical_accuracy: 0.8343\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.2205 - categorical_accuracy: 0.9244 - val_loss: 0.4527 - val_categorical_accuracy: 0.8343\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.1492 - categorical_accuracy: 0.9492 - val_loss: 0.5173 - val_categorical_accuracy: 0.8390\n",
            "Epoch 9/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.1469 - categorical_accuracy: 0.9518 - val_loss: 0.7158 - val_categorical_accuracy: 0.8324\n",
            "Epoch 10/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 0.1138 - categorical_accuracy: 0.9623 - val_loss: 0.7749 - val_categorical_accuracy: 0.8296\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 3.808951587416232e-05.\n",
            "Epoch 11/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.0951 - categorical_accuracy: 0.9700 - val_loss: 0.7216 - val_categorical_accuracy: 0.8390\n",
            "Epoch 12/50\n",
            "67/67 [==============================] - 8s 117ms/step - loss: 0.0763 - categorical_accuracy: 0.9754 - val_loss: 0.6982 - val_categorical_accuracy: 0.8436\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00012: early stopping\n",
            "loss fold  1 1.1493315696716309\n",
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 2 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 4272 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 1069 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 2670 validated image filenames belonging to 20 classes.\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 9s 136ms/step - loss: 2.8219 - categorical_accuracy: 0.5583 - val_loss: 0.9192 - val_categorical_accuracy: 0.7465\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 7s 111ms/step - loss: 0.9259 - categorical_accuracy: 0.7603 - val_loss: 0.9651 - val_categorical_accuracy: 0.7736\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.6474 - categorical_accuracy: 0.8069 - val_loss: 0.8238 - val_categorical_accuracy: 0.7867\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 8s 117ms/step - loss: 0.4413 - categorical_accuracy: 0.8640 - val_loss: 0.8795 - val_categorical_accuracy: 0.7905\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.3356 - categorical_accuracy: 0.8874 - val_loss: 0.9236 - val_categorical_accuracy: 0.8036\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.2499 - categorical_accuracy: 0.9176 - val_loss: 0.7804 - val_categorical_accuracy: 0.7905\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 0.2283 - categorical_accuracy: 0.9249 - val_loss: 0.9205 - val_categorical_accuracy: 0.7877\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.1590 - categorical_accuracy: 0.9483 - val_loss: 0.8594 - val_categorical_accuracy: 0.7989\n",
            "Epoch 9/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.1462 - categorical_accuracy: 0.9544 - val_loss: 0.8719 - val_categorical_accuracy: 0.8054\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 3.808951587416232e-05.\n",
            "Epoch 10/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.1070 - categorical_accuracy: 0.9670 - val_loss: 0.8568 - val_categorical_accuracy: 0.8017\n",
            "Epoch 11/50\n",
            "67/67 [==============================] - 8s 126ms/step - loss: 0.0834 - categorical_accuracy: 0.9757 - val_loss: 0.8609 - val_categorical_accuracy: 0.8007\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00011: early stopping\n",
            "loss fold  2 1.3666975498199463\n",
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 3 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 4272 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 1069 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 2670 validated image filenames belonging to 20 classes.\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 9s 128ms/step - loss: 2.8482 - categorical_accuracy: 0.5707 - val_loss: 0.7659 - val_categorical_accuracy: 0.7708\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.9580 - categorical_accuracy: 0.7568 - val_loss: 0.4970 - val_categorical_accuracy: 0.8120\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.6425 - categorical_accuracy: 0.8146 - val_loss: 0.6303 - val_categorical_accuracy: 0.8092\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.4850 - categorical_accuracy: 0.8490 - val_loss: 0.3024 - val_categorical_accuracy: 0.8082\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.3565 - categorical_accuracy: 0.8890 - val_loss: 0.3869 - val_categorical_accuracy: 0.8204\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.2717 - categorical_accuracy: 0.9085 - val_loss: 0.3611 - val_categorical_accuracy: 0.8288\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.2077 - categorical_accuracy: 0.9312 - val_loss: 0.3927 - val_categorical_accuracy: 0.8120\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 3.808951587416232e-05.\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.1734 - categorical_accuracy: 0.9405 - val_loss: 0.3242 - val_categorical_accuracy: 0.8241\n",
            "Epoch 9/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.1370 - categorical_accuracy: 0.9588 - val_loss: 0.3337 - val_categorical_accuracy: 0.8297\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00009: early stopping\n",
            "loss fold  3 0.6659004092216492\n",
            "loss media configurazione:  1.0606431762377422\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:smac.intensification.intensification.Intensifier:Challenger (1.0606) is better than incumbent (1.3116) on 1 runs.\n",
            "INFO:smac.intensification.intensification.Intensifier:Changes in incumbent:\n",
            "INFO:smac.intensification.intensification.Intensifier:  dropout_first_layer : 0.33654945057688657 -> 0.2952779400384898\n",
            "INFO:smac.intensification.intensification.Intensifier:  dropout_second_layer : None -> 0.2253110153221564\n",
            "INFO:smac.intensification.intensification.Intensifier:  freeze_to : 16 -> 18\n",
            "INFO:smac.intensification.intensification.Intensifier:  learning_rate : 0.0003515804035642449 -> 0.0003808951573694513\n",
            "INFO:smac.intensification.intensification.Intensifier:  number_first_layer : 802 -> 745\n",
            "INFO:smac.intensification.intensification.Intensifier:  number_layer : 1 -> 2\n",
            "INFO:smac.intensification.intensification.Intensifier:  number_second_layer : None -> 769\n",
            "INFO:smac.intensification.intensification.Intensifier:Wallclock time limit for intensification reached (used: 283.932178 sec, available: 0.000010 sec)\n",
            "INFO:smac.intensification.intensification.Intensifier:Updated estimated cost of incumbent on 1 runs: 1.0606\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 1 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 4272 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 1068 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 2671 validated image filenames belonging to 20 classes.\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 10s 153ms/step - loss: 2.7401 - categorical_accuracy: 0.3907 - val_loss: 1.5522 - val_categorical_accuracy: 0.6479\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 7s 111ms/step - loss: 1.1826 - categorical_accuracy: 0.6531 - val_loss: 1.4535 - val_categorical_accuracy: 0.7313\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.8655 - categorical_accuracy: 0.7430 - val_loss: 1.3805 - val_categorical_accuracy: 0.7669\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.7037 - categorical_accuracy: 0.7884 - val_loss: 1.2660 - val_categorical_accuracy: 0.7828\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 9s 134ms/step - loss: 0.5747 - categorical_accuracy: 0.8191 - val_loss: 1.2097 - val_categorical_accuracy: 0.7875\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 0.4534 - categorical_accuracy: 0.8542 - val_loss: 1.1876 - val_categorical_accuracy: 0.7828\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.4151 - categorical_accuracy: 0.8675 - val_loss: 1.3602 - val_categorical_accuracy: 0.7884\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.3388 - categorical_accuracy: 0.8935 - val_loss: 1.3748 - val_categorical_accuracy: 0.7968\n",
            "Epoch 9/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.3099 - categorical_accuracy: 0.9019 - val_loss: 1.4726 - val_categorical_accuracy: 0.7959\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 3.76444892026484e-05.\n",
            "Epoch 10/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.2170 - categorical_accuracy: 0.9316 - val_loss: 1.4437 - val_categorical_accuracy: 0.7996\n",
            "Epoch 11/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.2049 - categorical_accuracy: 0.9302 - val_loss: 1.4347 - val_categorical_accuracy: 0.8015\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00011: early stopping\n",
            "loss fold  1 1.1765388250350952\n",
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 2 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 4272 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 1069 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 2670 validated image filenames belonging to 20 classes.\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 9s 136ms/step - loss: 2.9170 - categorical_accuracy: 0.3722 - val_loss: 1.0948 - val_categorical_accuracy: 0.6941\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 7s 112ms/step - loss: 1.2782 - categorical_accuracy: 0.6346 - val_loss: 0.7586 - val_categorical_accuracy: 0.7680\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.9403 - categorical_accuracy: 0.7275 - val_loss: 0.5683 - val_categorical_accuracy: 0.7961\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.7725 - categorical_accuracy: 0.7690 - val_loss: 0.5968 - val_categorical_accuracy: 0.7989\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.6141 - categorical_accuracy: 0.8092 - val_loss: 0.6693 - val_categorical_accuracy: 0.8138\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.5043 - categorical_accuracy: 0.8371 - val_loss: 0.5494 - val_categorical_accuracy: 0.8167\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 8s 117ms/step - loss: 0.4204 - categorical_accuracy: 0.8633 - val_loss: 0.6731 - val_categorical_accuracy: 0.8185\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.3371 - categorical_accuracy: 0.8883 - val_loss: 0.6602 - val_categorical_accuracy: 0.8288\n",
            "Epoch 9/50\n",
            "67/67 [==============================] - 8s 117ms/step - loss: 0.3197 - categorical_accuracy: 0.8940 - val_loss: 0.6589 - val_categorical_accuracy: 0.8335\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 3.76444892026484e-05.\n",
            "Epoch 10/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.2314 - categorical_accuracy: 0.9263 - val_loss: 0.6546 - val_categorical_accuracy: 0.8335\n",
            "Epoch 11/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.2122 - categorical_accuracy: 0.9316 - val_loss: 0.6436 - val_categorical_accuracy: 0.8307\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00011: early stopping\n",
            "loss fold  2 0.6395903825759888\n",
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 3 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 4272 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 1069 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 2670 validated image filenames belonging to 20 classes.\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 9s 131ms/step - loss: 2.9185 - categorical_accuracy: 0.3659 - val_loss: 1.1980 - val_categorical_accuracy: 0.6829\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 1.3552 - categorical_accuracy: 0.6173 - val_loss: 1.0838 - val_categorical_accuracy: 0.7671\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.9784 - categorical_accuracy: 0.7168 - val_loss: 0.9945 - val_categorical_accuracy: 0.7680\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 8s 117ms/step - loss: 0.7793 - categorical_accuracy: 0.7760 - val_loss: 1.1172 - val_categorical_accuracy: 0.8064\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.6574 - categorical_accuracy: 0.8057 - val_loss: 1.1113 - val_categorical_accuracy: 0.7979\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.5489 - categorical_accuracy: 0.8291 - val_loss: 1.0451 - val_categorical_accuracy: 0.8204\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 3.76444892026484e-05.\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.4183 - categorical_accuracy: 0.8624 - val_loss: 1.0630 - val_categorical_accuracy: 0.8195\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.4162 - categorical_accuracy: 0.8617 - val_loss: 1.1065 - val_categorical_accuracy: 0.8176\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00008: early stopping\n",
            "loss fold  3 0.9346320033073425\n",
            "loss media configurazione:  0.9169204036394755\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:smac.intensification.intensification.Intensifier:Challenger (0.9169) is better than incumbent (1.0606) on 1 runs.\n",
            "INFO:smac.intensification.intensification.Intensifier:Changes in incumbent:\n",
            "INFO:smac.intensification.intensification.Intensifier:  dropout_first_layer : 0.2952779400384898 -> 0.2359697570605495\n",
            "INFO:smac.intensification.intensification.Intensifier:  dropout_second_layer : 0.2253110153221564 -> 0.31255434692892914\n",
            "INFO:smac.intensification.intensification.Intensifier:  dropout_third_layer : None -> 0.307068602192057\n",
            "INFO:smac.intensification.intensification.Intensifier:  freeze_to : 18 -> 19\n",
            "INFO:smac.intensification.intensification.Intensifier:  learning_rate : 0.0003808951573694513 -> 0.0003764448929249176\n",
            "INFO:smac.intensification.intensification.Intensifier:  number_first_layer : 745 -> 990\n",
            "INFO:smac.intensification.intensification.Intensifier:  number_layer : 2 -> 3\n",
            "INFO:smac.intensification.intensification.Intensifier:  number_second_layer : 769 -> 1009\n",
            "INFO:smac.intensification.intensification.Intensifier:  number_third_layer : None -> 143\n",
            "INFO:smac.intensification.intensification.Intensifier:Wallclock time limit for intensification reached (used: 268.809497 sec, available: 0.000010 sec)\n",
            "INFO:smac.intensification.intensification.Intensifier:Updated estimated cost of incumbent on 1 runs: 0.9169\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 1 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 4272 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 1068 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 2671 validated image filenames belonging to 20 classes.\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 12s 179ms/step - loss: 3.1325 - categorical_accuracy: 0.1332 - val_loss: 2.9536 - val_categorical_accuracy: 0.1348\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 8s 116ms/step - loss: 2.8856 - categorical_accuracy: 0.1531 - val_loss: 2.9075 - val_categorical_accuracy: 0.1348\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 8s 123ms/step - loss: 2.8373 - categorical_accuracy: 0.1531 - val_loss: 2.9239 - val_categorical_accuracy: 0.1348\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 8s 123ms/step - loss: 2.8346 - categorical_accuracy: 0.1531 - val_loss: 2.9357 - val_categorical_accuracy: 0.1348\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 8s 123ms/step - loss: 2.8353 - categorical_accuracy: 0.1531 - val_loss: 2.9356 - val_categorical_accuracy: 0.1348\n",
            "\n",
            "Epoch 00005: ReduceLROnPlateau reducing learning rate to 3.399160341359675e-05.\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 8s 123ms/step - loss: 2.8308 - categorical_accuracy: 0.1531 - val_loss: 2.9354 - val_categorical_accuracy: 0.1348\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 8s 123ms/step - loss: 2.8285 - categorical_accuracy: 0.1531 - val_loss: 2.9347 - val_categorical_accuracy: 0.1348\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00007: early stopping\n",
            "loss fold  1 2.5897743701934814\n",
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 2 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 4272 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 1069 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 2670 validated image filenames belonging to 20 classes.\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 10s 147ms/step - loss: 3.1173 - categorical_accuracy: 0.1222 - val_loss: 2.8864 - val_categorical_accuracy: 0.1487\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 8s 126ms/step - loss: 2.7961 - categorical_accuracy: 0.1426 - val_loss: 2.7835 - val_categorical_accuracy: 0.1487\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 8s 124ms/step - loss: 2.6638 - categorical_accuracy: 0.1498 - val_loss: 2.6717 - val_categorical_accuracy: 0.1777\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 2.5986 - categorical_accuracy: 0.1725 - val_loss: 2.6086 - val_categorical_accuracy: 0.1890\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 2.5105 - categorical_accuracy: 0.1987 - val_loss: 2.5811 - val_categorical_accuracy: 0.1993\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 2.4719 - categorical_accuracy: 0.1976 - val_loss: 2.5779 - val_categorical_accuracy: 0.2180\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 8s 123ms/step - loss: 2.3957 - categorical_accuracy: 0.2095 - val_loss: 2.5048 - val_categorical_accuracy: 0.2077\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 2.3178 - categorical_accuracy: 0.2355 - val_loss: 2.3435 - val_categorical_accuracy: 0.2713\n",
            "Epoch 9/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 2.2292 - categorical_accuracy: 0.2694 - val_loss: 2.2073 - val_categorical_accuracy: 0.3040\n",
            "Epoch 10/50\n",
            "67/67 [==============================] - 8s 124ms/step - loss: 2.1274 - categorical_accuracy: 0.2940 - val_loss: 2.0838 - val_categorical_accuracy: 0.3265\n",
            "Epoch 11/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 2.0019 - categorical_accuracy: 0.3120 - val_loss: 2.1043 - val_categorical_accuracy: 0.3237\n",
            "Epoch 12/50\n",
            "67/67 [==============================] - 8s 126ms/step - loss: 1.9286 - categorical_accuracy: 0.3275 - val_loss: 2.1334 - val_categorical_accuracy: 0.3312\n",
            "Epoch 13/50\n",
            "67/67 [==============================] - 8s 125ms/step - loss: 1.8272 - categorical_accuracy: 0.3478 - val_loss: 2.2278 - val_categorical_accuracy: 0.3396\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 3.399160341359675e-05.\n",
            "Epoch 14/50\n",
            "67/67 [==============================] - 8s 123ms/step - loss: 1.6310 - categorical_accuracy: 0.4050 - val_loss: 1.9727 - val_categorical_accuracy: 0.3789\n",
            "Epoch 15/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 1.5638 - categorical_accuracy: 0.4164 - val_loss: 1.9383 - val_categorical_accuracy: 0.3891\n",
            "Epoch 16/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 1.5184 - categorical_accuracy: 0.4328 - val_loss: 1.9170 - val_categorical_accuracy: 0.4088\n",
            "Epoch 17/50\n",
            "67/67 [==============================] - 8s 123ms/step - loss: 1.4558 - categorical_accuracy: 0.4630 - val_loss: 1.9804 - val_categorical_accuracy: 0.4069\n",
            "Epoch 18/50\n",
            "67/67 [==============================] - 8s 123ms/step - loss: 1.3882 - categorical_accuracy: 0.4846 - val_loss: 1.9334 - val_categorical_accuracy: 0.4135\n",
            "Epoch 19/50\n",
            "67/67 [==============================] - 8s 123ms/step - loss: 1.3114 - categorical_accuracy: 0.5208 - val_loss: 2.0012 - val_categorical_accuracy: 0.4238\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Epoch 20/50\n",
            "67/67 [==============================] - 8s 123ms/step - loss: 1.2353 - categorical_accuracy: 0.5405 - val_loss: 2.0929 - val_categorical_accuracy: 0.4275\n",
            "Epoch 21/50\n",
            "67/67 [==============================] - 8s 125ms/step - loss: 1.2038 - categorical_accuracy: 0.5597 - val_loss: 2.0781 - val_categorical_accuracy: 0.4322\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00021: early stopping\n",
            "loss fold  2 1.6855323314666748\n",
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 3 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 4272 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 1069 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 2670 validated image filenames belonging to 20 classes.\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 9s 134ms/step - loss: 3.1254 - categorical_accuracy: 0.1339 - val_loss: 2.9237 - val_categorical_accuracy: 0.1235\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 2.8313 - categorical_accuracy: 0.1543 - val_loss: 2.9206 - val_categorical_accuracy: 0.1235\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 2.7775 - categorical_accuracy: 0.1524 - val_loss: 2.7546 - val_categorical_accuracy: 0.1235\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 2.6744 - categorical_accuracy: 0.1575 - val_loss: 2.7535 - val_categorical_accuracy: 0.1553\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 2.6000 - categorical_accuracy: 0.1826 - val_loss: 2.5972 - val_categorical_accuracy: 0.1703\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 8s 123ms/step - loss: 2.5213 - categorical_accuracy: 0.1994 - val_loss: 2.6184 - val_categorical_accuracy: 0.1590\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 2.4248 - categorical_accuracy: 0.2081 - val_loss: 2.5163 - val_categorical_accuracy: 0.1534\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 2.3607 - categorical_accuracy: 0.2156 - val_loss: 2.2555 - val_categorical_accuracy: 0.2170\n",
            "Epoch 9/50\n",
            "67/67 [==============================] - 8s 125ms/step - loss: 2.3088 - categorical_accuracy: 0.2282 - val_loss: 2.2275 - val_categorical_accuracy: 0.2442\n",
            "Epoch 10/50\n",
            "67/67 [==============================] - 8s 123ms/step - loss: 2.2836 - categorical_accuracy: 0.2306 - val_loss: 2.1700 - val_categorical_accuracy: 0.2217\n",
            "Epoch 11/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 2.2227 - categorical_accuracy: 0.2516 - val_loss: 2.1240 - val_categorical_accuracy: 0.2741\n",
            "Epoch 12/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 2.1101 - categorical_accuracy: 0.2772 - val_loss: 2.0761 - val_categorical_accuracy: 0.2872\n",
            "Epoch 13/50\n",
            "67/67 [==============================] - 8s 123ms/step - loss: 2.0703 - categorical_accuracy: 0.2947 - val_loss: 2.0085 - val_categorical_accuracy: 0.2563\n",
            "Epoch 14/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 2.0271 - categorical_accuracy: 0.2949 - val_loss: 1.9341 - val_categorical_accuracy: 0.2909\n",
            "Epoch 15/50\n",
            "67/67 [==============================] - 8s 123ms/step - loss: 1.9214 - categorical_accuracy: 0.3373 - val_loss: 2.1122 - val_categorical_accuracy: 0.2610\n",
            "Epoch 16/50\n",
            "67/67 [==============================] - 8s 126ms/step - loss: 1.9269 - categorical_accuracy: 0.3315 - val_loss: 1.7782 - val_categorical_accuracy: 0.3274\n",
            "Epoch 17/50\n",
            "67/67 [==============================] - 9s 135ms/step - loss: 1.7716 - categorical_accuracy: 0.3691 - val_loss: 1.6418 - val_categorical_accuracy: 0.3573\n",
            "Epoch 18/50\n",
            "67/67 [==============================] - 8s 123ms/step - loss: 1.6646 - categorical_accuracy: 0.4029 - val_loss: 1.6636 - val_categorical_accuracy: 0.3442\n",
            "Epoch 19/50\n",
            "67/67 [==============================] - 9s 130ms/step - loss: 1.6369 - categorical_accuracy: 0.4099 - val_loss: 1.8826 - val_categorical_accuracy: 0.3658\n",
            "Epoch 20/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 1.6322 - categorical_accuracy: 0.4239 - val_loss: 1.5671 - val_categorical_accuracy: 0.3826\n",
            "Epoch 21/50\n",
            "67/67 [==============================] - 8s 124ms/step - loss: 1.5071 - categorical_accuracy: 0.4628 - val_loss: 1.6838 - val_categorical_accuracy: 0.3667\n",
            "Epoch 22/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 1.3936 - categorical_accuracy: 0.4759 - val_loss: 1.5673 - val_categorical_accuracy: 0.4200\n",
            "Epoch 23/50\n",
            "67/67 [==============================] - 8s 126ms/step - loss: 1.3049 - categorical_accuracy: 0.5044 - val_loss: 1.7096 - val_categorical_accuracy: 0.3957\n",
            "\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 3.399160341359675e-05.\n",
            "Epoch 24/50\n",
            "67/67 [==============================] - 8s 123ms/step - loss: 1.0992 - categorical_accuracy: 0.5550 - val_loss: 1.4793 - val_categorical_accuracy: 0.4312\n",
            "Epoch 25/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 1.0004 - categorical_accuracy: 0.6002 - val_loss: 1.4185 - val_categorical_accuracy: 0.4312\n",
            "Epoch 26/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 0.9279 - categorical_accuracy: 0.6224 - val_loss: 1.4654 - val_categorical_accuracy: 0.4378\n",
            "Epoch 27/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 0.8741 - categorical_accuracy: 0.6430 - val_loss: 1.5119 - val_categorical_accuracy: 0.4350\n",
            "Epoch 28/50\n",
            "67/67 [==============================] - 8s 123ms/step - loss: 0.8330 - categorical_accuracy: 0.6559 - val_loss: 1.4995 - val_categorical_accuracy: 0.4443\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Epoch 29/50\n",
            "67/67 [==============================] - 8s 124ms/step - loss: 0.7741 - categorical_accuracy: 0.6901 - val_loss: 1.5180 - val_categorical_accuracy: 0.4425\n",
            "Epoch 30/50\n",
            "67/67 [==============================] - 8s 123ms/step - loss: 0.7598 - categorical_accuracy: 0.6959 - val_loss: 1.5632 - val_categorical_accuracy: 0.4359\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00030: early stopping\n",
            "loss fold  3 2.100720167160034\n",
            "loss media configurazione:  2.12534228960673\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:smac.intensification.intensification.Intensifier:Wallclock time limit for intensification reached (used: 514.280500 sec, available: 0.000010 sec)\n",
            "INFO:smac.intensification.intensification.Intensifier:Updated estimated cost of incumbent on 1 runs: 0.9169\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 1 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 4272 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 1068 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 2671 validated image filenames belonging to 20 classes.\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 10s 155ms/step - loss: 2.7842 - categorical_accuracy: 0.4771 - val_loss: 1.1162 - val_categorical_accuracy: 0.7350\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 7s 110ms/step - loss: 0.9227 - categorical_accuracy: 0.7355 - val_loss: 0.7198 - val_categorical_accuracy: 0.7949\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.6157 - categorical_accuracy: 0.8148 - val_loss: 0.7571 - val_categorical_accuracy: 0.7940\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.4292 - categorical_accuracy: 0.8673 - val_loss: 0.7639 - val_categorical_accuracy: 0.7968\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 8s 125ms/step - loss: 0.3151 - categorical_accuracy: 0.9038 - val_loss: 0.6298 - val_categorical_accuracy: 0.7884\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.2381 - categorical_accuracy: 0.9260 - val_loss: 0.6631 - val_categorical_accuracy: 0.8277\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 0.1450 - categorical_accuracy: 0.9572 - val_loss: 0.6442 - val_categorical_accuracy: 0.8258\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.0965 - categorical_accuracy: 0.9721 - val_loss: 0.7171 - val_categorical_accuracy: 0.8202\n",
            "\n",
            "Epoch 00008: ReduceLROnPlateau reducing learning rate to 3.7397109554149215e-05.\n",
            "Epoch 9/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.0598 - categorical_accuracy: 0.9810 - val_loss: 0.7035 - val_categorical_accuracy: 0.8399\n",
            "Epoch 10/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.0519 - categorical_accuracy: 0.9848 - val_loss: 0.7020 - val_categorical_accuracy: 0.8390\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00010: early stopping\n",
            "loss fold  1 1.1758543252944946\n",
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 2 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 4272 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 1069 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 2670 validated image filenames belonging to 20 classes.\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 9s 136ms/step - loss: 2.8178 - categorical_accuracy: 0.4838 - val_loss: 1.1644 - val_categorical_accuracy: 0.7194\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 7s 111ms/step - loss: 0.8995 - categorical_accuracy: 0.7313 - val_loss: 1.2199 - val_categorical_accuracy: 0.7661\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.5804 - categorical_accuracy: 0.8237 - val_loss: 0.9754 - val_categorical_accuracy: 0.7858\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.4059 - categorical_accuracy: 0.8792 - val_loss: 1.2242 - val_categorical_accuracy: 0.7933\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.2698 - categorical_accuracy: 0.9183 - val_loss: 1.1323 - val_categorical_accuracy: 0.7923\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 0.2042 - categorical_accuracy: 0.9375 - val_loss: 1.2287 - val_categorical_accuracy: 0.7895\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 3.7397109554149215e-05.\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.1279 - categorical_accuracy: 0.9616 - val_loss: 1.1336 - val_categorical_accuracy: 0.8129\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.0798 - categorical_accuracy: 0.9750 - val_loss: 1.1465 - val_categorical_accuracy: 0.8045\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00008: early stopping\n",
            "loss fold  2 0.5673310160636902\n",
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 3 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 4272 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 1069 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 2670 validated image filenames belonging to 20 classes.\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 9s 130ms/step - loss: 2.8340 - categorical_accuracy: 0.4703 - val_loss: 0.8627 - val_categorical_accuracy: 0.7446\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.9408 - categorical_accuracy: 0.7357 - val_loss: 0.6025 - val_categorical_accuracy: 0.7961\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 9s 141ms/step - loss: 0.6073 - categorical_accuracy: 0.8214 - val_loss: 0.6672 - val_categorical_accuracy: 0.8185\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 8s 127ms/step - loss: 0.4350 - categorical_accuracy: 0.8666 - val_loss: 0.7190 - val_categorical_accuracy: 0.8316\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 0.2901 - categorical_accuracy: 0.9087 - val_loss: 0.5361 - val_categorical_accuracy: 0.8195\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 8s 125ms/step - loss: 0.2094 - categorical_accuracy: 0.9354 - val_loss: 0.8785 - val_categorical_accuracy: 0.8326\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 8s 126ms/step - loss: 0.1402 - categorical_accuracy: 0.9569 - val_loss: 0.9487 - val_categorical_accuracy: 0.8335\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 8s 123ms/step - loss: 0.1193 - categorical_accuracy: 0.9614 - val_loss: 1.1265 - val_categorical_accuracy: 0.8176\n",
            "\n",
            "Epoch 00008: ReduceLROnPlateau reducing learning rate to 3.7397109554149215e-05.\n",
            "Epoch 9/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 0.0679 - categorical_accuracy: 0.9813 - val_loss: 0.9953 - val_categorical_accuracy: 0.8372\n",
            "Epoch 10/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.0474 - categorical_accuracy: 0.9876 - val_loss: 1.0496 - val_categorical_accuracy: 0.8372\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00010: early stopping\n",
            "loss fold  3 0.9815860986709595\n",
            "loss media configurazione:  0.9082571466763815\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:smac.intensification.intensification.Intensifier:Challenger (0.9083) is better than incumbent (0.9169) on 1 runs.\n",
            "INFO:smac.intensification.intensification.Intensifier:Changes in incumbent:\n",
            "INFO:smac.intensification.intensification.Intensifier:  dropout_first_layer : 0.2359697570605495 -> 0.2707914396098712\n",
            "INFO:smac.intensification.intensification.Intensifier:  dropout_second_layer : 0.31255434692892914 -> 0.3890542656838425\n",
            "INFO:smac.intensification.intensification.Intensifier:  freeze_to : 19 -> 17\n",
            "INFO:smac.intensification.intensification.Intensifier:  learning_rate : 0.0003764448929249176 -> 0.00037397108913640265\n",
            "INFO:smac.intensification.intensification.Intensifier:  number_first_layer : 990 -> 346\n",
            "INFO:smac.intensification.intensification.Intensifier:  number_layer : 3 -> 2\n",
            "INFO:smac.intensification.intensification.Intensifier:  number_second_layer : 1009 -> 858\n",
            "INFO:smac.intensification.intensification.Intensifier:Wallclock time limit for intensification reached (used: 256.389873 sec, available: 0.000010 sec)\n",
            "INFO:smac.intensification.intensification.Intensifier:Updated estimated cost of incumbent on 1 runs: 0.9083\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 1 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 4272 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 1068 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 2671 validated image filenames belonging to 20 classes.\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 10s 151ms/step - loss: 3.4632 - categorical_accuracy: 0.4939 - val_loss: 0.7906 - val_categorical_accuracy: 0.7828\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 7s 111ms/step - loss: 1.2049 - categorical_accuracy: 0.7292 - val_loss: 0.5952 - val_categorical_accuracy: 0.7949\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.8426 - categorical_accuracy: 0.7748 - val_loss: 0.5611 - val_categorical_accuracy: 0.8006\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.6608 - categorical_accuracy: 0.8111 - val_loss: 0.4542 - val_categorical_accuracy: 0.8127\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.4786 - categorical_accuracy: 0.8556 - val_loss: 0.5099 - val_categorical_accuracy: 0.8258\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.3890 - categorical_accuracy: 0.8748 - val_loss: 0.5568 - val_categorical_accuracy: 0.8296\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.3131 - categorical_accuracy: 0.8961 - val_loss: 0.3749 - val_categorical_accuracy: 0.8371\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.2459 - categorical_accuracy: 0.9197 - val_loss: 0.5314 - val_categorical_accuracy: 0.8333\n",
            "Epoch 9/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.2259 - categorical_accuracy: 0.9281 - val_loss: 0.3838 - val_categorical_accuracy: 0.8268\n",
            "Epoch 10/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.1899 - categorical_accuracy: 0.9373 - val_loss: 0.4675 - val_categorical_accuracy: 0.8352\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 2.075990341836587e-05.\n",
            "Epoch 11/50\n",
            "67/67 [==============================] - 8s 125ms/step - loss: 0.1484 - categorical_accuracy: 0.9487 - val_loss: 0.4516 - val_categorical_accuracy: 0.8380\n",
            "Epoch 12/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.1419 - categorical_accuracy: 0.9555 - val_loss: 0.4178 - val_categorical_accuracy: 0.8361\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00012: early stopping\n",
            "loss fold  1 0.7653071880340576\n",
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 2 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 4272 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 1069 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 2670 validated image filenames belonging to 20 classes.\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 9s 135ms/step - loss: 3.5978 - categorical_accuracy: 0.4831 - val_loss: 1.0567 - val_categorical_accuracy: 0.7820\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 8s 113ms/step - loss: 1.1577 - categorical_accuracy: 0.7275 - val_loss: 0.7574 - val_categorical_accuracy: 0.7979\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.8584 - categorical_accuracy: 0.7680 - val_loss: 0.8409 - val_categorical_accuracy: 0.8129\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.6575 - categorical_accuracy: 0.8076 - val_loss: 0.7904 - val_categorical_accuracy: 0.8223\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.5068 - categorical_accuracy: 0.8509 - val_loss: 0.6952 - val_categorical_accuracy: 0.8344\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 8s 117ms/step - loss: 0.3866 - categorical_accuracy: 0.8738 - val_loss: 0.7147 - val_categorical_accuracy: 0.8241\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.3362 - categorical_accuracy: 0.8909 - val_loss: 0.7514 - val_categorical_accuracy: 0.8363\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.2776 - categorical_accuracy: 0.9101 - val_loss: 0.6768 - val_categorical_accuracy: 0.8354\n",
            "Epoch 9/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.2333 - categorical_accuracy: 0.9237 - val_loss: 0.6866 - val_categorical_accuracy: 0.8382\n",
            "Epoch 10/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.1707 - categorical_accuracy: 0.9434 - val_loss: 0.7052 - val_categorical_accuracy: 0.8541\n",
            "Epoch 11/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.1562 - categorical_accuracy: 0.9464 - val_loss: 0.7973 - val_categorical_accuracy: 0.8466\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 2.075990341836587e-05.\n",
            "Epoch 12/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.1191 - categorical_accuracy: 0.9621 - val_loss: 0.7605 - val_categorical_accuracy: 0.8513\n",
            "Epoch 13/50\n",
            "67/67 [==============================] - 8s 117ms/step - loss: 0.1210 - categorical_accuracy: 0.9579 - val_loss: 0.7686 - val_categorical_accuracy: 0.8466\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00013: early stopping\n",
            "loss fold  2 1.4959690570831299\n",
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 3 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 4272 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 1069 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 2670 validated image filenames belonging to 20 classes.\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 9s 130ms/step - loss: 3.6585 - categorical_accuracy: 0.4974 - val_loss: 0.5934 - val_categorical_accuracy: 0.7399\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 8s 117ms/step - loss: 1.1643 - categorical_accuracy: 0.7200 - val_loss: 0.6006 - val_categorical_accuracy: 0.7783\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 9s 135ms/step - loss: 0.8198 - categorical_accuracy: 0.7725 - val_loss: 0.4325 - val_categorical_accuracy: 0.7970\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.6103 - categorical_accuracy: 0.8277 - val_loss: 0.4478 - val_categorical_accuracy: 0.8064\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.4951 - categorical_accuracy: 0.8453 - val_loss: 0.4150 - val_categorical_accuracy: 0.8176\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.3889 - categorical_accuracy: 0.8724 - val_loss: 0.3896 - val_categorical_accuracy: 0.8110\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.3162 - categorical_accuracy: 0.8956 - val_loss: 0.3974 - val_categorical_accuracy: 0.8204\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 8s 123ms/step - loss: 0.2761 - categorical_accuracy: 0.9122 - val_loss: 0.3901 - val_categorical_accuracy: 0.8297\n",
            "Epoch 9/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 0.2187 - categorical_accuracy: 0.9253 - val_loss: 0.3667 - val_categorical_accuracy: 0.8176\n",
            "Epoch 10/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 0.1818 - categorical_accuracy: 0.9410 - val_loss: 0.3528 - val_categorical_accuracy: 0.8241\n",
            "Epoch 11/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.1580 - categorical_accuracy: 0.9485 - val_loss: 0.3313 - val_categorical_accuracy: 0.8326\n",
            "Epoch 12/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.1290 - categorical_accuracy: 0.9583 - val_loss: 0.4125 - val_categorical_accuracy: 0.8279\n",
            "Epoch 13/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.1207 - categorical_accuracy: 0.9623 - val_loss: 0.4120 - val_categorical_accuracy: 0.8269\n",
            "Epoch 14/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 0.1117 - categorical_accuracy: 0.9621 - val_loss: 0.3962 - val_categorical_accuracy: 0.8176\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 2.075990341836587e-05.\n",
            "Epoch 15/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.0833 - categorical_accuracy: 0.9731 - val_loss: 0.3939 - val_categorical_accuracy: 0.8213\n",
            "Epoch 16/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.0719 - categorical_accuracy: 0.9789 - val_loss: 0.3951 - val_categorical_accuracy: 0.8195\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00016: early stopping\n",
            "loss fold  3 0.37918925285339355\n",
            "loss media configurazione:  0.8801551659901937\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:smac.intensification.intensification.Intensifier:Challenger (0.8802) is better than incumbent (0.9083) on 1 runs.\n",
            "INFO:smac.intensification.intensification.Intensifier:Changes in incumbent:\n",
            "INFO:smac.intensification.intensification.Intensifier:  dropout_first_layer : 0.2707914396098712 -> 0.22962004322807786\n",
            "INFO:smac.intensification.intensification.Intensifier:  dropout_second_layer : 0.3890542656838425 -> 0.3851676822194797\n",
            "INFO:smac.intensification.intensification.Intensifier:  freeze_to : 17 -> 19\n",
            "INFO:smac.intensification.intensification.Intensifier:  learning_rate : 0.00037397108913640265 -> 0.00020759903165573382\n",
            "INFO:smac.intensification.intensification.Intensifier:  number_first_layer : 346 -> 984\n",
            "INFO:smac.intensification.intensification.Intensifier:  number_second_layer : 858 -> 955\n",
            "INFO:smac.intensification.intensification.Intensifier:Wallclock time limit for intensification reached (used: 356.639481 sec, available: 0.000010 sec)\n",
            "INFO:smac.intensification.intensification.Intensifier:Updated estimated cost of incumbent on 1 runs: 0.8802\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 1 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 4272 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 1068 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 2671 validated image filenames belonging to 20 classes.\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 10s 154ms/step - loss: 2.8251 - categorical_accuracy: 0.5634 - val_loss: 1.3591 - val_categorical_accuracy: 0.7659\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 7s 110ms/step - loss: 0.9493 - categorical_accuracy: 0.7680 - val_loss: 1.1346 - val_categorical_accuracy: 0.7884\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.6413 - categorical_accuracy: 0.8186 - val_loss: 1.0704 - val_categorical_accuracy: 0.7931\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 8s 123ms/step - loss: 0.4566 - categorical_accuracy: 0.8603 - val_loss: 1.0972 - val_categorical_accuracy: 0.7968\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.3485 - categorical_accuracy: 0.8909 - val_loss: 1.0145 - val_categorical_accuracy: 0.8099\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.2692 - categorical_accuracy: 0.9129 - val_loss: 0.9647 - val_categorical_accuracy: 0.8071\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.2238 - categorical_accuracy: 0.9258 - val_loss: 1.0301 - val_categorical_accuracy: 0.8146\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.1639 - categorical_accuracy: 0.9464 - val_loss: 1.1386 - val_categorical_accuracy: 0.8052\n",
            "Epoch 9/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.1223 - categorical_accuracy: 0.9593 - val_loss: 1.1706 - val_categorical_accuracy: 0.8184\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 2.709298278205097e-05.\n",
            "Epoch 10/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.1066 - categorical_accuracy: 0.9651 - val_loss: 1.1826 - val_categorical_accuracy: 0.8155\n",
            "Epoch 11/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 0.0883 - categorical_accuracy: 0.9703 - val_loss: 1.1608 - val_categorical_accuracy: 0.8184\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00011: early stopping\n",
            "loss fold  1 1.4597409963607788\n",
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 2 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 4272 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 1069 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 2670 validated image filenames belonging to 20 classes.\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 9s 136ms/step - loss: 3.0408 - categorical_accuracy: 0.5459 - val_loss: 0.6448 - val_categorical_accuracy: 0.7661\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 8s 112ms/step - loss: 0.9097 - categorical_accuracy: 0.7697 - val_loss: 0.3349 - val_categorical_accuracy: 0.7830\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.6373 - categorical_accuracy: 0.8165 - val_loss: 0.2536 - val_categorical_accuracy: 0.8138\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.4465 - categorical_accuracy: 0.8614 - val_loss: 0.2687 - val_categorical_accuracy: 0.8204\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.3519 - categorical_accuracy: 0.8858 - val_loss: 0.2610 - val_categorical_accuracy: 0.8129\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.2908 - categorical_accuracy: 0.9064 - val_loss: 0.1827 - val_categorical_accuracy: 0.8167\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.2270 - categorical_accuracy: 0.9239 - val_loss: 0.2092 - val_categorical_accuracy: 0.8260\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.1716 - categorical_accuracy: 0.9441 - val_loss: 0.1815 - val_categorical_accuracy: 0.8307\n",
            "Epoch 9/50\n",
            "67/67 [==============================] - 8s 123ms/step - loss: 0.1428 - categorical_accuracy: 0.9532 - val_loss: 0.2359 - val_categorical_accuracy: 0.8316\n",
            "Epoch 10/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.1116 - categorical_accuracy: 0.9677 - val_loss: 0.2639 - val_categorical_accuracy: 0.8185\n",
            "Epoch 11/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 0.0880 - categorical_accuracy: 0.9707 - val_loss: 0.1843 - val_categorical_accuracy: 0.8232\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 2.709298278205097e-05.\n",
            "Epoch 12/50\n",
            "67/67 [==============================] - 9s 136ms/step - loss: 0.0725 - categorical_accuracy: 0.9803 - val_loss: 0.1748 - val_categorical_accuracy: 0.8223\n",
            "Epoch 13/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.0699 - categorical_accuracy: 0.9780 - val_loss: 0.1820 - val_categorical_accuracy: 0.8195\n",
            "Epoch 14/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.0615 - categorical_accuracy: 0.9829 - val_loss: 0.1838 - val_categorical_accuracy: 0.8269\n",
            "Epoch 15/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.0655 - categorical_accuracy: 0.9787 - val_loss: 0.1671 - val_categorical_accuracy: 0.8316\n",
            "Epoch 16/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.0590 - categorical_accuracy: 0.9841 - val_loss: 0.1862 - val_categorical_accuracy: 0.8316\n",
            "Epoch 17/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.0576 - categorical_accuracy: 0.9817 - val_loss: 0.2043 - val_categorical_accuracy: 0.8316\n",
            "Epoch 18/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.0591 - categorical_accuracy: 0.9820 - val_loss: 0.2081 - val_categorical_accuracy: 0.8344\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Epoch 19/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.0519 - categorical_accuracy: 0.9850 - val_loss: 0.2032 - val_categorical_accuracy: 0.8335\n",
            "Epoch 20/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.0491 - categorical_accuracy: 0.9855 - val_loss: 0.1897 - val_categorical_accuracy: 0.8307\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00020: early stopping\n",
            "loss fold  2 0.9927036166191101\n",
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 3 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 4272 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 1069 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 2670 validated image filenames belonging to 20 classes.\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 9s 130ms/step - loss: 2.9482 - categorical_accuracy: 0.5365 - val_loss: 0.7200 - val_categorical_accuracy: 0.7811\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 8s 117ms/step - loss: 0.9252 - categorical_accuracy: 0.7647 - val_loss: 0.7041 - val_categorical_accuracy: 0.8036\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.6329 - categorical_accuracy: 0.8198 - val_loss: 0.6872 - val_categorical_accuracy: 0.8167\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 8s 117ms/step - loss: 0.4555 - categorical_accuracy: 0.8535 - val_loss: 0.7764 - val_categorical_accuracy: 0.8138\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.3325 - categorical_accuracy: 0.8883 - val_loss: 0.6392 - val_categorical_accuracy: 0.8185\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.2733 - categorical_accuracy: 0.9087 - val_loss: 0.6026 - val_categorical_accuracy: 0.8185\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.2300 - categorical_accuracy: 0.9202 - val_loss: 0.6070 - val_categorical_accuracy: 0.8223\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.1754 - categorical_accuracy: 0.9436 - val_loss: 0.6528 - val_categorical_accuracy: 0.8269\n",
            "Epoch 9/50\n",
            "67/67 [==============================] - 8s 126ms/step - loss: 0.1366 - categorical_accuracy: 0.9565 - val_loss: 0.6504 - val_categorical_accuracy: 0.8223\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 2.709298278205097e-05.\n",
            "Epoch 10/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.1103 - categorical_accuracy: 0.9632 - val_loss: 0.5719 - val_categorical_accuracy: 0.8307\n",
            "Epoch 11/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.0933 - categorical_accuracy: 0.9700 - val_loss: 0.5659 - val_categorical_accuracy: 0.8288\n",
            "Epoch 12/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.0828 - categorical_accuracy: 0.9724 - val_loss: 0.5710 - val_categorical_accuracy: 0.8297\n",
            "Epoch 13/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.0889 - categorical_accuracy: 0.9705 - val_loss: 0.5756 - val_categorical_accuracy: 0.8288\n",
            "Epoch 14/50\n",
            "67/67 [==============================] - 8s 117ms/step - loss: 0.0893 - categorical_accuracy: 0.9738 - val_loss: 0.5859 - val_categorical_accuracy: 0.8307\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Epoch 15/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.0782 - categorical_accuracy: 0.9771 - val_loss: 0.5806 - val_categorical_accuracy: 0.8307\n",
            "Epoch 16/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.0798 - categorical_accuracy: 0.9768 - val_loss: 0.5813 - val_categorical_accuracy: 0.8326\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00016: early stopping\n",
            "loss fold  3 0.6682411432266235\n",
            "loss media configurazione:  1.040228585402171\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:smac.intensification.intensification.Intensifier:Wallclock time limit for intensification reached (used: 405.558988 sec, available: 0.000010 sec)\n",
            "INFO:smac.intensification.intensification.Intensifier:Updated estimated cost of incumbent on 1 runs: 0.8802\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 1 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 4272 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 1068 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 2671 validated image filenames belonging to 20 classes.\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 10s 153ms/step - loss: 3.7059 - categorical_accuracy: 0.2135 - val_loss: 1.9405 - val_categorical_accuracy: 0.4738\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 7s 111ms/step - loss: 1.8949 - categorical_accuracy: 0.4551 - val_loss: 0.9619 - val_categorical_accuracy: 0.6639\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 1.4501 - categorical_accuracy: 0.5880 - val_loss: 0.7909 - val_categorical_accuracy: 0.7537\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 1.1582 - categorical_accuracy: 0.6653 - val_loss: 0.6844 - val_categorical_accuracy: 0.7884\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 1.0075 - categorical_accuracy: 0.7067 - val_loss: 0.7536 - val_categorical_accuracy: 0.7734\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.8600 - categorical_accuracy: 0.7355 - val_loss: 0.5522 - val_categorical_accuracy: 0.8071\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.7582 - categorical_accuracy: 0.7671 - val_loss: 0.5436 - val_categorical_accuracy: 0.8155\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.6888 - categorical_accuracy: 0.7823 - val_loss: 0.5224 - val_categorical_accuracy: 0.8146\n",
            "Epoch 9/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.5860 - categorical_accuracy: 0.8209 - val_loss: 0.5605 - val_categorical_accuracy: 0.8202\n",
            "Epoch 10/50\n",
            "67/67 [==============================] - 8s 126ms/step - loss: 0.5516 - categorical_accuracy: 0.8235 - val_loss: 0.5740 - val_categorical_accuracy: 0.8277\n",
            "Epoch 11/50\n",
            "67/67 [==============================] - 8s 123ms/step - loss: 0.4593 - categorical_accuracy: 0.8460 - val_loss: 0.4869 - val_categorical_accuracy: 0.8343\n",
            "Epoch 12/50\n",
            "67/67 [==============================] - 9s 140ms/step - loss: 0.4289 - categorical_accuracy: 0.8635 - val_loss: 0.4060 - val_categorical_accuracy: 0.8361\n",
            "Epoch 13/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 0.4026 - categorical_accuracy: 0.8717 - val_loss: 0.4533 - val_categorical_accuracy: 0.8390\n",
            "Epoch 14/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 0.3516 - categorical_accuracy: 0.8886 - val_loss: 0.4076 - val_categorical_accuracy: 0.8446\n",
            "Epoch 15/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 0.3243 - categorical_accuracy: 0.9019 - val_loss: 0.4617 - val_categorical_accuracy: 0.8333\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 3.7177201011218135e-05.\n",
            "Epoch 16/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.2714 - categorical_accuracy: 0.9082 - val_loss: 0.4359 - val_categorical_accuracy: 0.8371\n",
            "Epoch 17/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 0.2681 - categorical_accuracy: 0.9127 - val_loss: 0.4171 - val_categorical_accuracy: 0.8371\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00017: early stopping\n",
            "loss fold  1 1.3312621116638184\n",
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 2 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 4272 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 1069 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 2670 validated image filenames belonging to 20 classes.\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 9s 138ms/step - loss: 3.9783 - categorical_accuracy: 0.1868 - val_loss: 2.2100 - val_categorical_accuracy: 0.4359\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 8s 116ms/step - loss: 1.9953 - categorical_accuracy: 0.4258 - val_loss: 1.2849 - val_categorical_accuracy: 0.6791\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 1.4958 - categorical_accuracy: 0.5665 - val_loss: 1.1018 - val_categorical_accuracy: 0.7250\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 1.2047 - categorical_accuracy: 0.6444 - val_loss: 0.9318 - val_categorical_accuracy: 0.7680\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 1.0269 - categorical_accuracy: 0.6936 - val_loss: 0.9136 - val_categorical_accuracy: 0.7830\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.8968 - categorical_accuracy: 0.7343 - val_loss: 0.8132 - val_categorical_accuracy: 0.8082\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.7954 - categorical_accuracy: 0.7549 - val_loss: 0.8003 - val_categorical_accuracy: 0.7989\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.6880 - categorical_accuracy: 0.7853 - val_loss: 0.7858 - val_categorical_accuracy: 0.7979\n",
            "Epoch 9/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.6149 - categorical_accuracy: 0.8172 - val_loss: 0.8089 - val_categorical_accuracy: 0.7970\n",
            "Epoch 10/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.5559 - categorical_accuracy: 0.8270 - val_loss: 0.7028 - val_categorical_accuracy: 0.8073\n",
            "Epoch 11/50\n",
            "67/67 [==============================] - 8s 127ms/step - loss: 0.4864 - categorical_accuracy: 0.8418 - val_loss: 0.7921 - val_categorical_accuracy: 0.8073\n",
            "Epoch 12/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 0.4544 - categorical_accuracy: 0.8593 - val_loss: 0.7780 - val_categorical_accuracy: 0.8110\n",
            "Epoch 13/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.4309 - categorical_accuracy: 0.8666 - val_loss: 0.7375 - val_categorical_accuracy: 0.8148\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 3.7177201011218135e-05.\n",
            "Epoch 14/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.3584 - categorical_accuracy: 0.8865 - val_loss: 0.7367 - val_categorical_accuracy: 0.8101\n",
            "Epoch 15/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.3178 - categorical_accuracy: 0.8970 - val_loss: 0.7339 - val_categorical_accuracy: 0.8120\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00015: early stopping\n",
            "loss fold  2 0.5989149212837219\n",
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 3 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 4272 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 1069 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 2670 validated image filenames belonging to 20 classes.\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 9s 129ms/step - loss: 3.8324 - categorical_accuracy: 0.1948 - val_loss: 1.8792 - val_categorical_accuracy: 0.5023\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 2.0079 - categorical_accuracy: 0.4190 - val_loss: 0.9438 - val_categorical_accuracy: 0.6876\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 8s 117ms/step - loss: 1.5383 - categorical_accuracy: 0.5595 - val_loss: 0.6941 - val_categorical_accuracy: 0.7390\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 1.2429 - categorical_accuracy: 0.6419 - val_loss: 0.4964 - val_categorical_accuracy: 0.7802\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 1.0190 - categorical_accuracy: 0.6978 - val_loss: 0.5158 - val_categorical_accuracy: 0.7905\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 0.8597 - categorical_accuracy: 0.7416 - val_loss: 0.4540 - val_categorical_accuracy: 0.7989\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.7665 - categorical_accuracy: 0.7713 - val_loss: 0.3794 - val_categorical_accuracy: 0.8017\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 0.6873 - categorical_accuracy: 0.7900 - val_loss: 0.3422 - val_categorical_accuracy: 0.8129\n",
            "Epoch 9/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.5748 - categorical_accuracy: 0.8237 - val_loss: 0.3259 - val_categorical_accuracy: 0.8092\n",
            "Epoch 10/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.5485 - categorical_accuracy: 0.8282 - val_loss: 0.3489 - val_categorical_accuracy: 0.8232\n",
            "Epoch 11/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.4725 - categorical_accuracy: 0.8535 - val_loss: 0.3160 - val_categorical_accuracy: 0.8185\n",
            "Epoch 12/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.4198 - categorical_accuracy: 0.8663 - val_loss: 0.3061 - val_categorical_accuracy: 0.8138\n",
            "Epoch 13/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.3930 - categorical_accuracy: 0.8724 - val_loss: 0.2608 - val_categorical_accuracy: 0.8251\n",
            "Epoch 14/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.3607 - categorical_accuracy: 0.8823 - val_loss: 0.2187 - val_categorical_accuracy: 0.8251\n",
            "Epoch 15/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.3243 - categorical_accuracy: 0.8935 - val_loss: 0.2803 - val_categorical_accuracy: 0.8307\n",
            "Epoch 16/50\n",
            "67/67 [==============================] - 9s 137ms/step - loss: 0.2999 - categorical_accuracy: 0.9024 - val_loss: 0.2920 - val_categorical_accuracy: 0.8251\n",
            "Epoch 17/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 0.2629 - categorical_accuracy: 0.9120 - val_loss: 0.2606 - val_categorical_accuracy: 0.8269\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.7177201011218135e-05.\n",
            "Epoch 18/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.2267 - categorical_accuracy: 0.9244 - val_loss: 0.2377 - val_categorical_accuracy: 0.8279\n",
            "Epoch 19/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.2075 - categorical_accuracy: 0.9338 - val_loss: 0.2369 - val_categorical_accuracy: 0.8232\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00019: early stopping\n",
            "loss fold  3 0.9557542204856873\n",
            "loss media configurazione:  0.9619770844777426\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:smac.intensification.intensification.Intensifier:Wallclock time limit for intensification reached (used: 441.894774 sec, available: 0.000010 sec)\n",
            "INFO:smac.intensification.intensification.Intensifier:Updated estimated cost of incumbent on 1 runs: 0.8802\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 1 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 4272 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 1068 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 2671 validated image filenames belonging to 20 classes.\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 10s 156ms/step - loss: 2.6379 - categorical_accuracy: 0.3312 - val_loss: 1.6913 - val_categorical_accuracy: 0.4101\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 7s 112ms/step - loss: 2.2837 - categorical_accuracy: 0.3427 - val_loss: 1.5143 - val_categorical_accuracy: 0.4504\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 2.0530 - categorical_accuracy: 0.4157 - val_loss: 1.6890 - val_categorical_accuracy: 0.5094\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 1.9949 - categorical_accuracy: 0.4288 - val_loss: 1.4372 - val_categorical_accuracy: 0.5103\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 1.7631 - categorical_accuracy: 0.4853 - val_loss: 1.1698 - val_categorical_accuracy: 0.5337\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 1.6325 - categorical_accuracy: 0.5171 - val_loss: 1.6305 - val_categorical_accuracy: 0.5552\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 1.4910 - categorical_accuracy: 0.5637 - val_loss: 1.1670 - val_categorical_accuracy: 0.5946\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 1.3464 - categorical_accuracy: 0.5960 - val_loss: 1.3707 - val_categorical_accuracy: 0.6039\n",
            "Epoch 9/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 1.2614 - categorical_accuracy: 0.6131 - val_loss: 1.4930 - val_categorical_accuracy: 0.6348\n",
            "Epoch 10/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 1.2497 - categorical_accuracy: 0.6203 - val_loss: 1.1908 - val_categorical_accuracy: 0.6554\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 9.365707519464195e-05.\n",
            "Epoch 11/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 1.0062 - categorical_accuracy: 0.6791 - val_loss: 1.1165 - val_categorical_accuracy: 0.6966\n",
            "Epoch 12/50\n",
            "67/67 [==============================] - 8s 123ms/step - loss: 0.8969 - categorical_accuracy: 0.7177 - val_loss: 1.0657 - val_categorical_accuracy: 0.7228\n",
            "Epoch 13/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 0.8432 - categorical_accuracy: 0.7257 - val_loss: 1.0776 - val_categorical_accuracy: 0.7313\n",
            "Epoch 14/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.7870 - categorical_accuracy: 0.7486 - val_loss: 1.0404 - val_categorical_accuracy: 0.7463\n",
            "Epoch 15/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 0.7660 - categorical_accuracy: 0.7514 - val_loss: 1.0834 - val_categorical_accuracy: 0.7640\n",
            "Epoch 16/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.7004 - categorical_accuracy: 0.7718 - val_loss: 0.9547 - val_categorical_accuracy: 0.7734\n",
            "Epoch 17/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.6959 - categorical_accuracy: 0.7823 - val_loss: 1.0481 - val_categorical_accuracy: 0.7734\n",
            "Epoch 18/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.6322 - categorical_accuracy: 0.7956 - val_loss: 1.0169 - val_categorical_accuracy: 0.7781\n",
            "Epoch 19/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.5886 - categorical_accuracy: 0.8092 - val_loss: 1.1410 - val_categorical_accuracy: 0.7781\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Epoch 20/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.5408 - categorical_accuracy: 0.8202 - val_loss: 1.1187 - val_categorical_accuracy: 0.7809\n",
            "Epoch 21/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.5379 - categorical_accuracy: 0.8272 - val_loss: 1.1247 - val_categorical_accuracy: 0.7809\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00021: early stopping\n",
            "loss fold  1 1.2246896028518677\n",
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 2 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 4272 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 1069 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 2670 validated image filenames belonging to 20 classes.\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 9s 138ms/step - loss: 2.8077 - categorical_accuracy: 0.3048 - val_loss: 2.1120 - val_categorical_accuracy: 0.4191\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 7s 111ms/step - loss: 2.1384 - categorical_accuracy: 0.3944 - val_loss: 1.9901 - val_categorical_accuracy: 0.5033\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 8s 123ms/step - loss: 2.0075 - categorical_accuracy: 0.4345 - val_loss: 1.6935 - val_categorical_accuracy: 0.5379\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 1.8705 - categorical_accuracy: 0.4572 - val_loss: 1.8445 - val_categorical_accuracy: 0.5370\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 1.7269 - categorical_accuracy: 0.4956 - val_loss: 1.4030 - val_categorical_accuracy: 0.5856\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 1.5214 - categorical_accuracy: 0.5644 - val_loss: 1.3266 - val_categorical_accuracy: 0.5884\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 1.4305 - categorical_accuracy: 0.5630 - val_loss: 0.9553 - val_categorical_accuracy: 0.6595\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 1.3052 - categorical_accuracy: 0.5993 - val_loss: 0.9102 - val_categorical_accuracy: 0.6548\n",
            "Epoch 9/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 1.1260 - categorical_accuracy: 0.6498 - val_loss: 0.8657 - val_categorical_accuracy: 0.6548\n",
            "Epoch 10/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 1.0027 - categorical_accuracy: 0.6894 - val_loss: 0.8937 - val_categorical_accuracy: 0.6791\n",
            "Epoch 11/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.8914 - categorical_accuracy: 0.7163 - val_loss: 1.0741 - val_categorical_accuracy: 0.6894\n",
            "Epoch 12/50\n",
            "67/67 [==============================] - 9s 139ms/step - loss: 0.7511 - categorical_accuracy: 0.7549 - val_loss: 1.2727 - val_categorical_accuracy: 0.7390\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 9.365707519464195e-05.\n",
            "Epoch 13/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 0.5714 - categorical_accuracy: 0.8167 - val_loss: 0.7184 - val_categorical_accuracy: 0.7802\n",
            "Epoch 14/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.5034 - categorical_accuracy: 0.8425 - val_loss: 0.7235 - val_categorical_accuracy: 0.7867\n",
            "Epoch 15/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.4420 - categorical_accuracy: 0.8574 - val_loss: 0.7158 - val_categorical_accuracy: 0.7933\n",
            "Epoch 16/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.4037 - categorical_accuracy: 0.8635 - val_loss: 0.6806 - val_categorical_accuracy: 0.7979\n",
            "Epoch 17/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.3934 - categorical_accuracy: 0.8727 - val_loss: 0.7216 - val_categorical_accuracy: 0.8073\n",
            "Epoch 18/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.3506 - categorical_accuracy: 0.8874 - val_loss: 0.7051 - val_categorical_accuracy: 0.8110\n",
            "Epoch 19/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.3395 - categorical_accuracy: 0.8874 - val_loss: 0.7150 - val_categorical_accuracy: 0.8092\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Epoch 20/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.3211 - categorical_accuracy: 0.8951 - val_loss: 0.7204 - val_categorical_accuracy: 0.8092\n",
            "Epoch 21/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.3248 - categorical_accuracy: 0.8923 - val_loss: 0.7117 - val_categorical_accuracy: 0.8110\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00021: early stopping\n",
            "loss fold  2 3.062286615371704\n",
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 3 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 4272 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 1069 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 2670 validated image filenames belonging to 20 classes.\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 9s 133ms/step - loss: 2.8577 - categorical_accuracy: 0.2699 - val_loss: 2.3102 - val_categorical_accuracy: 0.3938\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 2.4020 - categorical_accuracy: 0.3301 - val_loss: 2.2171 - val_categorical_accuracy: 0.3480\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 8s 123ms/step - loss: 2.2113 - categorical_accuracy: 0.3872 - val_loss: 2.0346 - val_categorical_accuracy: 0.4911\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 2.1223 - categorical_accuracy: 0.4010 - val_loss: 2.0123 - val_categorical_accuracy: 0.4350\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 2.0958 - categorical_accuracy: 0.4223 - val_loss: 2.2202 - val_categorical_accuracy: 0.5304\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 9s 128ms/step - loss: 1.8812 - categorical_accuracy: 0.4733 - val_loss: 1.8296 - val_categorical_accuracy: 0.5341\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 1.7802 - categorical_accuracy: 0.4991 - val_loss: 1.8380 - val_categorical_accuracy: 0.5285\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 1.7270 - categorical_accuracy: 0.5059 - val_loss: 1.5278 - val_categorical_accuracy: 0.5388\n",
            "Epoch 9/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 1.7261 - categorical_accuracy: 0.5096 - val_loss: 1.6495 - val_categorical_accuracy: 0.5613\n",
            "Epoch 10/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 1.6673 - categorical_accuracy: 0.5239 - val_loss: 1.5624 - val_categorical_accuracy: 0.5809\n",
            "Epoch 11/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 1.6033 - categorical_accuracy: 0.5321 - val_loss: 1.4150 - val_categorical_accuracy: 0.5772\n",
            "Epoch 12/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 1.6189 - categorical_accuracy: 0.5302 - val_loss: 1.7407 - val_categorical_accuracy: 0.5444\n",
            "Epoch 13/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 1.4903 - categorical_accuracy: 0.5513 - val_loss: 1.3707 - val_categorical_accuracy: 0.5781\n",
            "Epoch 14/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 1.4039 - categorical_accuracy: 0.5726 - val_loss: 1.5034 - val_categorical_accuracy: 0.5949\n",
            "Epoch 15/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 1.2783 - categorical_accuracy: 0.5950 - val_loss: 1.2135 - val_categorical_accuracy: 0.6043\n",
            "Epoch 16/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 1.1773 - categorical_accuracy: 0.6271 - val_loss: 1.2327 - val_categorical_accuracy: 0.5847\n",
            "Epoch 17/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 1.1601 - categorical_accuracy: 0.6320 - val_loss: 1.1086 - val_categorical_accuracy: 0.6333\n",
            "Epoch 18/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 1.0486 - categorical_accuracy: 0.6657 - val_loss: 1.2903 - val_categorical_accuracy: 0.6305\n",
            "Epoch 19/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 1.0229 - categorical_accuracy: 0.6749 - val_loss: 1.1771 - val_categorical_accuracy: 0.6558\n",
            "Epoch 20/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 0.9378 - categorical_accuracy: 0.6917 - val_loss: 0.8437 - val_categorical_accuracy: 0.6688\n",
            "Epoch 21/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 0.7507 - categorical_accuracy: 0.7446 - val_loss: 0.9769 - val_categorical_accuracy: 0.7278\n",
            "Epoch 22/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.6206 - categorical_accuracy: 0.7865 - val_loss: 0.9063 - val_categorical_accuracy: 0.7063\n",
            "Epoch 23/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.5940 - categorical_accuracy: 0.8031 - val_loss: 0.7462 - val_categorical_accuracy: 0.7250\n",
            "Epoch 24/50\n",
            "67/67 [==============================] - 8s 117ms/step - loss: 0.5370 - categorical_accuracy: 0.8336 - val_loss: 0.8432 - val_categorical_accuracy: 0.7297\n",
            "Epoch 25/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.4484 - categorical_accuracy: 0.8596 - val_loss: 0.7355 - val_categorical_accuracy: 0.7287\n",
            "Epoch 26/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.3846 - categorical_accuracy: 0.8750 - val_loss: 1.0147 - val_categorical_accuracy: 0.7212\n",
            "Epoch 27/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.3746 - categorical_accuracy: 0.8816 - val_loss: 0.8260 - val_categorical_accuracy: 0.7418\n",
            "Epoch 28/50\n",
            "67/67 [==============================] - 9s 131ms/step - loss: 0.3022 - categorical_accuracy: 0.9026 - val_loss: 1.0313 - val_categorical_accuracy: 0.7399\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 9.365707519464195e-05.\n",
            "Epoch 29/50\n",
            "67/67 [==============================] - 8s 126ms/step - loss: 0.2344 - categorical_accuracy: 0.9160 - val_loss: 0.9654 - val_categorical_accuracy: 0.7755\n",
            "Epoch 30/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.1737 - categorical_accuracy: 0.9373 - val_loss: 0.9853 - val_categorical_accuracy: 0.7699\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00030: early stopping\n",
            "loss fold  3 1.4141596555709839\n",
            "loss media configurazione:  1.9003786245981853\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:smac.intensification.intensification.Intensifier:Wallclock time limit for intensification reached (used: 610.364038 sec, available: 0.000010 sec)\n",
            "INFO:smac.intensification.intensification.Intensifier:Updated estimated cost of incumbent on 1 runs: 0.8802\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 1 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 4272 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 1068 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 2671 validated image filenames belonging to 20 classes.\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 10s 154ms/step - loss: 4.7733 - categorical_accuracy: 0.2697 - val_loss: 1.2196 - val_categorical_accuracy: 0.6348\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 7s 111ms/step - loss: 1.8391 - categorical_accuracy: 0.5023 - val_loss: 0.7555 - val_categorical_accuracy: 0.7257\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 1.4293 - categorical_accuracy: 0.6021 - val_loss: 0.5850 - val_categorical_accuracy: 0.7743\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 1.1410 - categorical_accuracy: 0.6697 - val_loss: 0.4991 - val_categorical_accuracy: 0.7809\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 1.0351 - categorical_accuracy: 0.7001 - val_loss: 0.4844 - val_categorical_accuracy: 0.7809\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.8796 - categorical_accuracy: 0.7299 - val_loss: 0.4449 - val_categorical_accuracy: 0.7987\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.8025 - categorical_accuracy: 0.7617 - val_loss: 0.3870 - val_categorical_accuracy: 0.8212\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 8s 117ms/step - loss: 0.7138 - categorical_accuracy: 0.7711 - val_loss: 0.3910 - val_categorical_accuracy: 0.8099\n",
            "Epoch 9/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 0.6540 - categorical_accuracy: 0.7992 - val_loss: 0.4042 - val_categorical_accuracy: 0.8137\n",
            "Epoch 10/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.5768 - categorical_accuracy: 0.8125 - val_loss: 0.4228 - val_categorical_accuracy: 0.8202\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.948945428011939e-05.\n",
            "Epoch 11/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.5149 - categorical_accuracy: 0.8359 - val_loss: 0.4159 - val_categorical_accuracy: 0.8184\n",
            "Epoch 12/50\n",
            "67/67 [==============================] - 8s 124ms/step - loss: 0.5026 - categorical_accuracy: 0.8373 - val_loss: 0.4028 - val_categorical_accuracy: 0.8221\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00012: early stopping\n",
            "loss fold  1 0.7211402654647827\n",
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 2 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 4272 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 1069 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 2670 validated image filenames belonging to 20 classes.\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 9s 137ms/step - loss: 4.4207 - categorical_accuracy: 0.2783 - val_loss: 1.0923 - val_categorical_accuracy: 0.6866\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 7s 111ms/step - loss: 1.7834 - categorical_accuracy: 0.5199 - val_loss: 0.7900 - val_categorical_accuracy: 0.7343\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 1.3649 - categorical_accuracy: 0.6056 - val_loss: 0.6311 - val_categorical_accuracy: 0.7699\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 1.1233 - categorical_accuracy: 0.6788 - val_loss: 0.5558 - val_categorical_accuracy: 0.7933\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 1.0044 - categorical_accuracy: 0.7072 - val_loss: 0.5468 - val_categorical_accuracy: 0.7979\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 8s 123ms/step - loss: 0.8288 - categorical_accuracy: 0.7460 - val_loss: 0.5270 - val_categorical_accuracy: 0.8185\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.7803 - categorical_accuracy: 0.7629 - val_loss: 0.4843 - val_categorical_accuracy: 0.8185\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.7203 - categorical_accuracy: 0.7804 - val_loss: 0.4553 - val_categorical_accuracy: 0.8195\n",
            "Epoch 9/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.6507 - categorical_accuracy: 0.8013 - val_loss: 0.4338 - val_categorical_accuracy: 0.8316\n",
            "Epoch 10/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.6017 - categorical_accuracy: 0.8085 - val_loss: 0.4351 - val_categorical_accuracy: 0.8326\n",
            "Epoch 11/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.5092 - categorical_accuracy: 0.8345 - val_loss: 0.4187 - val_categorical_accuracy: 0.8241\n",
            "Epoch 12/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.4617 - categorical_accuracy: 0.8514 - val_loss: 0.4130 - val_categorical_accuracy: 0.8269\n",
            "Epoch 13/50\n",
            "67/67 [==============================] - 8s 117ms/step - loss: 0.4554 - categorical_accuracy: 0.8511 - val_loss: 0.4124 - val_categorical_accuracy: 0.8335\n",
            "Epoch 14/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.4019 - categorical_accuracy: 0.8656 - val_loss: 0.3944 - val_categorical_accuracy: 0.8382\n",
            "Epoch 15/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.3796 - categorical_accuracy: 0.8766 - val_loss: 0.4053 - val_categorical_accuracy: 0.8447\n",
            "Epoch 16/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.3479 - categorical_accuracy: 0.8879 - val_loss: 0.4027 - val_categorical_accuracy: 0.8382\n",
            "Epoch 17/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.3038 - categorical_accuracy: 0.8984 - val_loss: 0.4202 - val_categorical_accuracy: 0.8457\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.948945428011939e-05.\n",
            "Epoch 18/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.2845 - categorical_accuracy: 0.9078 - val_loss: 0.4097 - val_categorical_accuracy: 0.8466\n",
            "Epoch 19/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.2525 - categorical_accuracy: 0.9146 - val_loss: 0.4170 - val_categorical_accuracy: 0.8475\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00019: early stopping\n",
            "loss fold  2 1.1041747331619263\n",
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 3 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 4272 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 1069 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 2670 validated image filenames belonging to 20 classes.\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 9s 129ms/step - loss: 4.4449 - categorical_accuracy: 0.3034 - val_loss: 1.1601 - val_categorical_accuracy: 0.6595\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 9s 131ms/step - loss: 1.7844 - categorical_accuracy: 0.5403 - val_loss: 0.8943 - val_categorical_accuracy: 0.7156\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 8s 123ms/step - loss: 1.3657 - categorical_accuracy: 0.6217 - val_loss: 0.8154 - val_categorical_accuracy: 0.7493\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 1.1232 - categorical_accuracy: 0.6728 - val_loss: 0.6441 - val_categorical_accuracy: 0.7792\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.9866 - categorical_accuracy: 0.7111 - val_loss: 0.6383 - val_categorical_accuracy: 0.7792\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.8691 - categorical_accuracy: 0.7470 - val_loss: 0.5738 - val_categorical_accuracy: 0.7905\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 8s 117ms/step - loss: 0.7474 - categorical_accuracy: 0.7800 - val_loss: 0.5862 - val_categorical_accuracy: 0.7961\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.6931 - categorical_accuracy: 0.7947 - val_loss: 0.5282 - val_categorical_accuracy: 0.8129\n",
            "Epoch 9/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.6382 - categorical_accuracy: 0.8008 - val_loss: 0.5678 - val_categorical_accuracy: 0.8026\n",
            "Epoch 10/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.5773 - categorical_accuracy: 0.8198 - val_loss: 0.5183 - val_categorical_accuracy: 0.7989\n",
            "Epoch 11/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.5147 - categorical_accuracy: 0.8340 - val_loss: 0.5777 - val_categorical_accuracy: 0.8082\n",
            "Epoch 12/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.4792 - categorical_accuracy: 0.8493 - val_loss: 0.5815 - val_categorical_accuracy: 0.8138\n",
            "Epoch 13/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.4429 - categorical_accuracy: 0.8574 - val_loss: 0.6207 - val_categorical_accuracy: 0.8036\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.948945428011939e-05.\n",
            "Epoch 14/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.4120 - categorical_accuracy: 0.8694 - val_loss: 0.6034 - val_categorical_accuracy: 0.8129\n",
            "Epoch 15/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.3805 - categorical_accuracy: 0.8771 - val_loss: 0.5908 - val_categorical_accuracy: 0.8148\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00015: early stopping\n",
            "loss fold  3 0.7580363154411316\n",
            "loss media configurazione:  0.8611171046892802\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:smac.intensification.intensification.Intensifier:Challenger (0.8611) is better than incumbent (0.8802) on 1 runs.\n",
            "INFO:smac.intensification.intensification.Intensifier:Changes in incumbent:\n",
            "INFO:smac.intensification.intensification.Intensifier:  dropout_first_layer : 0.22962004322807786 -> 0.37049113463057776\n",
            "INFO:smac.intensification.intensification.Intensifier:  dropout_second_layer : 0.3851676822194797 -> 0.35451881390551154\n",
            "INFO:smac.intensification.intensification.Intensifier:  dropout_third_layer : None -> 0.3094849311696645\n",
            "INFO:smac.intensification.intensification.Intensifier:  freeze_to : 19 -> 18\n",
            "INFO:smac.intensification.intensification.Intensifier:  learning_rate : 0.00020759903165573382 -> 0.00019489454303259706\n",
            "INFO:smac.intensification.intensification.Intensifier:  number_first_layer : 984 -> 1009\n",
            "INFO:smac.intensification.intensification.Intensifier:  number_layer : 2 -> 3\n",
            "INFO:smac.intensification.intensification.Intensifier:  number_second_layer : 955 -> 1021\n",
            "INFO:smac.intensification.intensification.Intensifier:  number_third_layer : None -> 262\n",
            "INFO:smac.intensification.intensification.Intensifier:Wallclock time limit for intensification reached (used: 397.402653 sec, available: 0.000010 sec)\n",
            "INFO:smac.intensification.intensification.Intensifier:Updated estimated cost of incumbent on 1 runs: 0.8611\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 1 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 4272 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 1068 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 2671 validated image filenames belonging to 20 classes.\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 10s 157ms/step - loss: 4.4100 - categorical_accuracy: 0.0875 - val_loss: 2.7778 - val_categorical_accuracy: 0.2472\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 7s 111ms/step - loss: 2.7552 - categorical_accuracy: 0.1936 - val_loss: 2.1716 - val_categorical_accuracy: 0.3989\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 2.4361 - categorical_accuracy: 0.2870 - val_loss: 1.7021 - val_categorical_accuracy: 0.5019\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 2.1645 - categorical_accuracy: 0.3682 - val_loss: 1.3352 - val_categorical_accuracy: 0.5581\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 1.9302 - categorical_accuracy: 0.4302 - val_loss: 1.0876 - val_categorical_accuracy: 0.6236\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 8s 116ms/step - loss: 1.7549 - categorical_accuracy: 0.4853 - val_loss: 0.8873 - val_categorical_accuracy: 0.6742\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 1.6367 - categorical_accuracy: 0.5286 - val_loss: 0.8191 - val_categorical_accuracy: 0.7116\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 1.4714 - categorical_accuracy: 0.5681 - val_loss: 0.6803 - val_categorical_accuracy: 0.7416\n",
            "Epoch 9/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 1.3879 - categorical_accuracy: 0.5934 - val_loss: 0.5451 - val_categorical_accuracy: 0.7612\n",
            "Epoch 10/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 1.2536 - categorical_accuracy: 0.6257 - val_loss: 0.4620 - val_categorical_accuracy: 0.7669\n",
            "Epoch 11/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 1.1557 - categorical_accuracy: 0.6536 - val_loss: 0.4561 - val_categorical_accuracy: 0.7556\n",
            "Epoch 12/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 1.1154 - categorical_accuracy: 0.6681 - val_loss: 0.4155 - val_categorical_accuracy: 0.7687\n",
            "Epoch 13/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 1.0420 - categorical_accuracy: 0.6847 - val_loss: 0.3155 - val_categorical_accuracy: 0.7912\n",
            "Epoch 14/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.9769 - categorical_accuracy: 0.7107 - val_loss: 0.3222 - val_categorical_accuracy: 0.7903\n",
            "Epoch 15/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.9226 - categorical_accuracy: 0.7299 - val_loss: 0.2625 - val_categorical_accuracy: 0.7949\n",
            "Epoch 16/50\n",
            "67/67 [==============================] - 8s 117ms/step - loss: 0.8881 - categorical_accuracy: 0.7303 - val_loss: 0.2634 - val_categorical_accuracy: 0.7978\n",
            "Epoch 17/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.7837 - categorical_accuracy: 0.7533 - val_loss: 0.2540 - val_categorical_accuracy: 0.8024\n",
            "Epoch 18/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.7870 - categorical_accuracy: 0.7582 - val_loss: 0.2497 - val_categorical_accuracy: 0.8090\n",
            "Epoch 19/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.7542 - categorical_accuracy: 0.7704 - val_loss: 0.2242 - val_categorical_accuracy: 0.8052\n",
            "Epoch 20/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.7073 - categorical_accuracy: 0.7844 - val_loss: 0.2403 - val_categorical_accuracy: 0.8109\n",
            "Epoch 21/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.6799 - categorical_accuracy: 0.7907 - val_loss: 0.2322 - val_categorical_accuracy: 0.8081\n",
            "Epoch 22/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.6438 - categorical_accuracy: 0.7980 - val_loss: 0.2438 - val_categorical_accuracy: 0.8109\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.856087474152446e-05.\n",
            "Epoch 23/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.5858 - categorical_accuracy: 0.8109 - val_loss: 0.2297 - val_categorical_accuracy: 0.8184\n",
            "Epoch 24/50\n",
            "67/67 [==============================] - 9s 127ms/step - loss: 0.5708 - categorical_accuracy: 0.8230 - val_loss: 0.2176 - val_categorical_accuracy: 0.8184\n",
            "Epoch 25/50\n",
            "67/67 [==============================] - 9s 129ms/step - loss: 0.5986 - categorical_accuracy: 0.8172 - val_loss: 0.2147 - val_categorical_accuracy: 0.8230\n",
            "Epoch 26/50\n",
            "67/67 [==============================] - 8s 117ms/step - loss: 0.5529 - categorical_accuracy: 0.8258 - val_loss: 0.2127 - val_categorical_accuracy: 0.8212\n",
            "Epoch 27/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.5570 - categorical_accuracy: 0.8265 - val_loss: 0.2095 - val_categorical_accuracy: 0.8221\n",
            "Epoch 28/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.5607 - categorical_accuracy: 0.8174 - val_loss: 0.2104 - val_categorical_accuracy: 0.8202\n",
            "Epoch 29/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 0.5669 - categorical_accuracy: 0.8205 - val_loss: 0.2036 - val_categorical_accuracy: 0.8230\n",
            "Epoch 30/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.5609 - categorical_accuracy: 0.8254 - val_loss: 0.2020 - val_categorical_accuracy: 0.8240\n",
            "Epoch 31/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.5697 - categorical_accuracy: 0.8221 - val_loss: 0.2091 - val_categorical_accuracy: 0.8249\n",
            "Epoch 32/50\n",
            "67/67 [==============================] - 8s 117ms/step - loss: 0.5232 - categorical_accuracy: 0.8366 - val_loss: 0.2059 - val_categorical_accuracy: 0.8240\n",
            "Epoch 33/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.5326 - categorical_accuracy: 0.8361 - val_loss: 0.2054 - val_categorical_accuracy: 0.8230\n",
            "\n",
            "Epoch 00033: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Epoch 34/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.5373 - categorical_accuracy: 0.8317 - val_loss: 0.2057 - val_categorical_accuracy: 0.8258\n",
            "Epoch 35/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.5234 - categorical_accuracy: 0.8366 - val_loss: 0.2022 - val_categorical_accuracy: 0.8249\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00035: early stopping\n",
            "loss fold  1 1.047519326210022\n",
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 2 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 4272 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 1069 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 2670 validated image filenames belonging to 20 classes.\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 9s 136ms/step - loss: 4.3716 - categorical_accuracy: 0.1079 - val_loss: 2.4787 - val_categorical_accuracy: 0.2993\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 8s 113ms/step - loss: 2.6820 - categorical_accuracy: 0.2090 - val_loss: 1.9770 - val_categorical_accuracy: 0.4135\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 8s 124ms/step - loss: 2.3521 - categorical_accuracy: 0.3146 - val_loss: 1.5135 - val_categorical_accuracy: 0.5603\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 2.0906 - categorical_accuracy: 0.3816 - val_loss: 1.3198 - val_categorical_accuracy: 0.6174\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 1.8508 - categorical_accuracy: 0.4497 - val_loss: 1.0503 - val_categorical_accuracy: 0.6529\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 1.7427 - categorical_accuracy: 0.4829 - val_loss: 0.9843 - val_categorical_accuracy: 0.6866\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 1.5757 - categorical_accuracy: 0.5276 - val_loss: 0.9076 - val_categorical_accuracy: 0.7072\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 1.4912 - categorical_accuracy: 0.5581 - val_loss: 0.8090 - val_categorical_accuracy: 0.7381\n",
            "Epoch 9/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 1.4059 - categorical_accuracy: 0.5819 - val_loss: 0.7573 - val_categorical_accuracy: 0.7549\n",
            "Epoch 10/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 1.3002 - categorical_accuracy: 0.6049 - val_loss: 0.6299 - val_categorical_accuracy: 0.7792\n",
            "Epoch 11/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 1.2149 - categorical_accuracy: 0.6423 - val_loss: 0.5211 - val_categorical_accuracy: 0.7895\n",
            "Epoch 12/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 1.1445 - categorical_accuracy: 0.6515 - val_loss: 0.5108 - val_categorical_accuracy: 0.7951\n",
            "Epoch 13/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 1.0593 - categorical_accuracy: 0.6765 - val_loss: 0.4405 - val_categorical_accuracy: 0.8138\n",
            "Epoch 14/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 1.0077 - categorical_accuracy: 0.6934 - val_loss: 0.3699 - val_categorical_accuracy: 0.8092\n",
            "Epoch 15/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.9644 - categorical_accuracy: 0.7022 - val_loss: 0.3781 - val_categorical_accuracy: 0.8073\n",
            "Epoch 16/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.9100 - categorical_accuracy: 0.7158 - val_loss: 0.3588 - val_categorical_accuracy: 0.8213\n",
            "Epoch 17/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.8482 - categorical_accuracy: 0.7313 - val_loss: 0.3043 - val_categorical_accuracy: 0.8288\n",
            "Epoch 18/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.7999 - categorical_accuracy: 0.7533 - val_loss: 0.2936 - val_categorical_accuracy: 0.8167\n",
            "Epoch 19/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.7860 - categorical_accuracy: 0.7514 - val_loss: 0.2973 - val_categorical_accuracy: 0.8316\n",
            "Epoch 20/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.7485 - categorical_accuracy: 0.7685 - val_loss: 0.2950 - val_categorical_accuracy: 0.8316\n",
            "Epoch 21/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.6829 - categorical_accuracy: 0.7767 - val_loss: 0.2749 - val_categorical_accuracy: 0.8297\n",
            "Epoch 22/50\n",
            "67/67 [==============================] - 8s 117ms/step - loss: 0.6532 - categorical_accuracy: 0.7865 - val_loss: 0.2394 - val_categorical_accuracy: 0.8428\n",
            "Epoch 23/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.6203 - categorical_accuracy: 0.7975 - val_loss: 0.2773 - val_categorical_accuracy: 0.8372\n",
            "Epoch 24/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.6356 - categorical_accuracy: 0.8017 - val_loss: 0.2850 - val_categorical_accuracy: 0.8400\n",
            "Epoch 25/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.5829 - categorical_accuracy: 0.8120 - val_loss: 0.2739 - val_categorical_accuracy: 0.8372\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.856087474152446e-05.\n",
            "Epoch 26/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 0.5638 - categorical_accuracy: 0.8167 - val_loss: 0.2721 - val_categorical_accuracy: 0.8428\n",
            "Epoch 27/50\n",
            "67/67 [==============================] - 9s 136ms/step - loss: 0.5428 - categorical_accuracy: 0.8256 - val_loss: 0.2722 - val_categorical_accuracy: 0.8447\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00027: early stopping\n",
            "loss fold  2 0.6522083878517151\n",
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 3 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 4272 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 1069 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 2670 validated image filenames belonging to 20 classes.\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 9s 128ms/step - loss: 4.2768 - categorical_accuracy: 0.1159 - val_loss: 2.7365 - val_categorical_accuracy: 0.3022\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 2.7013 - categorical_accuracy: 0.2020 - val_loss: 2.3069 - val_categorical_accuracy: 0.4275\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 2.3778 - categorical_accuracy: 0.3003 - val_loss: 1.9307 - val_categorical_accuracy: 0.5332\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 2.1184 - categorical_accuracy: 0.3890 - val_loss: 1.5914 - val_categorical_accuracy: 0.6174\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 1.9375 - categorical_accuracy: 0.4389 - val_loss: 1.3751 - val_categorical_accuracy: 0.6520\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 1.7508 - categorical_accuracy: 0.4890 - val_loss: 1.1197 - val_categorical_accuracy: 0.6988\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 8s 117ms/step - loss: 1.6212 - categorical_accuracy: 0.5412 - val_loss: 1.0720 - val_categorical_accuracy: 0.7166\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 1.4589 - categorical_accuracy: 0.5754 - val_loss: 0.8856 - val_categorical_accuracy: 0.7343\n",
            "Epoch 9/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 1.3322 - categorical_accuracy: 0.6035 - val_loss: 0.8240 - val_categorical_accuracy: 0.7596\n",
            "Epoch 10/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 1.2638 - categorical_accuracy: 0.6297 - val_loss: 0.7634 - val_categorical_accuracy: 0.7755\n",
            "Epoch 11/50\n",
            "67/67 [==============================] - 8s 117ms/step - loss: 1.1794 - categorical_accuracy: 0.6540 - val_loss: 0.7085 - val_categorical_accuracy: 0.7774\n",
            "Epoch 12/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 1.0670 - categorical_accuracy: 0.6887 - val_loss: 0.7004 - val_categorical_accuracy: 0.7923\n",
            "Epoch 13/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 1.0059 - categorical_accuracy: 0.7060 - val_loss: 0.6182 - val_categorical_accuracy: 0.7998\n",
            "Epoch 14/50\n",
            "67/67 [==============================] - 8s 117ms/step - loss: 0.9401 - categorical_accuracy: 0.7259 - val_loss: 0.6936 - val_categorical_accuracy: 0.8101\n",
            "Epoch 15/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.9070 - categorical_accuracy: 0.7280 - val_loss: 0.7092 - val_categorical_accuracy: 0.7979\n",
            "Epoch 16/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.8574 - categorical_accuracy: 0.7467 - val_loss: 0.6650 - val_categorical_accuracy: 0.8054\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.856087474152446e-05.\n",
            "Epoch 17/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.7822 - categorical_accuracy: 0.7551 - val_loss: 0.6586 - val_categorical_accuracy: 0.8064\n",
            "Epoch 18/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.7953 - categorical_accuracy: 0.7587 - val_loss: 0.6489 - val_categorical_accuracy: 0.8045\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00018: early stopping\n",
            "loss fold  3 1.2273701429367065\n",
            "loss media configurazione:  0.9756992856661478\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:smac.intensification.intensification.Intensifier:Wallclock time limit for intensification reached (used: 669.653925 sec, available: 0.000010 sec)\n",
            "INFO:smac.intensification.intensification.Intensifier:Updated estimated cost of incumbent on 1 runs: 0.8611\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 1 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 4272 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 1068 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 2671 validated image filenames belonging to 20 classes.\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 11s 158ms/step - loss: 2.9352 - categorical_accuracy: 0.5351 - val_loss: 0.3539 - val_categorical_accuracy: 0.7669\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 7s 112ms/step - loss: 0.9276 - categorical_accuracy: 0.7371 - val_loss: 0.2777 - val_categorical_accuracy: 0.7715\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.6626 - categorical_accuracy: 0.8038 - val_loss: 0.2194 - val_categorical_accuracy: 0.7884\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.5164 - categorical_accuracy: 0.8411 - val_loss: 0.2711 - val_categorical_accuracy: 0.7893\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.3936 - categorical_accuracy: 0.8703 - val_loss: 0.2561 - val_categorical_accuracy: 0.8109\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 0.3259 - categorical_accuracy: 0.8975 - val_loss: 0.1760 - val_categorical_accuracy: 0.8081\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.2520 - categorical_accuracy: 0.9174 - val_loss: 0.3529 - val_categorical_accuracy: 0.8155\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.2121 - categorical_accuracy: 0.9305 - val_loss: 0.2205 - val_categorical_accuracy: 0.8099\n",
            "Epoch 9/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.1733 - categorical_accuracy: 0.9403 - val_loss: 0.2546 - val_categorical_accuracy: 0.8099\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 7.186431321315467e-05.\n",
            "Epoch 10/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.1075 - categorical_accuracy: 0.9663 - val_loss: 0.1932 - val_categorical_accuracy: 0.8202\n",
            "Epoch 11/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.1127 - categorical_accuracy: 0.9632 - val_loss: 0.1594 - val_categorical_accuracy: 0.8165\n",
            "Epoch 12/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.0935 - categorical_accuracy: 0.9712 - val_loss: 0.1522 - val_categorical_accuracy: 0.8184\n",
            "Epoch 13/50\n",
            "67/67 [==============================] - 8s 117ms/step - loss: 0.0994 - categorical_accuracy: 0.9672 - val_loss: 0.1460 - val_categorical_accuracy: 0.8221\n",
            "Epoch 14/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.0929 - categorical_accuracy: 0.9717 - val_loss: 0.1548 - val_categorical_accuracy: 0.8174\n",
            "Epoch 15/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.0888 - categorical_accuracy: 0.9721 - val_loss: 0.1594 - val_categorical_accuracy: 0.8165\n",
            "Epoch 16/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 0.0783 - categorical_accuracy: 0.9724 - val_loss: 0.1567 - val_categorical_accuracy: 0.8184\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Epoch 17/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.0786 - categorical_accuracy: 0.9764 - val_loss: 0.1545 - val_categorical_accuracy: 0.8221\n",
            "Epoch 18/50\n",
            "67/67 [==============================] - 9s 130ms/step - loss: 0.0658 - categorical_accuracy: 0.9817 - val_loss: 0.1538 - val_categorical_accuracy: 0.8193\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00018: early stopping\n",
            "loss fold  1 1.5200432538986206\n",
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 2 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 4272 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 1069 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 2670 validated image filenames belonging to 20 classes.\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 9s 136ms/step - loss: 3.0580 - categorical_accuracy: 0.5293 - val_loss: 1.1776 - val_categorical_accuracy: 0.7605\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 7s 111ms/step - loss: 0.9680 - categorical_accuracy: 0.7285 - val_loss: 1.2147 - val_categorical_accuracy: 0.7820\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.6842 - categorical_accuracy: 0.7907 - val_loss: 0.9556 - val_categorical_accuracy: 0.8157\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.4947 - categorical_accuracy: 0.8397 - val_loss: 0.8978 - val_categorical_accuracy: 0.8148\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.4109 - categorical_accuracy: 0.8598 - val_loss: 1.0528 - val_categorical_accuracy: 0.8167\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.3359 - categorical_accuracy: 0.8879 - val_loss: 0.9991 - val_categorical_accuracy: 0.8297\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.2545 - categorical_accuracy: 0.9185 - val_loss: 0.8832 - val_categorical_accuracy: 0.8157\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.2287 - categorical_accuracy: 0.9258 - val_loss: 0.9499 - val_categorical_accuracy: 0.8213\n",
            "Epoch 9/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.1793 - categorical_accuracy: 0.9382 - val_loss: 1.0972 - val_categorical_accuracy: 0.8241\n",
            "Epoch 10/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.1630 - categorical_accuracy: 0.9429 - val_loss: 1.2104 - val_categorical_accuracy: 0.8279\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 7.186431321315467e-05.\n",
            "Epoch 11/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.1203 - categorical_accuracy: 0.9621 - val_loss: 1.1850 - val_categorical_accuracy: 0.8344\n",
            "Epoch 12/50\n",
            "67/67 [==============================] - 9s 132ms/step - loss: 0.0868 - categorical_accuracy: 0.9714 - val_loss: 1.1835 - val_categorical_accuracy: 0.8363\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00012: early stopping\n",
            "loss fold  2 0.649003803730011\n",
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 3 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 4272 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 1069 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 2670 validated image filenames belonging to 20 classes.\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 9s 133ms/step - loss: 3.0505 - categorical_accuracy: 0.5265 - val_loss: 0.6762 - val_categorical_accuracy: 0.7689\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 8s 124ms/step - loss: 1.0124 - categorical_accuracy: 0.7243 - val_loss: 0.6158 - val_categorical_accuracy: 0.7998\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 8s 124ms/step - loss: 0.6569 - categorical_accuracy: 0.8041 - val_loss: 0.5098 - val_categorical_accuracy: 0.7979\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 8s 125ms/step - loss: 0.5347 - categorical_accuracy: 0.8345 - val_loss: 0.3751 - val_categorical_accuracy: 0.8110\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 8s 123ms/step - loss: 0.3917 - categorical_accuracy: 0.8736 - val_loss: 0.4938 - val_categorical_accuracy: 0.8195\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.3176 - categorical_accuracy: 0.8991 - val_loss: 0.5045 - val_categorical_accuracy: 0.8157\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.2600 - categorical_accuracy: 0.9148 - val_loss: 0.4621 - val_categorical_accuracy: 0.8185\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 7.186431321315467e-05.\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.1728 - categorical_accuracy: 0.9389 - val_loss: 0.4492 - val_categorical_accuracy: 0.8223\n",
            "Epoch 9/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.1583 - categorical_accuracy: 0.9450 - val_loss: 0.4269 - val_categorical_accuracy: 0.8260\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00009: early stopping\n",
            "loss fold  3 1.108319640159607\n",
            "loss media configurazione:  1.0924555659294128\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:smac.intensification.intensification.Intensifier:Wallclock time limit for intensification reached (used: 343.479037 sec, available: 0.000010 sec)\n",
            "INFO:smac.intensification.intensification.Intensifier:Updated estimated cost of incumbent on 1 runs: 0.8611\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 1 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 4272 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 1068 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 2671 validated image filenames belonging to 20 classes.\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 11s 157ms/step - loss: 2.7669 - categorical_accuracy: 0.1760 - val_loss: 2.1923 - val_categorical_accuracy: 0.2631\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 8s 112ms/step - loss: 2.3040 - categorical_accuracy: 0.2903 - val_loss: 2.0294 - val_categorical_accuracy: 0.2772\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 8s 124ms/step - loss: 2.0602 - categorical_accuracy: 0.3298 - val_loss: 1.7352 - val_categorical_accuracy: 0.3521\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 1.8858 - categorical_accuracy: 0.3703 - val_loss: 1.6074 - val_categorical_accuracy: 0.3989\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 1.7514 - categorical_accuracy: 0.4059 - val_loss: 1.4478 - val_categorical_accuracy: 0.4223\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 1.6270 - categorical_accuracy: 0.4469 - val_loss: 1.6811 - val_categorical_accuracy: 0.4298\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 1.5367 - categorical_accuracy: 0.4682 - val_loss: 1.5333 - val_categorical_accuracy: 0.4232\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 1.3916 - categorical_accuracy: 0.4981 - val_loss: 1.2947 - val_categorical_accuracy: 0.4747\n",
            "Epoch 9/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 1.2093 - categorical_accuracy: 0.5609 - val_loss: 1.0749 - val_categorical_accuracy: 0.5178\n",
            "Epoch 10/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 1.1064 - categorical_accuracy: 0.6035 - val_loss: 1.2507 - val_categorical_accuracy: 0.5449\n",
            "Epoch 11/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.9731 - categorical_accuracy: 0.6599 - val_loss: 1.2152 - val_categorical_accuracy: 0.5964\n",
            "Epoch 12/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.8690 - categorical_accuracy: 0.7018 - val_loss: 1.2769 - val_categorical_accuracy: 0.6227\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 4.3939269380643965e-05.\n",
            "Epoch 13/50\n",
            "67/67 [==============================] - 9s 128ms/step - loss: 0.6246 - categorical_accuracy: 0.7743 - val_loss: 1.0964 - val_categorical_accuracy: 0.6826\n",
            "Epoch 14/50\n",
            "67/67 [==============================] - 9s 138ms/step - loss: 0.5208 - categorical_accuracy: 0.8043 - val_loss: 1.1534 - val_categorical_accuracy: 0.6816\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00014: early stopping\n",
            "loss fold  1 1.3041131496429443\n",
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 2 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 4272 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 1069 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 2670 validated image filenames belonging to 20 classes.\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 9s 138ms/step - loss: 2.7192 - categorical_accuracy: 0.1983 - val_loss: 2.1297 - val_categorical_accuracy: 0.2601\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 7s 112ms/step - loss: 2.3648 - categorical_accuracy: 0.2905 - val_loss: 2.1562 - val_categorical_accuracy: 0.2535\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 2.1611 - categorical_accuracy: 0.3141 - val_loss: 1.7895 - val_categorical_accuracy: 0.3517\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 1.9843 - categorical_accuracy: 0.3675 - val_loss: 1.8950 - val_categorical_accuracy: 0.3761\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 1.8170 - categorical_accuracy: 0.3991 - val_loss: 1.7961 - val_categorical_accuracy: 0.3742\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 1.6827 - categorical_accuracy: 0.4263 - val_loss: 1.8271 - val_categorical_accuracy: 0.3751\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 4.3939269380643965e-05.\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 1.4979 - categorical_accuracy: 0.4546 - val_loss: 1.7422 - val_categorical_accuracy: 0.3976\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 1.4657 - categorical_accuracy: 0.4511 - val_loss: 1.7338 - val_categorical_accuracy: 0.3910\n",
            "Epoch 9/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 1.4223 - categorical_accuracy: 0.4656 - val_loss: 1.7194 - val_categorical_accuracy: 0.4013\n",
            "Epoch 10/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 1.3825 - categorical_accuracy: 0.4778 - val_loss: 1.7257 - val_categorical_accuracy: 0.4275\n",
            "Epoch 11/50\n",
            "67/67 [==============================] - 9s 128ms/step - loss: 1.3395 - categorical_accuracy: 0.4925 - val_loss: 1.7417 - val_categorical_accuracy: 0.4266\n",
            "Epoch 12/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 1.3091 - categorical_accuracy: 0.4923 - val_loss: 1.7410 - val_categorical_accuracy: 0.4294\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Epoch 13/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 1.2707 - categorical_accuracy: 0.5159 - val_loss: 1.7564 - val_categorical_accuracy: 0.4331\n",
            "Epoch 14/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 1.2674 - categorical_accuracy: 0.5115 - val_loss: 1.7556 - val_categorical_accuracy: 0.4312\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00014: early stopping\n",
            "loss fold  2 2.2684738636016846\n",
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 3 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 4272 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 1069 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 2670 validated image filenames belonging to 20 classes.\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 9s 131ms/step - loss: 2.8347 - categorical_accuracy: 0.1758 - val_loss: 2.6169 - val_categorical_accuracy: 0.2264\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 2.3890 - categorical_accuracy: 0.2706 - val_loss: 2.4948 - val_categorical_accuracy: 0.3087\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 2.1769 - categorical_accuracy: 0.3301 - val_loss: 2.2376 - val_categorical_accuracy: 0.3564\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 1.9840 - categorical_accuracy: 0.3916 - val_loss: 2.1574 - val_categorical_accuracy: 0.3508\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 1.8106 - categorical_accuracy: 0.4132 - val_loss: 2.0752 - val_categorical_accuracy: 0.3639\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 1.6678 - categorical_accuracy: 0.4249 - val_loss: 1.9923 - val_categorical_accuracy: 0.4369\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 1.4915 - categorical_accuracy: 0.4618 - val_loss: 1.9305 - val_categorical_accuracy: 0.4041\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 1.4001 - categorical_accuracy: 0.4810 - val_loss: 1.8849 - val_categorical_accuracy: 0.4116\n",
            "Epoch 9/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 1.3283 - categorical_accuracy: 0.4827 - val_loss: 1.9808 - val_categorical_accuracy: 0.4191\n",
            "Epoch 10/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 1.2631 - categorical_accuracy: 0.5194 - val_loss: 1.8002 - val_categorical_accuracy: 0.4350\n",
            "Epoch 11/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 1.2002 - categorical_accuracy: 0.5475 - val_loss: 1.6599 - val_categorical_accuracy: 0.4780\n",
            "Epoch 12/50\n",
            "67/67 [==============================] - 8s 123ms/step - loss: 1.0479 - categorical_accuracy: 0.6110 - val_loss: 1.2135 - val_categorical_accuracy: 0.5145\n",
            "Epoch 13/50\n",
            "67/67 [==============================] - 8s 123ms/step - loss: 0.9454 - categorical_accuracy: 0.6456 - val_loss: 1.3977 - val_categorical_accuracy: 0.5444\n",
            "Epoch 14/50\n",
            "67/67 [==============================] - 8s 123ms/step - loss: 0.8262 - categorical_accuracy: 0.6723 - val_loss: 1.4749 - val_categorical_accuracy: 0.5304\n",
            "Epoch 15/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 0.7612 - categorical_accuracy: 0.7107 - val_loss: 1.9111 - val_categorical_accuracy: 0.5454\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 4.3939269380643965e-05.\n",
            "Epoch 16/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.6357 - categorical_accuracy: 0.7427 - val_loss: 1.5186 - val_categorical_accuracy: 0.5856\n",
            "Epoch 17/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 0.5543 - categorical_accuracy: 0.7638 - val_loss: 1.4996 - val_categorical_accuracy: 0.5903\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00017: early stopping\n",
            "loss fold  3 1.9057477712631226\n",
            "loss media configurazione:  1.8261115948359172\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:smac.intensification.intensification.Intensifier:Wallclock time limit for intensification reached (used: 395.681369 sec, available: 0.000010 sec)\n",
            "INFO:smac.intensification.intensification.Intensifier:Updated estimated cost of incumbent on 1 runs: 0.8611\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 1 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 4272 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 1068 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 2671 validated image filenames belonging to 20 classes.\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 10s 154ms/step - loss: 9.1027 - categorical_accuracy: 0.0864 - val_loss: 4.9048 - val_categorical_accuracy: 0.1096\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 8s 112ms/step - loss: 8.4631 - categorical_accuracy: 0.0857 - val_loss: 4.3762 - val_categorical_accuracy: 0.1077\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 9s 132ms/step - loss: 7.9286 - categorical_accuracy: 0.0808 - val_loss: 3.9909 - val_categorical_accuracy: 0.1180\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 9s 131ms/step - loss: 7.6158 - categorical_accuracy: 0.0871 - val_loss: 3.7188 - val_categorical_accuracy: 0.1273\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 7.2114 - categorical_accuracy: 0.0819 - val_loss: 3.5117 - val_categorical_accuracy: 0.1442\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 6.7864 - categorical_accuracy: 0.0906 - val_loss: 3.3517 - val_categorical_accuracy: 0.1507\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 6.5726 - categorical_accuracy: 0.0974 - val_loss: 3.2206 - val_categorical_accuracy: 0.1657\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 6.4204 - categorical_accuracy: 0.0920 - val_loss: 3.1076 - val_categorical_accuracy: 0.1695\n",
            "Epoch 9/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 6.2614 - categorical_accuracy: 0.0985 - val_loss: 3.0130 - val_categorical_accuracy: 0.1826\n",
            "Epoch 10/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 5.9464 - categorical_accuracy: 0.1077 - val_loss: 2.9161 - val_categorical_accuracy: 0.1910\n",
            "Epoch 11/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 5.8662 - categorical_accuracy: 0.1145 - val_loss: 2.8365 - val_categorical_accuracy: 0.2013\n",
            "Epoch 12/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 5.7346 - categorical_accuracy: 0.1156 - val_loss: 2.7535 - val_categorical_accuracy: 0.2135\n",
            "Epoch 13/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 5.6355 - categorical_accuracy: 0.1140 - val_loss: 2.6709 - val_categorical_accuracy: 0.2257\n",
            "Epoch 14/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 5.4642 - categorical_accuracy: 0.1297 - val_loss: 2.5946 - val_categorical_accuracy: 0.2360\n",
            "Epoch 15/50\n",
            "67/67 [==============================] - 8s 125ms/step - loss: 5.4237 - categorical_accuracy: 0.1231 - val_loss: 2.5242 - val_categorical_accuracy: 0.2566\n",
            "Epoch 16/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 5.2796 - categorical_accuracy: 0.1339 - val_loss: 2.4552 - val_categorical_accuracy: 0.2622\n",
            "Epoch 17/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 5.1549 - categorical_accuracy: 0.1451 - val_loss: 2.3882 - val_categorical_accuracy: 0.2772\n",
            "Epoch 18/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 5.0017 - categorical_accuracy: 0.1376 - val_loss: 2.3279 - val_categorical_accuracy: 0.3015\n",
            "Epoch 19/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 4.9761 - categorical_accuracy: 0.1505 - val_loss: 2.2754 - val_categorical_accuracy: 0.3202\n",
            "Epoch 20/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 4.8058 - categorical_accuracy: 0.1503 - val_loss: 2.2150 - val_categorical_accuracy: 0.3333\n",
            "Epoch 21/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 4.7002 - categorical_accuracy: 0.1559 - val_loss: 2.1674 - val_categorical_accuracy: 0.3408\n",
            "Epoch 22/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 4.7422 - categorical_accuracy: 0.1482 - val_loss: 2.1240 - val_categorical_accuracy: 0.3539\n",
            "Epoch 23/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 4.5828 - categorical_accuracy: 0.1646 - val_loss: 2.0759 - val_categorical_accuracy: 0.3699\n",
            "Epoch 24/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 4.5215 - categorical_accuracy: 0.1721 - val_loss: 2.0288 - val_categorical_accuracy: 0.3839\n",
            "Epoch 25/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 4.3764 - categorical_accuracy: 0.1721 - val_loss: 1.9921 - val_categorical_accuracy: 0.3979\n",
            "Epoch 26/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 4.3094 - categorical_accuracy: 0.1824 - val_loss: 1.9497 - val_categorical_accuracy: 0.4026\n",
            "Epoch 27/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 4.3058 - categorical_accuracy: 0.1737 - val_loss: 1.9142 - val_categorical_accuracy: 0.4120\n",
            "Epoch 28/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 4.1479 - categorical_accuracy: 0.1882 - val_loss: 1.8794 - val_categorical_accuracy: 0.4213\n",
            "Epoch 29/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 4.1544 - categorical_accuracy: 0.1898 - val_loss: 1.8383 - val_categorical_accuracy: 0.4326\n",
            "Epoch 30/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 4.0375 - categorical_accuracy: 0.2020 - val_loss: 1.8103 - val_categorical_accuracy: 0.4401\n",
            "Epoch 31/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 4.1045 - categorical_accuracy: 0.1917 - val_loss: 1.7835 - val_categorical_accuracy: 0.4457\n",
            "Epoch 32/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 3.9946 - categorical_accuracy: 0.1971 - val_loss: 1.7487 - val_categorical_accuracy: 0.4504\n",
            "Epoch 33/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 3.8995 - categorical_accuracy: 0.1976 - val_loss: 1.7206 - val_categorical_accuracy: 0.4607\n",
            "Epoch 34/50\n",
            "67/67 [==============================] - 8s 123ms/step - loss: 3.8881 - categorical_accuracy: 0.1976 - val_loss: 1.6930 - val_categorical_accuracy: 0.4663\n",
            "Epoch 35/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 3.7774 - categorical_accuracy: 0.2137 - val_loss: 1.6662 - val_categorical_accuracy: 0.4747\n",
            "Epoch 36/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 3.7569 - categorical_accuracy: 0.2130 - val_loss: 1.6404 - val_categorical_accuracy: 0.4803\n",
            "Epoch 37/50\n",
            "67/67 [==============================] - 8s 117ms/step - loss: 3.6470 - categorical_accuracy: 0.2240 - val_loss: 1.6230 - val_categorical_accuracy: 0.4822\n",
            "Epoch 38/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 3.6910 - categorical_accuracy: 0.2217 - val_loss: 1.6065 - val_categorical_accuracy: 0.4897\n",
            "Epoch 39/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 3.5758 - categorical_accuracy: 0.2374 - val_loss: 1.5865 - val_categorical_accuracy: 0.4944\n",
            "Epoch 40/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 3.5064 - categorical_accuracy: 0.2350 - val_loss: 1.5631 - val_categorical_accuracy: 0.5000\n",
            "Epoch 41/50\n",
            "67/67 [==============================] - 8s 125ms/step - loss: 3.5638 - categorical_accuracy: 0.2301 - val_loss: 1.5414 - val_categorical_accuracy: 0.5056\n",
            "Epoch 42/50\n",
            "67/67 [==============================] - 9s 134ms/step - loss: 3.5393 - categorical_accuracy: 0.2310 - val_loss: 1.5239 - val_categorical_accuracy: 0.5084\n",
            "Epoch 43/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 3.3134 - categorical_accuracy: 0.2587 - val_loss: 1.5061 - val_categorical_accuracy: 0.5150\n",
            "Epoch 44/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 3.3799 - categorical_accuracy: 0.2505 - val_loss: 1.4937 - val_categorical_accuracy: 0.5159\n",
            "Epoch 45/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 3.3424 - categorical_accuracy: 0.2477 - val_loss: 1.4761 - val_categorical_accuracy: 0.5159\n",
            "Epoch 46/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 3.3557 - categorical_accuracy: 0.2563 - val_loss: 1.4659 - val_categorical_accuracy: 0.5215\n",
            "Epoch 47/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 3.2438 - categorical_accuracy: 0.2662 - val_loss: 1.4534 - val_categorical_accuracy: 0.5262\n",
            "Epoch 48/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 3.2310 - categorical_accuracy: 0.2584 - val_loss: 1.4386 - val_categorical_accuracy: 0.5272\n",
            "Epoch 49/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 3.1890 - categorical_accuracy: 0.2659 - val_loss: 1.4243 - val_categorical_accuracy: 0.5318\n",
            "Epoch 50/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 3.1367 - categorical_accuracy: 0.2755 - val_loss: 1.4143 - val_categorical_accuracy: 0.5356\n",
            "loss fold  1 1.595668077468872\n",
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 2 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 4272 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 1069 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 2670 validated image filenames belonging to 20 classes.\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 9s 140ms/step - loss: 9.7005 - categorical_accuracy: 0.0499 - val_loss: 5.5305 - val_categorical_accuracy: 0.0468\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 8s 116ms/step - loss: 8.7896 - categorical_accuracy: 0.0536 - val_loss: 4.7270 - val_categorical_accuracy: 0.0496\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 8.0317 - categorical_accuracy: 0.0566 - val_loss: 4.2028 - val_categorical_accuracy: 0.0580\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 7.5605 - categorical_accuracy: 0.0648 - val_loss: 3.8260 - val_categorical_accuracy: 0.0776\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 7.2333 - categorical_accuracy: 0.0716 - val_loss: 3.5568 - val_categorical_accuracy: 0.0992\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 6.8939 - categorical_accuracy: 0.0646 - val_loss: 3.3582 - val_categorical_accuracy: 0.1291\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 6.7087 - categorical_accuracy: 0.0787 - val_loss: 3.2150 - val_categorical_accuracy: 0.1422\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 6.3026 - categorical_accuracy: 0.0812 - val_loss: 3.0996 - val_categorical_accuracy: 0.1562\n",
            "Epoch 9/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 6.2087 - categorical_accuracy: 0.0885 - val_loss: 3.0070 - val_categorical_accuracy: 0.1749\n",
            "Epoch 10/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 6.0930 - categorical_accuracy: 0.0852 - val_loss: 2.9295 - val_categorical_accuracy: 0.1936\n",
            "Epoch 11/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 5.9303 - categorical_accuracy: 0.0885 - val_loss: 2.8465 - val_categorical_accuracy: 0.2067\n",
            "Epoch 12/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 5.6704 - categorical_accuracy: 0.0990 - val_loss: 2.7768 - val_categorical_accuracy: 0.2133\n",
            "Epoch 13/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 5.5504 - categorical_accuracy: 0.1060 - val_loss: 2.7133 - val_categorical_accuracy: 0.2245\n",
            "Epoch 14/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 5.3854 - categorical_accuracy: 0.1086 - val_loss: 2.6500 - val_categorical_accuracy: 0.2385\n",
            "Epoch 15/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 5.4144 - categorical_accuracy: 0.1103 - val_loss: 2.5989 - val_categorical_accuracy: 0.2507\n",
            "Epoch 16/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 5.3653 - categorical_accuracy: 0.1067 - val_loss: 2.5476 - val_categorical_accuracy: 0.2638\n",
            "Epoch 17/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 5.1698 - categorical_accuracy: 0.1163 - val_loss: 2.4969 - val_categorical_accuracy: 0.2788\n",
            "Epoch 18/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 4.9299 - categorical_accuracy: 0.1252 - val_loss: 2.4561 - val_categorical_accuracy: 0.2881\n",
            "Epoch 19/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 4.8817 - categorical_accuracy: 0.1294 - val_loss: 2.4103 - val_categorical_accuracy: 0.2975\n",
            "Epoch 20/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 4.8147 - categorical_accuracy: 0.1320 - val_loss: 2.3692 - val_categorical_accuracy: 0.3143\n",
            "Epoch 21/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 4.7357 - categorical_accuracy: 0.1323 - val_loss: 2.3400 - val_categorical_accuracy: 0.3209\n",
            "Epoch 22/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 4.6843 - categorical_accuracy: 0.1309 - val_loss: 2.3046 - val_categorical_accuracy: 0.3321\n",
            "Epoch 23/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 4.5289 - categorical_accuracy: 0.1397 - val_loss: 2.2697 - val_categorical_accuracy: 0.3480\n",
            "Epoch 24/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 4.4981 - categorical_accuracy: 0.1344 - val_loss: 2.2338 - val_categorical_accuracy: 0.3583\n",
            "Epoch 25/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 4.3951 - categorical_accuracy: 0.1566 - val_loss: 2.1961 - val_categorical_accuracy: 0.3658\n",
            "Epoch 26/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 4.2855 - categorical_accuracy: 0.1599 - val_loss: 2.1642 - val_categorical_accuracy: 0.3770\n",
            "Epoch 27/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 4.2653 - categorical_accuracy: 0.1629 - val_loss: 2.1347 - val_categorical_accuracy: 0.3873\n",
            "Epoch 28/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 4.2275 - categorical_accuracy: 0.1559 - val_loss: 2.1063 - val_categorical_accuracy: 0.3948\n",
            "Epoch 29/50\n",
            "67/67 [==============================] - 9s 136ms/step - loss: 4.1265 - categorical_accuracy: 0.1627 - val_loss: 2.0805 - val_categorical_accuracy: 0.4022\n",
            "Epoch 30/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 4.0280 - categorical_accuracy: 0.1781 - val_loss: 2.0534 - val_categorical_accuracy: 0.4125\n",
            "Epoch 31/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 3.9858 - categorical_accuracy: 0.1660 - val_loss: 2.0270 - val_categorical_accuracy: 0.4210\n",
            "Epoch 32/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 3.9776 - categorical_accuracy: 0.1772 - val_loss: 2.0050 - val_categorical_accuracy: 0.4275\n",
            "Epoch 33/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 3.9564 - categorical_accuracy: 0.1814 - val_loss: 1.9828 - val_categorical_accuracy: 0.4331\n",
            "Epoch 34/50\n",
            "67/67 [==============================] - 8s 117ms/step - loss: 3.7668 - categorical_accuracy: 0.2004 - val_loss: 1.9616 - val_categorical_accuracy: 0.4406\n",
            "Epoch 35/50\n",
            "67/67 [==============================] - 8s 117ms/step - loss: 3.7709 - categorical_accuracy: 0.1987 - val_loss: 1.9416 - val_categorical_accuracy: 0.4443\n",
            "Epoch 36/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 3.6820 - categorical_accuracy: 0.1987 - val_loss: 1.9235 - val_categorical_accuracy: 0.4500\n",
            "Epoch 37/50\n",
            "67/67 [==============================] - 8s 117ms/step - loss: 3.6768 - categorical_accuracy: 0.1950 - val_loss: 1.9061 - val_categorical_accuracy: 0.4537\n",
            "Epoch 38/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 3.6495 - categorical_accuracy: 0.2044 - val_loss: 1.8852 - val_categorical_accuracy: 0.4668\n",
            "Epoch 39/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 3.5561 - categorical_accuracy: 0.2182 - val_loss: 1.8677 - val_categorical_accuracy: 0.4743\n",
            "Epoch 40/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 3.5244 - categorical_accuracy: 0.2097 - val_loss: 1.8509 - val_categorical_accuracy: 0.4864\n",
            "Epoch 41/50\n",
            "67/67 [==============================] - 8s 117ms/step - loss: 3.4534 - categorical_accuracy: 0.2186 - val_loss: 1.8342 - val_categorical_accuracy: 0.4958\n",
            "Epoch 42/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 3.3766 - categorical_accuracy: 0.2250 - val_loss: 1.8165 - val_categorical_accuracy: 0.5005\n",
            "Epoch 43/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 3.4255 - categorical_accuracy: 0.2261 - val_loss: 1.7998 - val_categorical_accuracy: 0.5061\n",
            "Epoch 44/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 3.3403 - categorical_accuracy: 0.2303 - val_loss: 1.7867 - val_categorical_accuracy: 0.5117\n",
            "Epoch 45/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 3.3049 - categorical_accuracy: 0.2327 - val_loss: 1.7718 - val_categorical_accuracy: 0.5220\n",
            "Epoch 46/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 3.2293 - categorical_accuracy: 0.2374 - val_loss: 1.7542 - val_categorical_accuracy: 0.5257\n",
            "Epoch 47/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 3.2024 - categorical_accuracy: 0.2360 - val_loss: 1.7415 - val_categorical_accuracy: 0.5295\n",
            "Epoch 48/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 3.2028 - categorical_accuracy: 0.2355 - val_loss: 1.7296 - val_categorical_accuracy: 0.5416\n",
            "Epoch 49/50\n",
            "67/67 [==============================] - 8s 117ms/step - loss: 3.2183 - categorical_accuracy: 0.2540 - val_loss: 1.7170 - val_categorical_accuracy: 0.5444\n",
            "Epoch 50/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 3.1107 - categorical_accuracy: 0.2486 - val_loss: 1.7044 - val_categorical_accuracy: 0.5472\n",
            "loss fold  2 2.1963577270507812\n",
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 3 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 4272 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 1069 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 2670 validated image filenames belonging to 20 classes.\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 9s 132ms/step - loss: 8.8399 - categorical_accuracy: 0.0480 - val_loss: 4.8191 - val_categorical_accuracy: 0.0664\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 8.1512 - categorical_accuracy: 0.0545 - val_loss: 4.3433 - val_categorical_accuracy: 0.0795\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 7.6903 - categorical_accuracy: 0.0564 - val_loss: 4.0230 - val_categorical_accuracy: 0.0954\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 7.2164 - categorical_accuracy: 0.0709 - val_loss: 3.7909 - val_categorical_accuracy: 0.1169\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 6.9859 - categorical_accuracy: 0.0700 - val_loss: 3.6094 - val_categorical_accuracy: 0.1356\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 6.6572 - categorical_accuracy: 0.0747 - val_loss: 3.4578 - val_categorical_accuracy: 0.1431\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 8s 117ms/step - loss: 6.4812 - categorical_accuracy: 0.0794 - val_loss: 3.3471 - val_categorical_accuracy: 0.1562\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 6.2132 - categorical_accuracy: 0.0880 - val_loss: 3.2460 - val_categorical_accuracy: 0.1674\n",
            "Epoch 9/50\n",
            "67/67 [==============================] - 8s 117ms/step - loss: 6.1520 - categorical_accuracy: 0.0936 - val_loss: 3.1570 - val_categorical_accuracy: 0.1899\n",
            "Epoch 10/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 5.8819 - categorical_accuracy: 0.0969 - val_loss: 3.0666 - val_categorical_accuracy: 0.2105\n",
            "Epoch 11/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 5.8449 - categorical_accuracy: 0.0948 - val_loss: 2.9855 - val_categorical_accuracy: 0.2301\n",
            "Epoch 12/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 5.6063 - categorical_accuracy: 0.1058 - val_loss: 2.9131 - val_categorical_accuracy: 0.2423\n",
            "Epoch 13/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 5.5720 - categorical_accuracy: 0.1070 - val_loss: 2.8477 - val_categorical_accuracy: 0.2554\n",
            "Epoch 14/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 5.4468 - categorical_accuracy: 0.1086 - val_loss: 2.7821 - val_categorical_accuracy: 0.2647\n",
            "Epoch 15/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 5.3417 - categorical_accuracy: 0.1126 - val_loss: 2.7194 - val_categorical_accuracy: 0.2797\n",
            "Epoch 16/50\n",
            "67/67 [==============================] - 9s 127ms/step - loss: 5.1302 - categorical_accuracy: 0.1156 - val_loss: 2.6653 - val_categorical_accuracy: 0.3012\n",
            "Epoch 17/50\n",
            "67/67 [==============================] - 9s 130ms/step - loss: 5.1128 - categorical_accuracy: 0.1229 - val_loss: 2.6071 - val_categorical_accuracy: 0.3134\n",
            "Epoch 18/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 4.9214 - categorical_accuracy: 0.1323 - val_loss: 2.5540 - val_categorical_accuracy: 0.3293\n",
            "Epoch 19/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 4.8608 - categorical_accuracy: 0.1332 - val_loss: 2.5051 - val_categorical_accuracy: 0.3414\n",
            "Epoch 20/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 4.7790 - categorical_accuracy: 0.1292 - val_loss: 2.4534 - val_categorical_accuracy: 0.3517\n",
            "Epoch 21/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 4.6503 - categorical_accuracy: 0.1426 - val_loss: 2.4073 - val_categorical_accuracy: 0.3667\n",
            "Epoch 22/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 4.6560 - categorical_accuracy: 0.1484 - val_loss: 2.3607 - val_categorical_accuracy: 0.3779\n",
            "Epoch 23/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 4.5780 - categorical_accuracy: 0.1529 - val_loss: 2.3209 - val_categorical_accuracy: 0.3845\n",
            "Epoch 24/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 4.4028 - categorical_accuracy: 0.1585 - val_loss: 2.2828 - val_categorical_accuracy: 0.3957\n",
            "Epoch 25/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 4.4059 - categorical_accuracy: 0.1435 - val_loss: 2.2466 - val_categorical_accuracy: 0.4032\n",
            "Epoch 26/50\n",
            "67/67 [==============================] - 8s 124ms/step - loss: 4.2818 - categorical_accuracy: 0.1622 - val_loss: 2.2126 - val_categorical_accuracy: 0.4107\n",
            "Epoch 27/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 4.3016 - categorical_accuracy: 0.1678 - val_loss: 2.1777 - val_categorical_accuracy: 0.4275\n",
            "Epoch 28/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 4.2019 - categorical_accuracy: 0.1763 - val_loss: 2.1459 - val_categorical_accuracy: 0.4341\n",
            "Epoch 29/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 4.0827 - categorical_accuracy: 0.1774 - val_loss: 2.1142 - val_categorical_accuracy: 0.4425\n",
            "Epoch 30/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 4.0406 - categorical_accuracy: 0.1805 - val_loss: 2.0853 - val_categorical_accuracy: 0.4481\n",
            "Epoch 31/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 3.9511 - categorical_accuracy: 0.1891 - val_loss: 2.0583 - val_categorical_accuracy: 0.4518\n",
            "Epoch 32/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 3.9220 - categorical_accuracy: 0.1976 - val_loss: 2.0293 - val_categorical_accuracy: 0.4546\n",
            "Epoch 33/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 3.8928 - categorical_accuracy: 0.1896 - val_loss: 2.0048 - val_categorical_accuracy: 0.4602\n",
            "Epoch 34/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 3.6500 - categorical_accuracy: 0.2189 - val_loss: 1.9760 - val_categorical_accuracy: 0.4621\n",
            "Epoch 35/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 3.7384 - categorical_accuracy: 0.2032 - val_loss: 1.9521 - val_categorical_accuracy: 0.4668\n",
            "Epoch 36/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 3.6574 - categorical_accuracy: 0.2090 - val_loss: 1.9312 - val_categorical_accuracy: 0.4733\n",
            "Epoch 37/50\n",
            "67/67 [==============================] - 8s 117ms/step - loss: 3.6452 - categorical_accuracy: 0.2107 - val_loss: 1.9135 - val_categorical_accuracy: 0.4799\n",
            "Epoch 38/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 3.6413 - categorical_accuracy: 0.2163 - val_loss: 1.8972 - val_categorical_accuracy: 0.4864\n",
            "Epoch 39/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 3.5618 - categorical_accuracy: 0.2235 - val_loss: 1.8782 - val_categorical_accuracy: 0.4939\n",
            "Epoch 40/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 3.5245 - categorical_accuracy: 0.2168 - val_loss: 1.8538 - val_categorical_accuracy: 0.4995\n",
            "Epoch 41/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 3.4505 - categorical_accuracy: 0.2287 - val_loss: 1.8366 - val_categorical_accuracy: 0.5089\n",
            "Epoch 42/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 3.4663 - categorical_accuracy: 0.2266 - val_loss: 1.8192 - val_categorical_accuracy: 0.5108\n",
            "Epoch 43/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 3.4415 - categorical_accuracy: 0.2228 - val_loss: 1.8013 - val_categorical_accuracy: 0.5126\n",
            "Epoch 44/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 3.3135 - categorical_accuracy: 0.2392 - val_loss: 1.7838 - val_categorical_accuracy: 0.5173\n",
            "Epoch 45/50\n",
            "67/67 [==============================] - 8s 117ms/step - loss: 3.2957 - categorical_accuracy: 0.2315 - val_loss: 1.7679 - val_categorical_accuracy: 0.5239\n",
            "Epoch 46/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 3.2329 - categorical_accuracy: 0.2458 - val_loss: 1.7511 - val_categorical_accuracy: 0.5257\n",
            "Epoch 47/50\n",
            "67/67 [==============================] - 8s 117ms/step - loss: 3.2166 - categorical_accuracy: 0.2495 - val_loss: 1.7372 - val_categorical_accuracy: 0.5276\n",
            "Epoch 48/50\n",
            "67/67 [==============================] - 8s 117ms/step - loss: 3.1842 - categorical_accuracy: 0.2474 - val_loss: 1.7233 - val_categorical_accuracy: 0.5323\n",
            "Epoch 49/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 3.1327 - categorical_accuracy: 0.2584 - val_loss: 1.7105 - val_categorical_accuracy: 0.5341\n",
            "Epoch 50/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 3.1668 - categorical_accuracy: 0.2612 - val_loss: 1.6970 - val_categorical_accuracy: 0.5379\n",
            "loss fold  3 1.6828693151474\n",
            "loss media configurazione:  1.8249650398890178\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:smac.intensification.intensification.Intensifier:Wallclock time limit for intensification reached (used: 1238.335573 sec, available: 0.000010 sec)\n",
            "INFO:smac.intensification.intensification.Intensifier:Updated estimated cost of incumbent on 1 runs: 0.8611\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 1 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 4272 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 1068 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 2671 validated image filenames belonging to 20 classes.\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 10s 153ms/step - loss: 2.8601 - categorical_accuracy: 0.4532 - val_loss: 0.8793 - val_categorical_accuracy: 0.7182\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 8s 112ms/step - loss: 1.1181 - categorical_accuracy: 0.6634 - val_loss: 0.5165 - val_categorical_accuracy: 0.7753\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 8s 125ms/step - loss: 0.8274 - categorical_accuracy: 0.7411 - val_loss: 0.5231 - val_categorical_accuracy: 0.8024\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 9s 134ms/step - loss: 0.6999 - categorical_accuracy: 0.7802 - val_loss: 0.5170 - val_categorical_accuracy: 0.8071\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.5811 - categorical_accuracy: 0.8132 - val_loss: 0.4181 - val_categorical_accuracy: 0.8230\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.4646 - categorical_accuracy: 0.8493 - val_loss: 0.3627 - val_categorical_accuracy: 0.8118\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.4042 - categorical_accuracy: 0.8668 - val_loss: 0.4171 - val_categorical_accuracy: 0.8343\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.3502 - categorical_accuracy: 0.8879 - val_loss: 0.2842 - val_categorical_accuracy: 0.8268\n",
            "Epoch 9/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.3259 - categorical_accuracy: 0.8921 - val_loss: 0.4208 - val_categorical_accuracy: 0.8287\n",
            "Epoch 10/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.2644 - categorical_accuracy: 0.9113 - val_loss: 0.3415 - val_categorical_accuracy: 0.8221\n",
            "Epoch 11/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.2152 - categorical_accuracy: 0.9242 - val_loss: 0.3291 - val_categorical_accuracy: 0.8221\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 7.119416841305792e-05.\n",
            "Epoch 12/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.1734 - categorical_accuracy: 0.9429 - val_loss: 0.3440 - val_categorical_accuracy: 0.8296\n",
            "Epoch 13/50\n",
            "67/67 [==============================] - 8s 125ms/step - loss: 0.1466 - categorical_accuracy: 0.9506 - val_loss: 0.3435 - val_categorical_accuracy: 0.8361\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00013: early stopping\n",
            "loss fold  1 1.1377768516540527\n",
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 2 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 4272 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 1069 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 2670 validated image filenames belonging to 20 classes.\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 9s 137ms/step - loss: 2.7839 - categorical_accuracy: 0.4569 - val_loss: 0.9246 - val_categorical_accuracy: 0.7727\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 7s 112ms/step - loss: 1.0689 - categorical_accuracy: 0.6861 - val_loss: 0.6533 - val_categorical_accuracy: 0.8054\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.8678 - categorical_accuracy: 0.7486 - val_loss: 0.6210 - val_categorical_accuracy: 0.8167\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.7014 - categorical_accuracy: 0.7797 - val_loss: 0.6208 - val_categorical_accuracy: 0.8269\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.5898 - categorical_accuracy: 0.8200 - val_loss: 0.6132 - val_categorical_accuracy: 0.8251\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.5080 - categorical_accuracy: 0.8439 - val_loss: 0.6764 - val_categorical_accuracy: 0.8316\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.3978 - categorical_accuracy: 0.8706 - val_loss: 0.7602 - val_categorical_accuracy: 0.8297\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.3730 - categorical_accuracy: 0.8816 - val_loss: 0.6988 - val_categorical_accuracy: 0.8241\n",
            "\n",
            "Epoch 00008: ReduceLROnPlateau reducing learning rate to 7.119416841305792e-05.\n",
            "Epoch 9/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.2931 - categorical_accuracy: 0.9075 - val_loss: 0.6260 - val_categorical_accuracy: 0.8363\n",
            "Epoch 10/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.2519 - categorical_accuracy: 0.9181 - val_loss: 0.6740 - val_categorical_accuracy: 0.8391\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00010: early stopping\n",
            "loss fold  2 0.6121522784233093\n",
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 3 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 4272 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 1069 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 2670 validated image filenames belonging to 20 classes.\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 9s 129ms/step - loss: 2.8413 - categorical_accuracy: 0.4565 - val_loss: 1.0255 - val_categorical_accuracy: 0.7381\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 1.0617 - categorical_accuracy: 0.6884 - val_loss: 0.9818 - val_categorical_accuracy: 0.7736\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.8391 - categorical_accuracy: 0.7474 - val_loss: 0.7459 - val_categorical_accuracy: 0.7764\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.6884 - categorical_accuracy: 0.7884 - val_loss: 0.7791 - val_categorical_accuracy: 0.8157\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 0.5637 - categorical_accuracy: 0.8235 - val_loss: 0.7053 - val_categorical_accuracy: 0.8213\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.4930 - categorical_accuracy: 0.8399 - val_loss: 0.7017 - val_categorical_accuracy: 0.8232\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.4342 - categorical_accuracy: 0.8647 - val_loss: 0.7713 - val_categorical_accuracy: 0.8195\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.3514 - categorical_accuracy: 0.8858 - val_loss: 1.0049 - val_categorical_accuracy: 0.8007\n",
            "Epoch 9/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.3068 - categorical_accuracy: 0.8963 - val_loss: 0.6608 - val_categorical_accuracy: 0.8223\n",
            "Epoch 10/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.2543 - categorical_accuracy: 0.9103 - val_loss: 0.9362 - val_categorical_accuracy: 0.8232\n",
            "Epoch 11/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.2370 - categorical_accuracy: 0.9204 - val_loss: 1.1322 - val_categorical_accuracy: 0.8241\n",
            "Epoch 12/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.2022 - categorical_accuracy: 0.9338 - val_loss: 1.1673 - val_categorical_accuracy: 0.8223\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 7.119416841305792e-05.\n",
            "Epoch 13/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.1695 - categorical_accuracy: 0.9511 - val_loss: 0.9522 - val_categorical_accuracy: 0.8260\n",
            "Epoch 14/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 0.1342 - categorical_accuracy: 0.9581 - val_loss: 0.9126 - val_categorical_accuracy: 0.8195\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00014: early stopping\n",
            "loss fold  3 1.2758207321166992\n",
            "loss media configurazione:  1.0085832873980205\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:smac.intensification.intensification.Intensifier:Wallclock time limit for intensification reached (used: 326.372519 sec, available: 0.000010 sec)\n",
            "INFO:smac.intensification.intensification.Intensifier:Updated estimated cost of incumbent on 1 runs: 0.8611\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 1 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 4272 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 1068 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 2671 validated image filenames belonging to 20 classes.\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 11s 159ms/step - loss: 3.4586 - categorical_accuracy: 0.3455 - val_loss: 0.8902 - val_categorical_accuracy: 0.6863\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 8s 126ms/step - loss: 1.4304 - categorical_accuracy: 0.5761 - val_loss: 0.8086 - val_categorical_accuracy: 0.7509\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 8s 117ms/step - loss: 1.1078 - categorical_accuracy: 0.6646 - val_loss: 0.6209 - val_categorical_accuracy: 0.7659\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.9242 - categorical_accuracy: 0.7191 - val_loss: 0.5608 - val_categorical_accuracy: 0.7790\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.7576 - categorical_accuracy: 0.7638 - val_loss: 0.4360 - val_categorical_accuracy: 0.7987\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.6549 - categorical_accuracy: 0.7891 - val_loss: 0.4081 - val_categorical_accuracy: 0.7987\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.5702 - categorical_accuracy: 0.8160 - val_loss: 0.3068 - val_categorical_accuracy: 0.8202\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.5362 - categorical_accuracy: 0.8279 - val_loss: 0.3507 - val_categorical_accuracy: 0.8305\n",
            "Epoch 9/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 0.4707 - categorical_accuracy: 0.8401 - val_loss: 0.2503 - val_categorical_accuracy: 0.8174\n",
            "Epoch 10/50\n",
            "67/67 [==============================] - 9s 127ms/step - loss: 0.4383 - categorical_accuracy: 0.8565 - val_loss: 0.2584 - val_categorical_accuracy: 0.8240\n",
            "Epoch 11/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.3631 - categorical_accuracy: 0.8771 - val_loss: 0.2601 - val_categorical_accuracy: 0.8202\n",
            "Epoch 12/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.3277 - categorical_accuracy: 0.8909 - val_loss: 0.2163 - val_categorical_accuracy: 0.8305\n",
            "Epoch 13/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.2906 - categorical_accuracy: 0.9057 - val_loss: 0.3066 - val_categorical_accuracy: 0.8305\n",
            "Epoch 14/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.2625 - categorical_accuracy: 0.9160 - val_loss: 0.3587 - val_categorical_accuracy: 0.8146\n",
            "Epoch 15/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.2351 - categorical_accuracy: 0.9237 - val_loss: 0.3807 - val_categorical_accuracy: 0.8258\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 8.353964658454062e-05.\n",
            "Epoch 16/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.2041 - categorical_accuracy: 0.9377 - val_loss: 0.3039 - val_categorical_accuracy: 0.8333\n",
            "Epoch 17/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.1615 - categorical_accuracy: 0.9506 - val_loss: 0.2759 - val_categorical_accuracy: 0.8343\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00017: early stopping\n",
            "loss fold  1 1.3757575750350952\n",
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 2 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 4272 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 1069 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 2670 validated image filenames belonging to 20 classes.\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 9s 139ms/step - loss: 3.4212 - categorical_accuracy: 0.3371 - val_loss: 1.1740 - val_categorical_accuracy: 0.6763\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 7s 111ms/step - loss: 1.4730 - categorical_accuracy: 0.5763 - val_loss: 0.6734 - val_categorical_accuracy: 0.7558\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 1.1147 - categorical_accuracy: 0.6667 - val_loss: 0.5701 - val_categorical_accuracy: 0.7717\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 8s 117ms/step - loss: 0.8968 - categorical_accuracy: 0.7308 - val_loss: 0.5473 - val_categorical_accuracy: 0.7886\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.7502 - categorical_accuracy: 0.7666 - val_loss: 0.5763 - val_categorical_accuracy: 0.7979\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.6508 - categorical_accuracy: 0.7985 - val_loss: 0.4590 - val_categorical_accuracy: 0.8036\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.5845 - categorical_accuracy: 0.8198 - val_loss: 0.4518 - val_categorical_accuracy: 0.8036\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.5338 - categorical_accuracy: 0.8317 - val_loss: 0.4704 - val_categorical_accuracy: 0.8017\n",
            "Epoch 9/50\n",
            "67/67 [==============================] - 8s 120ms/step - loss: 0.4644 - categorical_accuracy: 0.8497 - val_loss: 0.4887 - val_categorical_accuracy: 0.8176\n",
            "Epoch 10/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.4381 - categorical_accuracy: 0.8586 - val_loss: 0.5554 - val_categorical_accuracy: 0.8073\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 8.353964658454062e-05.\n",
            "Epoch 11/50\n",
            "67/67 [==============================] - 8s 121ms/step - loss: 0.3579 - categorical_accuracy: 0.8876 - val_loss: 0.5341 - val_categorical_accuracy: 0.8148\n",
            "Epoch 12/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.3186 - categorical_accuracy: 0.9003 - val_loss: 0.5223 - val_categorical_accuracy: 0.8176\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00012: early stopping\n",
            "loss fold  2 0.47061845660209656\n",
            "create generator\n",
            "create train val and test\n",
            "=========================================\n",
            "====== K Fold Validation step => 3 =======\n",
            "=========================================\n",
            "flow from train dataframe\n",
            "Found 4272 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 1069 validated image filenames belonging to 20 classes.\n",
            "flow from test dataframe\n",
            "Found 2670 validated image filenames belonging to 20 classes.\n",
            "Inizio fit\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 9s 130ms/step - loss: 3.4763 - categorical_accuracy: 0.3493 - val_loss: 1.2835 - val_categorical_accuracy: 0.6763\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 8s 117ms/step - loss: 1.4239 - categorical_accuracy: 0.5913 - val_loss: 1.0631 - val_categorical_accuracy: 0.7418\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 8s 117ms/step - loss: 1.0707 - categorical_accuracy: 0.6852 - val_loss: 0.9042 - val_categorical_accuracy: 0.7699\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.8812 - categorical_accuracy: 0.7397 - val_loss: 0.8746 - val_categorical_accuracy: 0.7792\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.7911 - categorical_accuracy: 0.7587 - val_loss: 0.7892 - val_categorical_accuracy: 0.7933\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.6727 - categorical_accuracy: 0.7921 - val_loss: 0.6781 - val_categorical_accuracy: 0.7961\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.5605 - categorical_accuracy: 0.8296 - val_loss: 0.7556 - val_categorical_accuracy: 0.8026\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 0.5002 - categorical_accuracy: 0.8490 - val_loss: 0.6313 - val_categorical_accuracy: 0.8045\n",
            "Epoch 9/50\n",
            "67/67 [==============================] - 9s 136ms/step - loss: 0.4370 - categorical_accuracy: 0.8579 - val_loss: 0.7713 - val_categorical_accuracy: 0.8120\n",
            "Epoch 10/50\n",
            "67/67 [==============================] - 8s 122ms/step - loss: 0.3875 - categorical_accuracy: 0.8778 - val_loss: 0.7946 - val_categorical_accuracy: 0.8110\n",
            "Epoch 11/50\n",
            "67/67 [==============================] - 8s 118ms/step - loss: 0.3345 - categorical_accuracy: 0.8883 - val_loss: 0.6495 - val_categorical_accuracy: 0.8120\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 8.353964658454062e-05.\n",
            "Epoch 12/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.2903 - categorical_accuracy: 0.9068 - val_loss: 0.6435 - val_categorical_accuracy: 0.8120\n",
            "Epoch 13/50\n",
            "67/67 [==============================] - 8s 119ms/step - loss: 0.2564 - categorical_accuracy: 0.9181 - val_loss: 0.6490 - val_categorical_accuracy: 0.8157\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00013: early stopping\n",
            "loss fold  3 1.257750153541565\n",
            "loss media configurazione:  1.0347087283929188\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:smac.intensification.intensification.Intensifier:Wallclock time limit for intensification reached (used: 367.384738 sec, available: 0.000010 sec)\n",
            "INFO:smac.stats.stats.Stats:##########################################################\n",
            "INFO:smac.stats.stats.Stats:Statistics:\n",
            "INFO:smac.stats.stats.Stats:#Incumbent changed: 6\n",
            "INFO:smac.stats.stats.Stats:#Target algorithm runs: 20 / 20.0\n",
            "INFO:smac.stats.stats.Stats:#Configurations: 21\n",
            "INFO:smac.stats.stats.Stats:Used wallclock time: 8898.96 / inf sec \n",
            "INFO:smac.stats.stats.Stats:Used target algorithm runtime: 8880.43 / inf sec\n",
            "INFO:smac.stats.stats.Stats:##########################################################\n",
            "INFO:smac.facade.smac_hpo_facade.SMAC4HPO:Final Incumbent: Configuration:\n",
            "  dropout_first_layer, Value: 0.37049113463057776\n",
            "  dropout_second_layer, Value: 0.35451881390551154\n",
            "  dropout_third_layer, Value: 0.3094849311696645\n",
            "  freeze_to, Value: 18\n",
            "  learning_rate, Value: 0.00019489454303259706\n",
            "  number_first_layer, Value: 1009\n",
            "  number_layer, Value: 3\n",
            "  number_second_layer, Value: 1021\n",
            "  number_third_layer, Value: 262\n",
            "\n",
            "INFO:smac.facade.smac_hpo_facade.SMAC4HPO:Estimated cost of incumbent: 0.861117\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUM7BP6b6U_K",
        "colab_type": "text"
      },
      "source": [
        "## Plot loss for configurations tried and best seen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCQeT4Ff58RB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def losses_optimization(run_history):\n",
        "  losses = []\n",
        "  c = run_history.get_all_configs()\n",
        "  for conf in c:\n",
        "    losses.append(run_history.get_cost(conf))\n",
        "  return losses\n",
        "\n",
        "def plot_loss_configurations(run_history):\n",
        "  losses = losses_optimization(run_history)[1:]\n",
        "  plt.plot(losses)\n",
        "  plt.title('History losses for configurations tried')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xlabel('Number iteration')\n",
        "  plt.show()\n",
        "\n",
        "def plot_best_seen(run_history):\n",
        "  final = []\n",
        "  init = 100000000\n",
        "  losses = losses_optimization(run_history)[1:]\n",
        "  for loss in losses:\n",
        "    if loss < init:\n",
        "      final.append(loss)\n",
        "      init = loss\n",
        "    else:\n",
        "      final.append(init)\n",
        "  plt.plot(final, marker=\"o\")\n",
        "  plt.title('Best seen')\n",
        "  plt.xlabel('Number iteration')\n",
        "  plt.ylabel('Best seen')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEbfrZ_d6ZzM",
        "colab_type": "code",
        "outputId": "b5ac3448-51c6-4497-a786-1c7c6fe05a64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        }
      },
      "source": [
        "if to_optimize == 'feature_extraction':\n",
        "  with open(PATH_OPTIMIZATION + 'history_feature_extraction.pkl', 'rb') as f:\n",
        "      run_history = pickle.load(f)\n",
        "if to_optimize == 'fine_tuning':\n",
        "  with open(PATH_OPTIMIZATION + 'history_fine_tuning.pkl', 'rb') as f:\n",
        "      run_history = pickle.load(f)\n",
        "\n",
        "plot_loss_configurations(run_history)\n",
        "plot_best_seen(run_history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-ZHDD5C4Ksp",
        "colab_type": "text"
      },
      "source": [
        "## Train model on incumbent on all train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IN__wfa4unN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dense_dropouts_from_incumbent(incumbent):\n",
        "  dense = []\n",
        "  dropouts = []\n",
        "  layers = ['number_first_layer','number_second_layer','number_third_layer','number_fourth_layer']\n",
        "  dropout_layers = ['dropout_first_layer', 'dropout_second_layer', 'dropout_third_layer', 'dropout_fourth_layer']\n",
        "\n",
        "  for i, layer, dropout in zip(range(incumbent['number_layer']), layers, dropout_layers):\n",
        "    dense.append(incumbent[layers[i]])\n",
        "    dropouts.append(incumbent[dropout_layers[i]])\n",
        "  return dense, dropouts\n",
        "\n",
        "def optimizer_from_incumbent(incumbent):\n",
        "  '''\n",
        "  if incumbent['optimizier'] == 'SGD':\n",
        "    optimizer = SGD(learning_rate=incumbent['learning_rate'], momentum=incumbent['momentum'])\n",
        "    if incumbent['nestorov'] == True:\n",
        "      optimizer.nestorov = True\n",
        "  elif incumbent['optimizier'] == 'adam':\n",
        "    optimizer = Adam(learning_rate=incumbent['learning_rate']) \n",
        "  '''\n",
        "  optimizer = Adam(learning_rate=incumbent['learning_rate']) \n",
        "  return optimizer\n",
        "\n",
        "def model_from_incumbent(to_optimize, incumbent):\n",
        "  dense, dropouts = dense_dropouts_from_incumbent(incumbent)\n",
        "  optimizer = optimizer_from_incumbent(incumbent)\n",
        "  if to_optimize == 'feature_extraction':\n",
        "    cut_layer = incumbent['cut_layer']\n",
        "    model = feature_extraction(cut_layer, dense, dropouts, IM_SIZE, 'categorical_crossentropy', optimizer, ['categorical_accuracy'], verbose=True)\n",
        "  if to_optimize == 'fine_tuning':\n",
        "    freeze_to = incumbent['freeze_to']\n",
        "    model = fine_tuning(freeze_to, dense, dropouts, IM_SIZE, 'categorical_crossentropy', optimizer, ['categorical_accuracy'], verbose=True)    \n",
        "  return model\n",
        "\n",
        "def create_generator(df_train, df_val):\n",
        "    train_gen = ImageDataGenerator(preprocessing_function=preprocess_input_vgg16,\n",
        "                                  rotation_range=30,\n",
        "                                  horizontal_flip=True,\n",
        "                                  zoom_range=0.1\n",
        "                                  )\n",
        "    val_gen = ImageDataGenerator(preprocessing_function=preprocess_input_vgg16)\n",
        "\n",
        "    print('flow from train dataframe')\n",
        "    train_flow = train_gen.flow_from_dataframe(\n",
        "      dataframe = df_train,\n",
        "      x_col='absolute_path',\n",
        "      y_col='class',\n",
        "      shuffle=True,\n",
        "      class_mode='categorical',\n",
        "      target_size=IM_SIZE,\n",
        "      batch_size=BATCH_SIZE,\n",
        "      classes=LABELS,\n",
        "    )\n",
        "\n",
        "    print('flow from val dataframe')\n",
        "    val_flow = val_gen.flow_from_dataframe(\n",
        "      dataframe = df_val,\n",
        "      x_col='absolute_path',\n",
        "      y_col='class',\n",
        "      shuffle=False,\n",
        "      class_mode='categorical',\n",
        "      target_size=IM_SIZE,\n",
        "      batch_size=BATCH_SIZE,\n",
        "      classes=LABELS,\n",
        "    )\n",
        "    return train_flow, val_flow\n",
        "\n",
        "def train_model_all_train(to_optimize, df_train):\n",
        "  reduce_on_plateau = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, \n",
        "                                        verbose=1, mode='auto', min_delta=0.0001, \n",
        "                                        cooldown=0, min_lr=0.00001)\n",
        "  early_stop = EarlyStopping(monitor='val_loss', \n",
        "                            patience=5, \n",
        "                            verbose=1,\n",
        "                            restore_best_weights=True\n",
        "                            )\n",
        "  model_checkpoint = ModelCheckpoint(filepath=PATH_OPTIMIZATION +  to_optimize + '_final_model.h5', \n",
        "                                    monitor='val_loss', \n",
        "                                    save_best_only=True)\n",
        "  if to_optimize == 'feature_extraction':\n",
        "    with open(PATH_OPTIMIZATION + 'incubent_feature_extraction.pkl', 'rb') as f:\n",
        "        incumbent = pickle.load(f)\n",
        "    model = model_from_incumbent(to_optimize, incumbent)\n",
        "    \n",
        "  if to_optimize == 'fine_tuning':\n",
        "    with open(PATH_OPTIMIZATION + 'incumbent_fine_tuning.pkl', 'rb') as f:\n",
        "        incumbent = pickle.load(f)\n",
        "    model = model_from_incumbent(to_optimize, incumbent)\n",
        "\n",
        "  df_train, df_val = train_test_split(df_train, test_size=0.3) \n",
        "  gen_train, gen_val = create_generator(df_train, df_val)\n",
        "  history = model.fit_generator(generator = gen_train, \n",
        "            epochs = 50, \n",
        "            verbose = 1, \n",
        "            callbacks = [early_stop, reduce_on_plateau, model_checkpoint], \n",
        "            validation_data = gen_val, \n",
        "            steps_per_epoch = ceil(gen_train.n/gen_train.batch_size),\n",
        "            validation_steps = ceil(gen_val.n/gen_val.batch_size))\n",
        "  \n",
        "  return model, history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCmBHM4C-EVG",
        "colab_type": "code",
        "outputId": "b67b0197-2ea5-486b-ba6f-22c2983b1dc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "df_train = pd.read_csv(PATH_OPTIMIZATION + 'train_k_cross.csv')\n",
        "model, history = train_model_all_train(to_optimize, df_train)\n",
        "\n",
        "# save history training\n",
        "if to_optimize == 'feature_extraction':\n",
        "  with open(PATH_OPTIMIZATION + 'history_training_feature_extraction.pkl', 'wb') as f:  \n",
        "      pickle.dump(incumbent, f)\n",
        "if to_optimize == 'fine_tuning':\n",
        "  with open(PATH_OPTIMIZATION + 'history_training_fine_tuning.pkl', 'wb') as f:  \n",
        "      pickle.dump(incumbent, f)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 128, 128, 3)       0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 128, 128, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 128, 128, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 64, 64, 64)        0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 64, 64, 128)       73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 64, 64, 128)       147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 32, 32, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 32, 32, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 32, 32, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 32, 32, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 16, 16, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 16, 16, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 8, 8, 512)         0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_1 ( (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1009)              517617    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 1009)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1021)              1031210   \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 1021)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 262)               267764    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 262)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 20)                5260      \n",
            "=================================================================\n",
            "Total params: 16,536,539\n",
            "Trainable params: 1,821,851\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n",
            "flow from train dataframe\n",
            "Found 5607 validated image filenames belonging to 20 classes.\n",
            "flow from val dataframe\n",
            "Found 2404 validated image filenames belonging to 20 classes.\n",
            "Epoch 1/50\n",
            "88/88 [==============================] - 30s 342ms/step - loss: 3.7049 - categorical_accuracy: 0.2813 - val_loss: 1.5703 - val_categorical_accuracy: 0.6572\n",
            "Epoch 2/50\n",
            "88/88 [==============================] - 28s 313ms/step - loss: 1.7729 - categorical_accuracy: 0.4946 - val_loss: 1.2584 - val_categorical_accuracy: 0.7379\n",
            "Epoch 3/50\n",
            "88/88 [==============================] - 29s 328ms/step - loss: 1.4382 - categorical_accuracy: 0.5782 - val_loss: 1.1836 - val_categorical_accuracy: 0.7700\n",
            "Epoch 4/50\n",
            "88/88 [==============================] - 27s 309ms/step - loss: 1.2203 - categorical_accuracy: 0.6315 - val_loss: 1.0236 - val_categorical_accuracy: 0.7953\n",
            "Epoch 5/50\n",
            "88/88 [==============================] - 27s 311ms/step - loss: 1.0962 - categorical_accuracy: 0.6702 - val_loss: 0.9843 - val_categorical_accuracy: 0.8037\n",
            "Epoch 6/50\n",
            "88/88 [==============================] - 27s 308ms/step - loss: 1.0144 - categorical_accuracy: 0.6831 - val_loss: 0.9593 - val_categorical_accuracy: 0.8174\n",
            "Epoch 7/50\n",
            "88/88 [==============================] - 26s 300ms/step - loss: 0.9156 - categorical_accuracy: 0.7098 - val_loss: 0.8959 - val_categorical_accuracy: 0.8207\n",
            "Epoch 8/50\n",
            "88/88 [==============================] - 27s 303ms/step - loss: 0.8571 - categorical_accuracy: 0.7341 - val_loss: 0.8745 - val_categorical_accuracy: 0.8174\n",
            "Epoch 9/50\n",
            "88/88 [==============================] - 28s 314ms/step - loss: 0.8278 - categorical_accuracy: 0.7412 - val_loss: 0.7638 - val_categorical_accuracy: 0.8220\n",
            "Epoch 10/50\n",
            "88/88 [==============================] - 28s 318ms/step - loss: 0.7811 - categorical_accuracy: 0.7519 - val_loss: 0.8416 - val_categorical_accuracy: 0.8199\n",
            "Epoch 11/50\n",
            "88/88 [==============================] - 28s 319ms/step - loss: 0.7691 - categorical_accuracy: 0.7576 - val_loss: 0.8159 - val_categorical_accuracy: 0.8274\n",
            "Epoch 12/50\n",
            "88/88 [==============================] - 27s 310ms/step - loss: 0.6848 - categorical_accuracy: 0.7765 - val_loss: 0.7908 - val_categorical_accuracy: 0.8270\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.948945428011939e-05.\n",
            "Epoch 13/50\n",
            "88/88 [==============================] - 27s 306ms/step - loss: 0.6581 - categorical_accuracy: 0.7919 - val_loss: 0.7669 - val_categorical_accuracy: 0.8253\n",
            "Epoch 14/50\n",
            "88/88 [==============================] - 28s 315ms/step - loss: 0.6727 - categorical_accuracy: 0.7817 - val_loss: 0.7733 - val_categorical_accuracy: 0.8253\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00014: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVxkjM3JXU2D",
        "colab_type": "code",
        "outputId": "f60150bd-e665-4bb2-9070-fb6cef5d571a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        }
      },
      "source": [
        "plot_performance(history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngqgVtqrIsD-",
        "colab_type": "text"
      },
      "source": [
        "## Performance of model on test set, after AutoML"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2GI2JyjIvBU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def performance_on_test(model, test_generator):\n",
        "  predictions = model.predict_generator(test_generator, \n",
        "                          steps=ceil(test_generator.n / test_generator.batch_size))\n",
        "  predict_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "  performance = classification_report(  y_true = test_generator.classes, \n",
        "                                        y_pred = predict_classes,\n",
        "                                        target_names=LABELS\n",
        "  )\n",
        "\n",
        "  return performance"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KzjJgzhHttl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "to_optimize = 'fine_tuning'\n",
        "df_test = pd.read_csv(PATH_OPTIMIZATION + 'test.csv')\n",
        "model = load_model(PATH_OPTIMIZATION +  to_optimize + '_final_model.h5') # load optimized model\n",
        "test_generator = ImageDataGenerator(preprocessing_function=preprocess_input_vgg16)\n",
        "test_gen = test_generator.flow_from_dataframe(\n",
        "  dataframe = df_test,\n",
        "  x_col='absolute_path',\n",
        "  y_col='class',\n",
        "  shuffle=False,\n",
        "  class_mode='categorical',\n",
        "  target_size=IM_SIZE,\n",
        "  batch_size=BATCH_SIZE,\n",
        "  classes=LABELS,\n",
        ")\n",
        "result = performance_on_test(model, test_gen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h2eF2vMiYyG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(result)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}