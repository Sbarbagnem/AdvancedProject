{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "bwTcVNKCAoJt",
        "awq2lzbxBXgj",
        "AHm9t-_NGYDa",
        "WgzdNcHZGgch",
        "dDw9W32GaThb",
        "XXwxrNdCShlc",
        "pNm8VPigbjay"
      ],
      "authorship_tag": "ABX9TyPrQ3Sr+P7KYcAQyv6EhjJE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sbarbagnem/AdvancedProject/blob/master/Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGjYNEjPFiCE",
        "colab_type": "text"
      },
      "source": [
        "# Module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJr3vEODFZE1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import os\n",
        "import xml.etree.ElementTree as ET\n",
        "from math import ceil\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications import VGG16, ResNet50, MobileNet\n",
        "from keras.applications.vgg16 import preprocess_input as preprocess_input_vgg16\n",
        "from keras.applications.resnet50 import preprocess_input as preprocess_input_resnet50\n",
        "from keras.applications.mobilenet import preprocess_input as preprocess_input_mobilenet\n",
        "from keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
        "from keras import Model\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eD9R4p_JGS8j",
        "colab_type": "text"
      },
      "source": [
        "# Costant"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jui_q9SGGXsi",
        "colab_type": "code",
        "outputId": "5f77e7d7-5698-4341-c209-d6f0e600a635",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "PATH_ANNOTATIONS = '/content/drive/My Drive/Laurea Magistrale/Adavanched Machine Learning/Progetto/Dataset/Train/Annotations/'\n",
        "PATH_MAIN = '/content/drive/My Drive/Laurea Magistrale/Adavanched Machine Learning/Progetto/Dataset/Train/Main/'\n",
        "PATH_IMAGES = '/content/drive/My Drive/Laurea Magistrale/Adavanched Machine Learning/Progetto/Dataset/Train/Images/'\n",
        "PATH_IMAGES_CROPPED_TRAIN = '/content/drive/My Drive/Laurea Magistrale/Adavanched Machine Learning/Progetto/Dataset/Train/Cropped_train/'\n",
        "PATH_IMAGES_CROPPED_VAL_TEST = '/content/drive/My Drive/Laurea Magistrale/Adavanched Machine Learning/Progetto/Dataset/Train/Cropped_val_test/'\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "IM_SIZE = (224, 224)\n",
        "\n",
        "LABELS = ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', \n",
        "          'car', 'cat', 'chair', 'cow', 'diningtable',\n",
        "          'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa',\n",
        "          'train', 'tvmonitor']"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwTcVNKCAoJt",
        "colab_type": "text"
      },
      "source": [
        "# Directory for cropped images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shqDlbvsAsmi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creo 20 directory, una per ogni classe, per fare classificazione (flow_from_datframe) sulle immagini croppate\n",
        "def create_directories(path, labels):\n",
        "  for label in labels:\n",
        "    os.mkdir(os.path.join(path, label))\n",
        "\n",
        "create_folder_classes = False\n",
        "\n",
        "if create_folder_classes == True:\n",
        "  create_directories(PATH_IMAGES_CROPPED_TRAIN, LABELS)\n",
        "  create_directories(PATH_IMAGES_CROPPED_VAL_TEST, LABELS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awq2lzbxBXgj",
        "colab_type": "text"
      },
      "source": [
        "# Util function for mapping from label to #class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NN3ewyc_BenA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_dict_mapping(labels):\n",
        "  mapping = {}\n",
        "  for label,i in zip(labels, range(len(labels))):\n",
        "    mapping[label] = i\n",
        "  return mapping\n",
        "\n",
        "def from_label_to_number(mapping, label):\n",
        "  return mapping[label]\n",
        "\n",
        "def from_number_to_label(mapping, number):\n",
        "  for key, val in mapping.items(): \n",
        "    if val == number: \n",
        "      return key \n",
        "  return \"key doesn't exist\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EGlorv3K-NG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "dae62782-f9bf-43d9-d521-766861e3aed0"
      },
      "source": [
        "mapping = create_dict_mapping(LABELS)\n",
        "print(mapping)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'aeroplane': 0, 'bicycle': 1, 'bird': 2, 'boat': 3, 'bottle': 4, 'bus': 5, 'car': 6, 'cat': 7, 'chair': 8, 'cow': 9, 'diningtable': 10, 'dog': 11, 'horse': 12, 'motorbike': 13, 'person': 14, 'pottedplant': 15, 'sheep': 16, 'sofa': 17, 'train': 18, 'tvmonitor': 19}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHm9t-_NGYDa",
        "colab_type": "text"
      },
      "source": [
        "# Crop and save image for class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5P1zAHxAGdTJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def crop_image(img, x_min, y_min, x_max, y_max):\n",
        "  crop_img = img[y_min:y_max, x_min:x_max]\n",
        "  return crop_img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QkTnM8cPtA_",
        "colab_type": "code",
        "outputId": "e49e0040-c93b-4b11-e11e-e4e0c9612f85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "def list_images(path_images, path_txt, file_txt):\n",
        "  '''\n",
        "  creo liste immagini presenti in file_txt\n",
        "  '''\n",
        "  temp = []\n",
        "  f = open(os.path.join(path_txt, file_txt), \"r\")\n",
        "  for line in f.readlines():\n",
        "    temp.append(line.split('\\n')[0] + '.jpg')\n",
        "  list_images = [os.path.join(path_images, name) for name in temp]\n",
        "  print('Ho trovato ', len(list_images), 'per il file ', file_txt)\n",
        "  return list_images\n",
        "\n",
        "# creo liste immagini da train.txt e val.txt\n",
        "list_images_train = list_images(PATH_IMAGES, PATH_MAIN, 'train.txt')\n",
        "list_images_val = list_images(PATH_IMAGES, PATH_MAIN, 'val.txt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Laurea Magistrale/Adavanched Machine Learning/Progetto/Dataset/Train/Images/2008_000008.jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCnf_0tOQ687",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_and_crop(list_images, annotation_dir, path_to_save):\n",
        "  '''\n",
        "  leggo annotations dei file presenti nelle due liste di immagini \n",
        "  '''\n",
        "  for path in list_images:\n",
        "    image_name = path.split('/')[-1].split('.')[0] \n",
        "    print(image_name)\n",
        "    with open(os.path.join(annotation_dir, image_name + '.xml')) as f:\n",
        "      read_xml(f.read(), path, path_to_save)\n",
        "  return\n",
        "\n",
        "def read_xml(file_xml, path_image, path_to_save):\n",
        "  '''\n",
        "  leggo xml e per ogni box che trovo croppo e salvo\n",
        "  '''\n",
        "  img = cv2.imread(path_image)  \n",
        "  root = ET.XML(file_xml)\n",
        "  for _, child in enumerate(root):\n",
        "    if child.tag == 'object':\n",
        "      x_min = None\n",
        "      y_min = None\n",
        "      x_max = None\n",
        "      y_max = None\n",
        "      for subchild in child:\n",
        "        if subchild.tag == 'name':\n",
        "          name_object = subchild.text\n",
        "          #print(name_object)\n",
        "        if subchild.tag == 'bndbox':\n",
        "          for bndbox in subchild:\n",
        "            if bndbox.tag == 'xmin':\n",
        "              x_min = int(bndbox.text)\n",
        "              #print('x_min ', x_min)\n",
        "            if bndbox.tag == 'ymin':\n",
        "              y_min = int(bndbox.text)\n",
        "              #print('y_min ', y_min)\n",
        "            if bndbox.tag == 'xmax':\n",
        "              x_max = int(bndbox.text)\n",
        "              #print('x_max ', x_max)\n",
        "            if bndbox.tag == 'ymax':\n",
        "              y_max = int(bndbox.text)\n",
        "              #print('y_max ', y_max)\n",
        "        if(x_min!=None and y_min!=None and x_max!=None and y_max!=None):\n",
        "          image_cropped = crop_image(img, x_min, y_min, x_max, y_max)\n",
        "          x_min = None\n",
        "          y_min = None\n",
        "          x_max = None\n",
        "          y_max = None\n",
        "          #cv2_imshow(image_cropped)\n",
        "          save_image_cropped(name_object, image_cropped, path_to_save)\n",
        "  return \n",
        "\n",
        "def save_image_cropped(obj, img, path_to_save):\n",
        "  '''\n",
        "  salvo immagine croppata con numero progressivo in base \n",
        "  all'ultima presente nella cartella\n",
        "  '''\n",
        "  list_dir_classes = os.listdir(path_to_save)\n",
        "  for dir_class in list_dir_classes:\n",
        "    if dir_class == obj:\n",
        "      dir_temp = os.path.join(path_to_save,dir_class)\n",
        "      list_temp = os.listdir(dir_temp)\n",
        "      if list_temp == []:\n",
        "        cv2.imwrite(os.path.join(dir_temp, obj + '_1.jpg'), img)\n",
        "      else:\n",
        "        number_file = int(list_temp[-1].split('.')[0].split('_')[1]) + 1\n",
        "        cv2.imwrite(os.path.join(dir_temp, obj + '_' + str(number_file) + '.jpg'), img)\n",
        "  return\n",
        "\n",
        "read_and_crop(list_images=list_images_val, annotation_dir=PATH_ANNOTATIONS, path_to_save=PATH_IMAGES_CROPPED_VAL_TEST)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgzdNcHZGgch",
        "colab_type": "text"
      },
      "source": [
        "# Util function to create generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMtwpZ_kGfsx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_generator( batch_size, im_size, base_net, train_directory, val_directory, labels,\n",
        "                      validation_split):\n",
        "  \n",
        "  if base_net == 'vgg16':\n",
        "    preprocess_function = preprocess_input_vgg16\n",
        "  elif base_net == 'mobilenet':\n",
        "    preprocess_function = preprocess_input_mobilenet\n",
        "  elif base_net == 'resnet':\n",
        "    preprocess_function = preprocess_input_resnet50\n",
        "\n",
        "  img_gen_train = ImageDataGenerator( rescale=1/255,\n",
        "                                      preprocessing_function=preprocess_function,\n",
        "                                    )\n",
        "  img_gen_val = ImageDataGenerator( rescale=1/255,\n",
        "                                    preprocessing_function=preprocess_function,\n",
        "                                    validation_split = validation_split\n",
        "                                  )\n",
        "\n",
        "  train_gen = img_gen_train.flow_from_directory(\n",
        "      directory = train_directory,\n",
        "      shuffle=True,\n",
        "      class_mode='categorical',\n",
        "      target_size=im_size,\n",
        "      batch_size=batch_size,\n",
        "      classes=labels\n",
        "\n",
        "  )\n",
        "\n",
        "  val_gen = img_gen_val.flow_from_directory(\n",
        "      directory = val_directory,\n",
        "      shuffle=False,\n",
        "      class_mode='categorical',\n",
        "      target_size=im_size,\n",
        "      batch_size=batch_size,\n",
        "      subset='training',\n",
        "      classes=labels\n",
        "  )\n",
        "\n",
        "  test_gen = img_gen_val.flow_from_directory(\n",
        "      directory = val_directory,\n",
        "      shuffle=False,\n",
        "      class_mode='categorical',\n",
        "      target_size=im_size,\n",
        "      batch_size=batch_size,\n",
        "      subset='validation',\n",
        "      classes=labels\n",
        "  )\n",
        "\n",
        "  return train_gen, val_gen, test_gen"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDw9W32GaThb",
        "colab_type": "text"
      },
      "source": [
        "# Util function for frequency classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teCq1HPhaXTg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_frequency_from_generator(generator):\n",
        "  mapping = generator.class_indices\n",
        "  classes, count = np.unique(generator.labels, return_counts=True)\n",
        "  labels = [from_number_to_label(mapping, label) for label in classes]\n",
        "  freq = count\n",
        "  return labels, freq\n",
        "\n",
        "def normalize_frequency(freq):\n",
        "  min_freq = np.min(freq)\n",
        "  freq_normalize = [min_freq/x for x in freq]\n",
        "  return freq_normalize * freq\n",
        "\n",
        "def plot_label_frequency(labels, freq):\n",
        "  freq = freq / np.sum(freq)\n",
        "  l = list(range(1, len(labels)+1))\n",
        "  plt.barh(l, width=freq, height=0.5)\n",
        "  plt.yticks(l, labels, rotation='horizontal')\n",
        "  plt.show()\n",
        "\n",
        "def plot_stacked_bar_freq(labels, freq, freq_normalize):\n",
        "  N = len(labels)\n",
        "  ind = np.arange(N)    # the x locations for the groups\n",
        "  width = 0.35       # the width of the bars: can also be len(x) sequence\n",
        "\n",
        "  p1 = plt.bar(ind, freq, width)\n",
        "  p2 = plt.bar(ind, freq_normalize, width)\n",
        "\n",
        "  plt.ylabel('Frequency')\n",
        "  plt.legend((p1[0], p2[0]), ('Freq', 'Freq_normalize'))\n",
        "\n",
        "  plt.show() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXwxrNdCShlc",
        "colab_type": "text"
      },
      "source": [
        "# Util function to create model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etwOZM2GSjL_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(base_net, freeze_all, freeze_to, dense, dropout):\n",
        "  if base_net == 'vgg16':\n",
        "    base_model = VGG16(weights = 'imagenet', input_shape=(224,224,3), include_top = False)\n",
        "  elif base_net == 'resnet':\n",
        "    base_model = ResNet50(weights = 'imagenet', input_shape=(224,224,3), include_top = False)\n",
        "  elif base_net == 'mobilenet':\n",
        "    base_model = MobileNet(weights = 'imagenet', input_shape=(224,224,3), include_top = False)\n",
        "\n",
        "  if freeze_all == True:\n",
        "    for layer in base_model.layers:\n",
        "      layer.trainable=False\n",
        "  else:\n",
        "    for layer in base_model.layers[:freeze_to]:\n",
        "      layer.trainable=False\n",
        "    for layer in model.layers[freeze_to:]:\n",
        "      layer.trainable = True\n",
        "  \n",
        "  # aggiungere cut ad un certo layer?\n",
        "\n",
        "  x = base_model.output\n",
        "  x = GlobalAveragePooling2D()(x)\n",
        "  for layer in dense:\n",
        "    x = Dense(layer, activation='relu')(x)\n",
        "    x = Dropout(dropout)(x)\n",
        "  predictions = Dense(20, activation = 'softmax')(x)\n",
        "  model = Model(input = base_model.input, output = predictions)\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22ReLsjTEeCZ",
        "colab_type": "text"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "se7bKuAJbulU",
        "colab_type": "text"
      },
      "source": [
        "## Set up generator and model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDNTdRNpGBpI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_net = 'mobilenet'\n",
        "\n",
        "train_gen, val_gen, test_gen = create_generator(batch_size=BATCH_SIZE, \n",
        "                                                im_size=IM_SIZE, \n",
        "                                                base_net=base_net, \n",
        "                                                train_directory=PATH_IMAGES_CROPPED_TRAIN, \n",
        "                                                val_directory=PATH_IMAGES_CROPPED_VAL_TEST, \n",
        "                                                labels=LABELS,\n",
        "                                                validation_split=0.5)\n",
        "\n",
        "model = create_model( base_net=base_net, \n",
        "                      freeze_all=True, \n",
        "                      freeze_to=0, \n",
        "                      dense=[512],\n",
        "                      dropout=0.3\n",
        "                     )\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNm8VPigbjay",
        "colab_type": "text"
      },
      "source": [
        "## Normalize frequency labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7QYMSTWbaof",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "65c7bb9b-84b7-4e94-d2a9-1054d7ebe8f5"
      },
      "source": [
        "labels, freq = get_frequency_from_generator(train_gen)\n",
        "#plot_label_frequency(labels, freq)\n",
        "freq_normalize = normalize_frequency(freq)\n",
        "#plot_label_frequency(labels, freq_normalize)\n",
        "plot_stacked_bar_freq(labels, freq, freq_normalize)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAD4CAYAAADLhBA1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWSacUMzMLBNOKGZmlgknFDMzy8T/BzfHShP7jHwXAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAg57-_ib21t",
        "colab_type": "text"
      },
      "source": [
        "## Set up iper-parameter for fit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QSFiR2Ab6v1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss = 'categorical_crossentropy'\n",
        "optimizer = 'adam'\n",
        "metrics = ['categorical_accuracy']\n",
        "epochs = 30\n",
        "steps_per_epoch = ceil(train_gen.n/BATCH_SIZE)\n",
        "validation_steps = ceil(val_gen.n/BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9HmoEWOUkkU",
        "colab_type": "code",
        "outputId": "26f786b6-8e65-435d-babf-d2320252259b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "model.compile(loss=loss, \n",
        "              optimizer=optimizer, \n",
        "              metrics=metrics\n",
        "              )\n",
        "\n",
        "# callbacks\n",
        "early_stop = EarlyStopping(monitor='val_loss', \n",
        "                           patience=2, \n",
        "                           verbose=1\n",
        "                           )\n",
        "\n",
        "net_history = model.fit_generator(train_gen, epochs=epochs, verbose=1,\n",
        "                                  validation_data = val_gen,\n",
        "                                  steps_per_epoch = steps_per_epoch,\n",
        "                                  validation_steps = validation_steps,\n",
        "                                  callbacks = [early_stop],\n",
        "                                  workers = 8,\n",
        "                                  use_multiprocessing=True,\n",
        "                                  )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "177/247 [====================>.........] - ETA: 6:29 - loss: 1.3470 - categorical_accuracy: 0.6061"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}